{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys,os,imageio,lpips\n",
    "root = '/home/youngsun/documents/mvs/mvsnerf_timing'\n",
    "os.chdir(root)\n",
    "sys.path.append(root)\n",
    "\n",
    "from opt import config_parser\n",
    "from data import dataset_dict\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# models\n",
    "from models_timer import *\n",
    "from renderer_timer import *\n",
    "from data.ray_utils import get_rays\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from skimage.metrics import structural_similarity\n",
    "\n",
    "# pytorch-lightning\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning import LightningModule, Trainer, loggers\n",
    "\n",
    "\n",
    "from data.ray_utils import ray_marcher\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/envs/mvsnerf/lib/python3.8/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    }
   ],
   "source": [
    "def decode_batch(batch):\n",
    "    rays = batch['rays']  # (B, 8)\n",
    "    rgbs = batch['rgbs']  # (B, 3)\n",
    "    return rays, rgbs\n",
    "\n",
    "def unpreprocess(data, shape=(1,1,3,1,1)):\n",
    "    # to unnormalize image for visualization\n",
    "    # data N V C H W\n",
    "    device = data.device\n",
    "    mean = torch.tensor([-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225]).view(*shape).to(device)\n",
    "    std = torch.tensor([1 / 0.229, 1 / 0.224, 1 / 0.225]).view(*shape).to(device)\n",
    "\n",
    "    return (data - mean) / std\n",
    "\n",
    "def read_depth(filename):\n",
    "    depth_h = np.array(read_pfm(filename)[0], dtype=np.float32) # (800, 800)\n",
    "    depth_h = cv2.resize(depth_h, None, fx=0.5, fy=0.5,\n",
    "                       interpolation=cv2.INTER_NEAREST)  # (600, 800)\n",
    "    depth_h = depth_h[44:556, 80:720]  # (512, 640)\n",
    "#     depth = cv2.resize(depth_h, None, fx=0.5, fy=0.5,interpolation=cv2.INTER_NEAREST)#!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    mask = depth>0\n",
    "    return depth_h,mask\n",
    "\n",
    "loss_fn_vgg = lpips.LPIPS(net='vgg') \n",
    "mse2psnr = lambda x : -10. * np.log(x) / np.log(10.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quantity evauation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os,imageio,lpips,cv2,torch,glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/envs/mvsnerf/lib/python3.8/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    }
   ],
   "source": [
    "loss_fn_vgg = lpips.LPIPS(net='vgg') \n",
    "mse2psnr = lambda x : -10. * np.log(x) / np.log(10.)\n",
    "\n",
    "\n",
    "\n",
    "def acc_threshold(abs_err, threshold):\n",
    "    \"\"\"\n",
    "    computes the percentage of pixels whose depth error is less than @threshold\n",
    "    \"\"\"\n",
    "    acc_mask = abs_err < threshold\n",
    "    return  acc_mask.astype('float') if type(abs_err) is np.ndarray else acc_mask.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTU no fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rendering novel views with nearest 3 views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_and_record(records_processed, name='test', scenes=[1], num_src=4, img_scale=1.0, save_as_image=True):\n",
    "    \n",
    "    psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "    \n",
    "    for i_scene, scene in enumerate(scenes):#,8,21,103,114\n",
    "        \n",
    "        psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "        \n",
    "        # create timing variables\n",
    "\n",
    "        # measure time - all processes\n",
    "        start_all = torch.cuda.Event(enable_timing=True)\n",
    "        end_all = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        # measure time - Feature extraction and neural volume encoding\n",
    "        start_mvsnet = torch.cuda.Event(enable_timing=True)\n",
    "        end_mvsnet = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        # measure time - MVSNeRF and volume rendering\n",
    "        start_mvsnerf = torch.cuda.Event(enable_timing=True)\n",
    "        end_mvsnerf = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        # measure time - MVSNeRF and volume rendering loop\n",
    "        start_loop = torch.cuda.Event(enable_timing=True)\n",
    "        end_loop = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        # measure time - MVSNeRF and volume rendering all loops\n",
    "        start_loops = torch.cuda.Event(enable_timing=True)\n",
    "        end_loops = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        # for combining all recorded time\n",
    "        records = []\n",
    "        records_general = {\"0_all\" : [], \"1_loop\" : [], \"2_loops\" : []}\n",
    "        records_mvsnet = {\"0_total\" : [], \"1_feat\" : [], \"2_costvol\" : [], \"3_3dcnn\" : []}\n",
    "        records_mvsnerf = {\"0_total\" : [],\"1_sample\" : [], \"2_nerf\" : [], \"3_rend\" : []}\n",
    "\n",
    "        cmd = f'--datadir /mnt/hdd/mvsnerf_data/dtu/scan{scene}  \\\n",
    "         --dataset_name dtu_ft  \\\n",
    "         --net_type v0 --ckpt ./ckpts/mvsnerf-v0.tar \\\n",
    "         --imgScale_train {img_scale} --imgScale_test {img_scale} --img_downscale {img_scale}'\n",
    "\n",
    "        args = config_parser(cmd.split())\n",
    "        args.use_viewdirs = True\n",
    "\n",
    "        args.N_samples = 128\n",
    "        args.feat_dim =  8+4*3\n",
    "\n",
    "        # create models\n",
    "        if 0==i_scene:\n",
    "            render_kwargs_train, render_kwargs_test, start, grad_vars = create_nerf_mvs(args, use_mvs=True, dir_embedder=False, pts_embedder=True)\n",
    "            filter_keys(render_kwargs_train)\n",
    "\n",
    "            MVSNet = render_kwargs_train['network_mvs']\n",
    "            render_kwargs_train.pop('network_mvs')\n",
    "\n",
    "\n",
    "        datadir = args.datadir\n",
    "        datatype = 'train'\n",
    "        pad = 16\n",
    "        args.chunk = 5120\n",
    "\n",
    "        dataset_train = dataset_dict[args.dataset_name](args, split='train')\n",
    "        dataset_val = dataset_dict[args.dataset_name](args, split='val')\n",
    "        val_idx = dataset_val.img_idx\n",
    "\n",
    "        save_as_image = True\n",
    "        save_dir = f'results/test_dtu_{name}'\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        MVSNet.train()\n",
    "        MVSNet = MVSNet.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "#             try:\n",
    "#                 tqdm._instances.clear() \n",
    "#             except Exception:     \n",
    "#                 pass       \n",
    "\n",
    "#             for i, batch in enumerate(tqdm(dataset_val)):\n",
    "            for i, batch in enumerate(dataset_val):\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                ##################\n",
    "                # time everything\n",
    "                ##################\n",
    "                start_all.record()\n",
    "                #\n",
    "\n",
    "                rays, img = decode_batch(batch)\n",
    "                rays = rays.squeeze().to(device)  # (H*W, 3)\n",
    "                img = img.squeeze().cpu().numpy()  # (H, W, 3)\n",
    "                depth = batch['depth'].squeeze().numpy()  # (H, W)\n",
    "\n",
    "                # find nearest image idx from training views\n",
    "                positions = dataset_train.poses[:,:3,3]\n",
    "                dis = np.sum(np.abs(positions - dataset_val.poses[[i],:3,3]), axis=-1)\n",
    "                pair_idx = np.argsort(dis)[:num_src]\n",
    "                pair_idx = [dataset_train.img_idx[item] for item in pair_idx]\n",
    "\n",
    "                imgs_source, proj_mats, near_far_source, pose_source = dataset_train.read_source_views(pair_idx=pair_idx,device=device)\n",
    "                \n",
    "                ##################\n",
    "                # time mvsnet 0\n",
    "                ##################\n",
    "                start_mvsnet.record()\n",
    "                #\n",
    "                volume_feature, _, _, records_mvsnet = MVSNet(imgs_source, proj_mats, near_far_source, records=records_mvsnet, pad=pad)\n",
    "                #\n",
    "                end_mvsnet.record()\n",
    "                torch.cuda.synchronize()\n",
    "                records_mvsnet['0_total'].append(start_mvsnet.elapsed_time(end_mvsnet))\n",
    "                ##############################################################\n",
    "\n",
    "\n",
    "                imgs_source = unpreprocess(imgs_source)\n",
    "\n",
    "                N_rays_all = rays.shape[0]\n",
    "                rgb_rays, depth_rays_preds = [],[]\n",
    "\n",
    "                ##################\n",
    "                # time loops\n",
    "                ##################\n",
    "                start_loops.record()\n",
    "                #\n",
    "\n",
    "                for chunk_idx in range(N_rays_all//args.chunk + int(N_rays_all%args.chunk>0)):\n",
    "\n",
    "                    # for loop timing\n",
    "                    ##################\n",
    "                    # time loop\n",
    "                    ##################\n",
    "                    start_loop.record()\n",
    "                    #\n",
    "                    xyz_coarse_sampled, rays_o, rays_d, z_vals = ray_marcher(rays[chunk_idx*args.chunk:(chunk_idx+1)*args.chunk],\n",
    "                                                        N_samples=args.N_samples)\n",
    "\n",
    "                    # Converting world coordinate to ndc coordinate\n",
    "                    H, W = img.shape[:2]\n",
    "                    inv_scale = torch.tensor([W - 1, H - 1]).to(device)\n",
    "                    w2c_ref, intrinsic_ref = pose_source['w2cs'][0], pose_source['intrinsics'][0].clone()\n",
    "                    xyz_NDC = get_ndc_coordinate(w2c_ref, intrinsic_ref, xyz_coarse_sampled, inv_scale,\n",
    "                                                 near=near_far_source[0], far=near_far_source[1], pad=pad*args.imgScale_test)\n",
    "\n",
    "\n",
    "                    # rendering\n",
    "\n",
    "                    ##################\n",
    "                    # time mvsnerf 0\n",
    "                    ##################\n",
    "                    start_mvsnerf.record()\n",
    "                    #\n",
    "                    rgb, disp, acc, depth_pred, alpha, extras, records_mvsnerf = rendering(args, pose_source, xyz_coarse_sampled,\n",
    "                                                                           xyz_NDC, z_vals, rays_o, rays_d, records_mvsnerf,\n",
    "                                                                           volume_feature,imgs_source, **render_kwargs_train)\n",
    "                    #\n",
    "                    end_mvsnerf.record()\n",
    "                    torch.cuda.synchronize()\n",
    "                    records_mvsnerf['0_total'].append(start_mvsnerf.elapsed_time(end_mvsnerf))\n",
    "                    ##############################################################\n",
    "\n",
    "\n",
    "\n",
    "                    rgb, depth_pred = torch.clamp(rgb.cpu(),0,1.0).numpy(), depth_pred.cpu().numpy()\n",
    "                    rgb_rays.append(rgb)\n",
    "                    depth_rays_preds.append(depth_pred)\n",
    "\n",
    "                    #\n",
    "                    end_loop.record()\n",
    "                    torch.cuda.synchronize()\n",
    "                    records_general['1_loop'].append(start_loop.elapsed_time(end_loop))\n",
    "                    ##############################################################\n",
    "\n",
    "                #\n",
    "                end_loops.record()\n",
    "                torch.cuda.synchronize()\n",
    "                records_general['2_loops'].append(start_loops.elapsed_time(end_loops))\n",
    "                ##############################################################\n",
    "\n",
    "                #\n",
    "                end_all.record()\n",
    "                torch.cuda.synchronize()\n",
    "                records_general['0_all'].append(start_all.elapsed_time(end_all))\n",
    "                ##############################################################\n",
    "                \n",
    "                depth_rays_preds = np.concatenate(depth_rays_preds).reshape(H, W)\n",
    "                depth_rays_preds, _ = visualize_depth_numpy(depth_rays_preds, near_far_source)\n",
    "\n",
    "                rgb_rays = np.concatenate(rgb_rays).reshape(H, W, 3)\n",
    "                img_vis = np.concatenate((img*255,rgb_rays*255,depth_rays_preds),axis=1)\n",
    "\n",
    "                if save_as_image:\n",
    "                    imageio.imwrite(f'{save_dir}/scan{scene}_{val_idx[i]:03d}.png', img_vis.astype('uint8'))\n",
    "                \n",
    "                # quantity\n",
    "                # mask background since they are outside the far boundle\n",
    "                mask = depth==0\n",
    "                imageio.imwrite(f'{save_dir}/scan{scene}_{val_idx[i]:03d}_mask.png', mask.astype('uint8')*255)\n",
    "                rgb_rays[mask],img[mask] = 0.0,0.0\n",
    "                psnr.append( mse2psnr(np.mean((rgb_rays[~mask]-img[~mask])**2)))\n",
    "                ssim.append( structural_similarity(rgb_rays, img, multichannel=True))\n",
    "\n",
    "                img_tensor = torch.from_numpy(rgb_rays)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "                img_gt_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0\n",
    "                LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "            psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "\n",
    "    records_processed = append_records(records_processed, records_general, records_mvsnet, records_mvsnerf, \n",
    "                                       scenes, num_src, img_scale,\n",
    "                                       np.mean(psnr_all), np.mean(ssim_all), np.mean(LPIPS_vgg_all), \n",
    "                                       name)\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return records_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_records():\n",
    "    return {'experiment name':[],\n",
    "            'novel scenes synthesized':[],\n",
    "            'number of source images':[],\n",
    "            'image scale':[],\n",
    "            'psnr':[],\n",
    "            'ssim':[],\n",
    "            'lpips':[],\n",
    "            'total time':[],\n",
    "            'mvsnet total time':[],\n",
    "            'mvsnet feature extraction':[],\n",
    "            'mvsnet cost volume':[],\n",
    "            'mvsnet 3D-CNN':[],\n",
    "            'mvsnerf total time':[],\n",
    "            'mvsnerf volume sampling':[],\n",
    "            'mvsnerf nerf':[],\n",
    "            'mvsnerf rendering':[]\n",
    "           }\n",
    "\n",
    "def append_records(records_processed, records_general, records_mvsnet, records_mvsnerf, \n",
    "                   scenes, num_src, img_scale,\n",
    "                   psnr, ssim, lpips,\n",
    "                   name='this experiment wants a name'):\n",
    "\n",
    "    records_processed['experiment name'] += [f'{name}']\n",
    "    \n",
    "    ###\n",
    "    records_processed['novel scenes synthesized'] += [scenes]\n",
    "    records_processed['number of source images'] += [num_src]\n",
    "    records_processed['image scale'] += [img_scale]\n",
    "\n",
    "    records_processed['psnr'] += [psnr]\n",
    "    records_processed['ssim'] += [ssim]\n",
    "    records_processed['lpips'] += [lpips]\n",
    "    \n",
    "    records_processed['total time'] += [np.mean(records_general['0_all'])]\n",
    "\n",
    "    records_processed['mvsnet total time'] += [np.mean(records_mvsnet['0_total'])]\n",
    "    records_processed['mvsnet feature extraction'] += [np.mean(records_mvsnet['1_feat'])]\n",
    "    records_processed['mvsnet cost volume'] += [np.mean(records_mvsnet['2_costvol'])]\n",
    "    records_processed['mvsnet 3D-CNN'] += [np.mean(records_mvsnet['3_3dcnn'])]\n",
    "    \n",
    "    records_processed['mvsnerf total time'] += [np.mean(records_mvsnerf['0_total']) * len (records_mvsnerf['0_total']) / len(records_general['0_all'])]\n",
    "    records_processed['mvsnerf volume sampling'] += [np.mean(records_mvsnerf['1_sample']) * len (records_mvsnerf['1_sample']) / len(records_general['0_all'])]\n",
    "    records_processed['mvsnerf nerf'] += [np.mean(records_mvsnerf['2_nerf']) * len (records_mvsnerf['2_nerf']) / len(records_general['0_all'])]\n",
    "    records_processed['mvsnerf rendering'] += [np.mean(records_mvsnerf['3_rend']) * len (records_mvsnerf['3_rend']) / len(records_general['0_all'])]\n",
    "\n",
    "    return records_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ckpts ['./ckpts/mvsnerf-v0.tar']\n",
      "Reloading from ./ckpts/mvsnerf-v0.tar\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mvsnerf/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_752/3174675028.py:200: FutureWarning: `multichannel` is a deprecated argument name for `structural_similarity`. It will be removed in version 1.0.Please use `channel_axis` instead.\n",
      "  ssim.append( structural_similarity(rgb_rays, img, multichannel=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "Found ckpts ['./ckpts/mvsnerf-v0.tar']\n",
      "Reloading from ./ckpts/mvsnerf-v0.tar\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "Found ckpts ['./ckpts/mvsnerf-v0.tar']\n",
      "Reloading from ./ckpts/mvsnerf-v0.tar\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "Found ckpts ['./ckpts/mvsnerf-v0.tar']\n",
      "Reloading from ./ckpts/mvsnerf-v0.tar\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "Found ckpts ['./ckpts/mvsnerf-v0.tar']\n",
      "Reloading from ./ckpts/mvsnerf-v0.tar\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "Found ckpts ['./ckpts/mvsnerf-v0.tar']\n",
      "Reloading from ./ckpts/mvsnerf-v0.tar\n",
      "==> image down scale: 0.8\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 0.8\n",
      "===> valing index: [32, 24, 23, 44]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 409, 512]' is invalid for input of size 839680",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m records_processed \u001b[38;5;241m=\u001b[39m experiment_and_record(records_processed, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_src=12\u001b[39m\u001b[38;5;124m'\u001b[39m, scenes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m21\u001b[39m,\u001b[38;5;241m30\u001b[39m,\u001b[38;5;241m31\u001b[39m,\u001b[38;5;241m34\u001b[39m,\u001b[38;5;241m63\u001b[39m,\u001b[38;5;241m82\u001b[39m,\u001b[38;5;241m103\u001b[39m,\u001b[38;5;241m114\u001b[39m], num_src\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, img_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, save_as_image\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# decreasing image scale\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m records_processed \u001b[38;5;241m=\u001b[39m \u001b[43mexperiment_and_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecords_processed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimg_scale=0.8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscenes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m21\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m31\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m34\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m63\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m82\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m103\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m114\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_src\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_as_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m records_processed \u001b[38;5;241m=\u001b[39m experiment_and_record(records_processed, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_scale=0.6\u001b[39m\u001b[38;5;124m'\u001b[39m, scenes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m21\u001b[39m,\u001b[38;5;241m30\u001b[39m,\u001b[38;5;241m31\u001b[39m,\u001b[38;5;241m34\u001b[39m,\u001b[38;5;241m63\u001b[39m,\u001b[38;5;241m82\u001b[39m,\u001b[38;5;241m103\u001b[39m,\u001b[38;5;241m114\u001b[39m], num_src\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, img_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m, save_as_image\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m records_processed \u001b[38;5;241m=\u001b[39m experiment_and_record(records_processed, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_scale=0.4\u001b[39m\u001b[38;5;124m'\u001b[39m, scenes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m21\u001b[39m,\u001b[38;5;241m30\u001b[39m,\u001b[38;5;241m31\u001b[39m,\u001b[38;5;241m34\u001b[39m,\u001b[38;5;241m63\u001b[39m,\u001b[38;5;241m82\u001b[39m,\u001b[38;5;241m103\u001b[39m,\u001b[38;5;241m114\u001b[39m], num_src\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, img_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m, save_as_image\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mexperiment_and_record\u001b[0;34m(records_processed, name, scenes, num_src, img_scale, save_as_image)\u001b[0m\n\u001b[1;32m     60\u001b[0m args\u001b[38;5;241m.\u001b[39mchunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5120\u001b[39m\n\u001b[1;32m     62\u001b[0m dataset_train \u001b[38;5;241m=\u001b[39m dataset_dict[args\u001b[38;5;241m.\u001b[39mdataset_name](args, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m dataset_val \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m val_idx \u001b[38;5;241m=\u001b[39m dataset_val\u001b[38;5;241m.\u001b[39mimg_idx\n\u001b[1;32m     66\u001b[0m save_as_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/documents/mvs/mvsnerf_timing/data/dtu_ft.py:37\u001b[0m, in \u001b[0;36mDTU_ft.__init__\u001b[0;34m(self, args, split, load_ref)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnear_far \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m2.125\u001b[39m, \u001b[38;5;241m4.525\u001b[39m]\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m load_ref:\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_meta\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/documents/mvs/mvsnerf_timing/data/dtu_ft.py:192\u001b[0m, in \u001b[0;36mDTU_ft.read_meta\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_rays \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_rays, \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# (len(self.meta['frames]),h*w, 3)\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_rgbs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_rgbs, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_wh[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# (len(self.meta['frames]),h,w,3)\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_depth \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_wh\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 409, 512]' is invalid for input of size 839680"
     ]
    }
   ],
   "source": [
    "## 이걸로 계속 돌리고 쌓으면 됨!\n",
    "\n",
    "records_processed = init_records()\n",
    "records_processed = experiment_and_record(records_processed, name='base', scenes=[1,8,21,103,114], num_src=3, img_scale=1.0, save_as_image=True)\n",
    "records_processed = experiment_and_record(records_processed, name='base+more_scenes', scenes=[1,8,21,30,31,34,63,82,103,114], num_src=3, img_scale=1.0, save_as_image=True)\n",
    "\n",
    "# number of source views \n",
    "records_processed = experiment_and_record(records_processed, name='num_src=4', scenes=[1,8,21,30,31,34,63,82,103,114], num_src=4, img_scale=1.0, save_as_image=True)\n",
    "records_processed = experiment_and_record(records_processed, name='num_src=8', scenes=[1,8,21,30,31,34,63,82,103,114], num_src=8, img_scale=1.0, save_as_image=True)\n",
    "records_processed = experiment_and_record(records_processed, name='num_src=12', scenes=[1,8,21,30,31,34,63,82,103,114], num_src=12, img_scale=1.0, save_as_image=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ckpts ['./ckpts/mvsnerf-v0.tar']\n",
      "Reloading from ./ckpts/mvsnerf-v0.tar\n",
      "==> image down scale: 0.75\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 0.75\n",
      "===> valing index: [32, 24, 23, 44]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# decreasing image scale 1.00 = (512, 640, 3) (384, 480, 3) (256, 320, 3) (128, 160, 3)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m records_processed \u001b[38;5;241m=\u001b[39m \u001b[43mexperiment_and_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecords_processed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimg_scale=0.75\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscenes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m21\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m31\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m34\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m63\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m82\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m103\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m114\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_src\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.75\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_as_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m records_processed \u001b[38;5;241m=\u001b[39m experiment_and_record(records_processed, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_scale=0.50\u001b[39m\u001b[38;5;124m'\u001b[39m, scenes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m21\u001b[39m,\u001b[38;5;241m30\u001b[39m,\u001b[38;5;241m31\u001b[39m,\u001b[38;5;241m34\u001b[39m,\u001b[38;5;241m63\u001b[39m,\u001b[38;5;241m82\u001b[39m,\u001b[38;5;241m103\u001b[39m,\u001b[38;5;241m114\u001b[39m], num_src\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, img_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.50\u001b[39m, save_as_image\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m records_processed \u001b[38;5;241m=\u001b[39m experiment_and_record(records_processed, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_scale=0.25\u001b[39m\u001b[38;5;124m'\u001b[39m, scenes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m21\u001b[39m,\u001b[38;5;241m30\u001b[39m,\u001b[38;5;241m31\u001b[39m,\u001b[38;5;241m34\u001b[39m,\u001b[38;5;241m63\u001b[39m,\u001b[38;5;241m82\u001b[39m,\u001b[38;5;241m103\u001b[39m,\u001b[38;5;241m114\u001b[39m], num_src\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, img_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m, save_as_image\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mexperiment_and_record\u001b[0;34m(records_processed, name, scenes, num_src, img_scale, save_as_image)\u001b[0m\n\u001b[1;32m    150\u001b[0m start_mvsnerf\u001b[38;5;241m.\u001b[39mrecord()\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m rgb, disp, acc, depth_pred, alpha, extras, records_mvsnerf \u001b[38;5;241m=\u001b[39m \u001b[43mrendering\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpose_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxyz_coarse_sampled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mxyz_NDC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrays_o\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrays_d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecords_mvsnerf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mvolume_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimgs_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrender_kwargs_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    156\u001b[0m end_mvsnerf\u001b[38;5;241m.\u001b[39mrecord()\n",
      "File \u001b[0;32m~/documents/mvs/mvsnerf_timing/renderer_timer.py:184\u001b[0m, in \u001b[0;36mrendering\u001b[0;34m(args, pose_ref, rays_pts, rays_ndc, depth_candidates, rays_o, rays_dir, records, volume_feature, imgs, network_fn, img_feat, network_query_fn, white_bkgd, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    183\u001b[0m end_2\u001b[38;5;241m.\u001b[39mrecord()\n\u001b[0;32m--> 184\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m records[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2_nerf\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(start_2\u001b[38;5;241m.\u001b[39melapsed_time(end_2))\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m##############################################################\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/mvsnerf/lib/python3.8/site-packages/torch/cuda/__init__.py:493\u001b[0m, in \u001b[0;36msynchronize\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    491\u001b[0m _lazy_init()\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[0;32m--> 493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_synchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# decreasing image scale \n",
    "# 1.00 = (512, 640, 3) \n",
    "# 0.75 = (384, 480, 3) \n",
    "# 0.50 = (256, 320, 3) \n",
    "# 0.25 = (128, 160, 3)\n",
    "records_processed = experiment_and_record(records_processed, name='img_scale=0.75', scenes=[1,8,21,30,31,34,63,82,103,114], num_src=3, img_scale=0.75, save_as_image=True)\n",
    "records_processed = experiment_and_record(records_processed, name='img_scale=0.50', scenes=[1,8,21,30,31,34,63,82,103,114], num_src=3, img_scale=0.50, save_as_image=True)\n",
    "records_processed = experiment_and_record(records_processed, name='img_scale=0.25', scenes=[1,8,21,30,31,34,63,82,103,114], num_src=3, img_scale=0.25, save_as_image=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_df = pd.DataFrame(records_processed).round({'psnr':1, 'ssim':2, 'lpips':2, \n",
    "                                       'total time':0, 'mvsnet total time':0, 'mvsnerf total time':0, \n",
    "                                       'mvsnet feature extraction' : 1, 'mvsnet cost volume':1, 'mvsnet 3D-CNN':1,\n",
    "                                       'mvsnerf volume sampling': 1, 'mvsnerf nerf' : 1, 'mvsnerf rendering' : 1\n",
    "                                      })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_df['experiment name'] = records_df['experiment name'].replace(['img_scale=0.8', 'img_scale=0.6', 'img_scale=0.4'], ['img_scale=0.75', 'img_scale=0.50', 'img_scale=0.25'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment name</th>\n",
       "      <th>novel scenes synthesized</th>\n",
       "      <th>number of source images</th>\n",
       "      <th>image scale</th>\n",
       "      <th>psnr</th>\n",
       "      <th>ssim</th>\n",
       "      <th>lpips</th>\n",
       "      <th>total time</th>\n",
       "      <th>mvsnet total time</th>\n",
       "      <th>mvsnet feature extraction</th>\n",
       "      <th>mvsnet cost volume</th>\n",
       "      <th>mvsnet 3D-CNN</th>\n",
       "      <th>mvsnerf total time</th>\n",
       "      <th>mvsnerf volume sampling</th>\n",
       "      <th>mvsnerf nerf</th>\n",
       "      <th>mvsnerf rendering</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>base</td>\n",
       "      <td>[1, 8, 21, 103, 114]</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.17</td>\n",
       "      <td>2693.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>33.9</td>\n",
       "      <td>65.8</td>\n",
       "      <td>2497.0</td>\n",
       "      <td>129.2</td>\n",
       "      <td>2333.0</td>\n",
       "      <td>19.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>base+more_scenes</td>\n",
       "      <td>[1, 8, 21, 30, 31, 34, 63, 82, 103, 114]</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>25.8</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2718.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>33.9</td>\n",
       "      <td>65.4</td>\n",
       "      <td>2521.0</td>\n",
       "      <td>131.8</td>\n",
       "      <td>2354.9</td>\n",
       "      <td>19.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>num_src=4</td>\n",
       "      <td>[1, 8, 21, 30, 31, 34, 63, 82, 103, 114]</td>\n",
       "      <td>4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>25.9</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2741.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>39.1</td>\n",
       "      <td>65.2</td>\n",
       "      <td>2524.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>2356.8</td>\n",
       "      <td>19.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>num_src=8</td>\n",
       "      <td>[1, 8, 21, 30, 31, 34, 63, 82, 103, 114]</td>\n",
       "      <td>8</td>\n",
       "      <td>1.00</td>\n",
       "      <td>25.8</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2821.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>63.3</td>\n",
       "      <td>65.9</td>\n",
       "      <td>2520.0</td>\n",
       "      <td>131.2</td>\n",
       "      <td>2354.0</td>\n",
       "      <td>19.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>num_src=12</td>\n",
       "      <td>[1, 8, 21, 30, 31, 34, 63, 82, 103, 114]</td>\n",
       "      <td>12</td>\n",
       "      <td>1.00</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2919.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>93.5</td>\n",
       "      <td>66.1</td>\n",
       "      <td>2520.0</td>\n",
       "      <td>131.7</td>\n",
       "      <td>2353.5</td>\n",
       "      <td>19.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>img_scale=0.75</td>\n",
       "      <td>[1, 8, 21, 30, 31, 34, 63, 82, 103, 114]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.75</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1563.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>18.1</td>\n",
       "      <td>41.2</td>\n",
       "      <td>1418.0</td>\n",
       "      <td>73.3</td>\n",
       "      <td>1323.3</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>img_scale=0.50</td>\n",
       "      <td>[1, 8, 21, 30, 31, 34, 63, 82, 103, 114]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.50</td>\n",
       "      <td>20.6</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.21</td>\n",
       "      <td>716.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>20.9</td>\n",
       "      <td>622.0</td>\n",
       "      <td>31.3</td>\n",
       "      <td>581.6</td>\n",
       "      <td>4.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>img_scale=0.25</td>\n",
       "      <td>[1, 8, 21, 30, 31, 34, 63, 82, 103, 114]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.25</td>\n",
       "      <td>17.3</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.23</td>\n",
       "      <td>221.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>9.5</td>\n",
       "      <td>155.0</td>\n",
       "      <td>7.6</td>\n",
       "      <td>144.7</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    experiment name                  novel scenes synthesized  \\\n",
       "0              base                      [1, 8, 21, 103, 114]   \n",
       "1  base+more_scenes  [1, 8, 21, 30, 31, 34, 63, 82, 103, 114]   \n",
       "2         num_src=4  [1, 8, 21, 30, 31, 34, 63, 82, 103, 114]   \n",
       "3         num_src=8  [1, 8, 21, 30, 31, 34, 63, 82, 103, 114]   \n",
       "4        num_src=12  [1, 8, 21, 30, 31, 34, 63, 82, 103, 114]   \n",
       "5    img_scale=0.75  [1, 8, 21, 30, 31, 34, 63, 82, 103, 114]   \n",
       "6    img_scale=0.50  [1, 8, 21, 30, 31, 34, 63, 82, 103, 114]   \n",
       "7    img_scale=0.25  [1, 8, 21, 30, 31, 34, 63, 82, 103, 114]   \n",
       "\n",
       "   number of source images  image scale  psnr  ssim  lpips  total time  \\\n",
       "0                        3         1.00  26.2  0.93   0.17      2693.0   \n",
       "1                        3         1.00  25.8  0.93   0.16      2718.0   \n",
       "2                        4         1.00  25.9  0.94   0.16      2741.0   \n",
       "3                        8         1.00  25.8  0.94   0.16      2821.0   \n",
       "4                       12         1.00  25.6  0.93   0.16      2919.0   \n",
       "5                        3         0.75  24.0  0.91   0.17      1563.0   \n",
       "6                        3         0.50  20.6  0.87   0.21       716.0   \n",
       "7                        3         0.25  17.3  0.82   0.23       221.0   \n",
       "\n",
       "   mvsnet total time  mvsnet feature extraction  mvsnet cost volume  \\\n",
       "0              105.0                        4.9                33.9   \n",
       "1              105.0                        5.1                33.9   \n",
       "2              111.0                        6.2                39.1   \n",
       "3              140.0                       10.3                63.3   \n",
       "4              175.0                       15.3                93.5   \n",
       "5               63.0                        3.9                18.1   \n",
       "6               35.0                        3.5                 9.9   \n",
       "7               19.0                        4.0                 4.9   \n",
       "\n",
       "   mvsnet 3D-CNN  mvsnerf total time  mvsnerf volume sampling  mvsnerf nerf  \\\n",
       "0           65.8              2497.0                    129.2        2333.0   \n",
       "1           65.4              2521.0                    131.8        2354.9   \n",
       "2           65.2              2524.0                    132.0        2356.8   \n",
       "3           65.9              2520.0                    131.2        2354.0   \n",
       "4           66.1              2520.0                    131.7        2353.5   \n",
       "5           41.2              1418.0                     73.3        1323.3   \n",
       "6           20.9               622.0                     31.3         581.6   \n",
       "7            9.5               155.0                      7.6         144.7   \n",
       "\n",
       "   mvsnerf rendering  \n",
       "0               19.8  \n",
       "1               19.7  \n",
       "2               19.8  \n",
       "3               19.3  \n",
       "4               19.8  \n",
       "5               11.9  \n",
       "6                4.9  \n",
       "7                1.3  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_df.to_csv('./csv/3_renderer_dtu_timer_numviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_df = pd.read_csv('./csv/3_renderer_dtu_timer_numviews.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['total time',\n",
       " 'mvsnet total time',\n",
       " 'mvsnet feature extraction',\n",
       " 'mvsnet cost volume',\n",
       " 'mvsnet 3D-CNN',\n",
       " 'mvsnerf total time',\n",
       " 'mvsnerf volume sampling',\n",
       " 'mvsnerf nerf',\n",
       " 'mvsnerf rendering']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(records_df.columns.values[7:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2693.0, 105.0, 4.9, 33.9, 65.8, 2497.0, 129.2, 2333.0, 19.8]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(records_df.iloc[0].values[7:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'base'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records_df.iloc[0].values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = list(records_df.columns.values[7:])\n",
    "times1 = list(records_df.iloc[0].values[7:])\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.1  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - 3*width, list(records_df.iloc[1].values[7:]), width, label=records_df.iloc[1].values[0])\n",
    "rects2 = ax.bar(x - 2*width, list(records_df.iloc[2].values[7:]), width, label=records_df.iloc[2].values[0])\n",
    "rects3 = ax.bar(x - 1*width, list(records_df.iloc[3].values[7:]), width, label=records_df.iloc[3].values[0])\n",
    "rects4 = ax.bar(x, list(records_df.iloc[4].values[7:]), width, label=records_df.iloc[4].values[0])\n",
    "rects5 = ax.bar(x + 1*width, list(records_df.iloc[5].values[7:]), width, label=records_df.iloc[5].values[0])\n",
    "rects6 = ax.bar(x + 2*width, list(records_df.iloc[6].values[7:]), width, label=records_df.iloc[6].values[0])\n",
    "rects7 = ax.bar(x + 3*width, list(records_df.iloc[7].values[7:]), width, label=records_df.iloc[7].values[0])\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('time / ms')\n",
    "ax.set_title('Graph of time spent during each rendering process')\n",
    "ax.set_xticks(x, labels)\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (18,8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = list(records_df.columns.values[7:])\n",
    "times1 = list(records_df.iloc[0].values[7:])\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.1  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects6 = ax.bar(x - 0.5*width, list(records_df.iloc[6].values[7:]), width, label=records_df.iloc[6].values[0])\n",
    "rects7 = ax.bar(x + 0.5*width, list(records_df.iloc[7].values[7:]), width, label=records_df.iloc[7].values[0])\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('time / ms')\n",
    "ax.set_title('Graph of time spent during each rendering process')\n",
    "ax.set_xticks(x, labels)\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (5,12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = torch.load('./configs/pairs.th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'horns_train': array([23, 32, 24, 41, 34, 22, 25, 35, 42, 12, 21, 51, 13, 39, 50, 26]),\n",
       " 'horns_test': array([33, 40, 31, 59]),\n",
       " 'horns_val': array([33, 40, 31, 59]),\n",
       " 'leaves_train': array([12, 18,  8, 17, 14,  7,  3, 22,  2, 19, 23,  9, 10,  6, 15, 21]),\n",
       " 'leaves_test': array([13, 11, 16,  4]),\n",
       " 'leaves_val': array([13, 11, 16,  4]),\n",
       " 'room_train': array([14, 39, 34, 13, 20, 36, 30, 31,  8,  7, 33, 25, 12, 29, 40, 19]),\n",
       " 'room_test': array([35, 15, 38, 21]),\n",
       " 'room_val': array([35, 15, 38, 21]),\n",
       " 'fortress_train': array([15, 20, 26, 14, 22, 27, 33, 39, 16, 32, 19,  8, 10,  3, 13, 28]),\n",
       " 'fortress_test': array([21,  9, 40, 25]),\n",
       " 'fortress_val': array([21,  9, 40, 25]),\n",
       " 'trex_train': array([52, 19, 47, 12, 46, 28, 18, 51, 27, 13, 48, 43, 29, 11, 45, 42]),\n",
       " 'trex_test': array([20, 21, 53, 22]),\n",
       " 'trex_val': array([20, 21, 53, 22]),\n",
       " 'orchids_train': array([ 8, 13, 11, 17,  7,  9, 14, 18, 23,  6,  2, 15,  5,  3, 22, 24]),\n",
       " 'orchids_test': array([12, 10, 16, 19]),\n",
       " 'orchids_val': array([12, 10, 16, 19]),\n",
       " 'fern_train': array([17,  2,  7,  6, 11,  1,  8, 18,  3, 16,  4, 14,  9, 10,  0, 15]),\n",
       " 'fern_test': array([12, 13,  5, 19]),\n",
       " 'fern_val': array([12, 13,  5, 19]),\n",
       " 'flower_train': array([12, 29, 11,  4, 31, 13, 15, 30, 10, 27, 14, 19, 28,  3, 18, 21]),\n",
       " 'flower_test': array([20,  6, 22,  5]),\n",
       " 'flower_val': array([20,  6, 22,  5]),\n",
       " 'lego_train': [6, 43, 33, 13, 17, 19, 20, 25, 30, 37, 46, 48, 49, 55, 59, 65],\n",
       " 'lego_test': [63, 70, 18, 28],\n",
       " 'lego_val': [63, 70, 18, 28],\n",
       " 'chair_train': array([62, 56, 26, 67, 92, 31, 63, 77, 85, 82, 47, 41, 55, 61, 99, 25]),\n",
       " 'chair_test': array([ 8, 24, 32, 78]),\n",
       " 'chair_val': array([ 8, 24, 32, 78]),\n",
       " 'ship_train': array([12, 32, 44, 17, 47,  3, 19,  2, 33, 77, 95, 54, 11, 98, 67, 87]),\n",
       " 'ship_test': array([80, 86, 22, 20]),\n",
       " 'ship_val': array([80, 86, 22, 20]),\n",
       " 'drums_train': [43, 81, 14, 3, 9, 11, 20, 21, 22, 40, 41, 42, 46, 51, 52, 55],\n",
       " 'drums_test': [79, 74, 91, 68],\n",
       " 'drums_val': [79, 74, 91, 68],\n",
       " 'materials_train': array([34, 73, 94, 97, 47, 19, 58, 16, 21, 13, 82, 61, 18, 79, 78, 37]),\n",
       " 'materials_test': array([36, 63, 46, 96]),\n",
       " 'materials_val': array([36, 63, 46, 96]),\n",
       " 'ficus_train': array([92, 69, 56, 98,  3, 64, 61, 45,  8, 62, 83, 17,  7, 44, 32, 90]),\n",
       " 'ficus_test': array([38, 23,  0,  5]),\n",
       " 'ficus_val': array([38, 23,  0,  5]),\n",
       " 'hotdog_train': array([48, 61,  0,  3, 33, 73, 78, 58, 28, 63, 71, 38, 22, 30,  9, 31]),\n",
       " 'hotdog_test': array([26, 60, 13, 47]),\n",
       " 'hotdog_val': array([26, 60, 13, 47]),\n",
       " 'mic_train': array([61, 80, 64,  2, 85, 15, 97, 93, 53, 44, 71, 68, 32, 90, 99,  6]),\n",
       " 'mic_test': array([20, 49, 55, 72]),\n",
       " 'mic_val': array([20, 49, 55, 72]),\n",
       " 'dtu_train': [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36],\n",
       " 'dtu_test': [32, 24, 23, 44],\n",
       " 'dtu_val': [32, 24, 23, 44],\n",
       " 'horns_teaser_train': [0,\n",
       "  6,\n",
       "  33,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  52,\n",
       "  53,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61],\n",
       " 'horns_teaser_val': [34],\n",
       " 'horns_teaser_test': [34],\n",
       " 'xgaze_11images_cropped_colmapCODE_test': array([0, 5]),\n",
       " 'xgaze_11images_cropped_colmapCODE_val': array([0, 5]),\n",
       " 'xgaze_11images_cropped_colmapCODE_train': array([ 1,  7,  6, 10,  8,  2,  9,  4,  3])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of source views \n",
    "records_processed = experiment_and_record(records_processed, name='num_src=4', scenes=[1,8,21,30,31,34,63,82,103,114], num_src=4, img_scale=1.0, save_as_image=True)\n",
    "records_processed = experiment_and_record(records_processed, name='num_src=8', scenes=[1,8,21,30,31,34,63,82,103,114], num_src=8, img_scale=1.0, save_as_image=True)\n",
    "records_processed = experiment_and_record(records_processed, name='num_src=12', scenes=[1,8,21,30,31,34,63,82,103,114], num_src=12, img_scale=1.0, save_as_image=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decreasing image scale\n",
    "records_processed = experiment_and_record(records_processed, name='img_scale=0.8', scenes=[1,8,21,30,31,34,63,82,103,114], num_src=3, img_scale=0.8, save_as_image=True)\n",
    "records_processed = experiment_and_record(records_processed, name='img_scale=0.6', scenes=[1,8,21,30,31,34,63,82,103,114], num_src=3, img_scale=0.6, save_as_image=True)\n",
    "records_processed = experiment_and_record(records_processed, name='img_scale=0.4', scenes=[1,8,21,30,31,34,63,82,103,114], num_src=3, img_scale=0.4, save_as_image=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ckpts ['./ckpts/mvsnerf-v0.tar']\n",
      "Reloading from ./ckpts/mvsnerf-v0.tar\n",
      "============> rendering dataset <===================\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                           | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                           | 0/4 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/mvsnerf/lib/python3.8/site-packages/PIL/ImageFile.py:495\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 495\u001b[0m     fh \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileno\u001b[49m()\n\u001b[1;32m    496\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28mprint\u001b[39m(rgb_rays\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_as_image:\n\u001b[0;32m--> 212\u001b[0m     \u001b[43mimageio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msave_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/scan\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mscene\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mval_idx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m03d\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_vis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muint8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     rgbs\u001b[38;5;241m.\u001b[39mappend(img_vis\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muint8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/opt/conda/envs/mvsnerf/lib/python3.8/site-packages/imageio/core/functions.py:196\u001b[0m, in \u001b[0;36mimwrite\u001b[0;34m(uri, im, format, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage must be 2D (grayscale, RGB, or RGBA).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m imopen(uri, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwi\u001b[39m\u001b[38;5;124m\"\u001b[39m, plugin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/mvsnerf/lib/python3.8/site-packages/imageio/core/legacy_plugin_wrapper.py:172\u001b[0m, in \u001b[0;36mLegacyPlugin.write\u001b[0;34m(self, image, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlegacy_get_writer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;241m.\u001b[39mimage_mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miv\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 172\u001b[0m         \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(image) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/envs/mvsnerf/lib/python3.8/site-packages/imageio/core/format.py:559\u001b[0m, in \u001b[0;36mFormat.Writer.append_data\u001b[0;34m(self, im, meta)\u001b[0m\n\u001b[1;32m    557\u001b[0m im \u001b[38;5;241m=\u001b[39m asarray(im)\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# Call\u001b[39;00m\n\u001b[0;32m--> 559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_append_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_meta\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/mvsnerf/lib/python3.8/site-packages/imageio/plugins/pillow_legacy.py:460\u001b[0m, in \u001b[0;36mPNGFormat.Writer._append_data\u001b[0;34m(self, im, meta)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m     im \u001b[38;5;241m=\u001b[39m image_as_uint(im, bitdepth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m--> 460\u001b[0m \u001b[43mPillowFormat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_append_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/mvsnerf/lib/python3.8/site-packages/imageio/plugins/pillow_legacy.py:382\u001b[0m, in \u001b[0;36mPillowFormat.Writer._append_data\u001b[0;34m(self, im, meta)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbits\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_meta:\n\u001b[1;32m    381\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mquantize()  \u001b[38;5;66;03m# Make it a P image, so bits arg is used\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplugin_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_meta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m save_pillow_close(img)\n",
      "File \u001b[0;32m/opt/conda/envs/mvsnerf/lib/python3.8/site-packages/PIL/Image.py:2212\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2209\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2211\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2212\u001b[0m     \u001b[43msave_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2213\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# do what we can to clean up\u001b[39;00m\n\u001b[1;32m   2215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m open_fp:\n",
      "File \u001b[0;32m/opt/conda/envs/mvsnerf/lib/python3.8/site-packages/PIL/PngImagePlugin.py:1348\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[1;32m   1346\u001b[0m     _write_multiple_frames(im, fp, chunk, rawmode)\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1348\u001b[0m     \u001b[43mImageFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_idat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrawmode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info:\n\u001b[1;32m   1351\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m info_chunk \u001b[38;5;129;01min\u001b[39;00m info\u001b[38;5;241m.\u001b[39mchunks:\n",
      "File \u001b[0;32m/opt/conda/envs/mvsnerf/lib/python3.8/site-packages/PIL/ImageFile.py:509\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 509\u001b[0m         l, s, d \u001b[38;5;241m=\u001b[39m \u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m         fp\u001b[38;5;241m.\u001b[39mwrite(d)\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m s:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "depth_acc = {}\n",
    "eval_metric = [0.1,0.05,0.01]\n",
    "depth_acc[f'abs_err'],depth_acc[f'acc_l_{eval_metric[0]}'],depth_acc[f'acc_l_{eval_metric[1]}'],depth_acc[f'acc_l_{eval_metric[2]}'] = {},{},{},{}\n",
    "   \n",
    "\n",
    "for i_scene, scene in enumerate([1]):#,8,21,103,114\n",
    "\n",
    "    # create timing variables\n",
    "\n",
    "    # measure time - all processes\n",
    "    start_all = torch.cuda.Event(enable_timing=True)\n",
    "    end_all = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # measure time - Feature extraction and neural volume encoding\n",
    "    start_mvsnet = torch.cuda.Event(enable_timing=True)\n",
    "    end_mvsnet = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    # measure time - MVSNeRF and volume rendering\n",
    "    start_mvsnerf = torch.cuda.Event(enable_timing=True)\n",
    "    end_mvsnerf = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    # measure time - MVSNeRF and volume rendering loop\n",
    "    start_loop = torch.cuda.Event(enable_timing=True)\n",
    "    end_loop = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # measure time - MVSNeRF and volume rendering all loops\n",
    "    start_loops = torch.cuda.Event(enable_timing=True)\n",
    "    end_loops = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    # for combining all recorded time\n",
    "    records = []\n",
    "    records_general = {\"0_all\" : [], \"1_loop\" : [], \"2_loops\" : []}\n",
    "    records_mvsnet = {\"0_total\" : [], \"1_feat\" : [], \"2_costvol\" : [], \"3_3dcnn\" : []}\n",
    "    records_mvsnerf = {\"0_total\" : [],\"1_sample\" : [], \"2_nerf\" : [], \"3_rend\" : []}\n",
    "    \n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    cmd = f'--datadir /mnt/hdd/mvsnerf_data/dtu/scan{scene}  \\\n",
    "     --dataset_name dtu_ft  \\\n",
    "     --net_type v0 --ckpt ./ckpts/mvsnerf-v0.tar \\\n",
    "     --imgScale_train 1 --imgScale_test 1 --img_downscale 1'\n",
    "\n",
    "    args = config_parser(cmd.split())\n",
    "    args.use_viewdirs = True\n",
    "\n",
    "    args.N_samples = 128\n",
    "    args.feat_dim =  8+4*3\n",
    "\n",
    "    # create models\n",
    "    if 0==i_scene:\n",
    "        render_kwargs_train, render_kwargs_test, start, grad_vars = create_nerf_mvs(args, use_mvs=True, dir_embedder=False, pts_embedder=True)\n",
    "        filter_keys(render_kwargs_train)\n",
    "\n",
    "        MVSNet = render_kwargs_train['network_mvs']\n",
    "        render_kwargs_train.pop('network_mvs')\n",
    "\n",
    "\n",
    "    datadir = args.datadir\n",
    "    datatype = 'train'\n",
    "    pad = 16\n",
    "    args.chunk = 5120\n",
    "\n",
    "\n",
    "    print('============> rendering dataset <===================')\n",
    "    dataset_train = dataset_dict[args.dataset_name](args, split='train')\n",
    "    dataset_val = dataset_dict[args.dataset_name](args, split='val')\n",
    "    val_idx = dataset_val.img_idx\n",
    "\n",
    "    save_as_image = True\n",
    "    save_dir = f'results/test_dtu'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    MVSNet.train()\n",
    "    MVSNet = MVSNet.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        try:\n",
    "            tqdm._instances.clear() \n",
    "        except Exception:     \n",
    "            pass       \n",
    "\n",
    "        for i, batch in enumerate(tqdm(dataset_val)):\n",
    "                        \n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            ##################\n",
    "            # time everything\n",
    "            ##################\n",
    "            start_all.record()\n",
    "            #\n",
    "\n",
    "            rays, img = decode_batch(batch)\n",
    "            rays = rays.squeeze().to(device)  # (H*W, 3)\n",
    "            img = img.squeeze().cpu().numpy()  # (H, W, 3)\n",
    "            depth = batch['depth'].squeeze().numpy()  # (H, W)\n",
    "\n",
    "            # find nearest image idx from training views\n",
    "            positions = dataset_train.poses[:,:3,3]\n",
    "            dis = np.sum(np.abs(positions - dataset_val.poses[[i],:3,3]), axis=-1)\n",
    "            pair_idx = np.argsort(dis)[:3]\n",
    "            pair_idx = [dataset_train.img_idx[item] for item in pair_idx]\n",
    "\n",
    "            imgs_source, proj_mats, near_far_source, pose_source = dataset_train.read_source_views(pair_idx=pair_idx,device=device)\n",
    "            \n",
    "            ##################\n",
    "            # time mvsnet 0\n",
    "            ##################\n",
    "            start_mvsnet.record()\n",
    "            #\n",
    "            volume_feature, _, _, records_mvsnet = MVSNet(imgs_source, proj_mats, near_far_source, records=records_mvsnet, pad=pad)\n",
    "            #\n",
    "            end_mvsnet.record()\n",
    "            torch.cuda.synchronize()\n",
    "            records_mvsnet['0_total'].append(start_mvsnet.elapsed_time(end_mvsnet))\n",
    "            ##############################################################\n",
    "            \n",
    "                \n",
    "            imgs_source = unpreprocess(imgs_source)\n",
    "\n",
    "            N_rays_all = rays.shape[0]\n",
    "            rgb_rays, depth_rays_preds = [],[]\n",
    "            \n",
    "            ##################\n",
    "            # time loops\n",
    "            ##################\n",
    "            start_loops.record()\n",
    "            #\n",
    "            \n",
    "            for chunk_idx in range(N_rays_all//args.chunk + int(N_rays_all%args.chunk>0)):\n",
    "                \n",
    "                # for loop timing\n",
    "                ##################\n",
    "                # time loop\n",
    "                ##################\n",
    "                start_loop.record()\n",
    "                #\n",
    "                xyz_coarse_sampled, rays_o, rays_d, z_vals = ray_marcher(rays[chunk_idx*args.chunk:(chunk_idx+1)*args.chunk],\n",
    "                                                    N_samples=args.N_samples)\n",
    "\n",
    "                # Converting world coordinate to ndc coordinate\n",
    "                H, W = img.shape[:2]\n",
    "                inv_scale = torch.tensor([W - 1, H - 1]).to(device)\n",
    "                w2c_ref, intrinsic_ref = pose_source['w2cs'][0], pose_source['intrinsics'][0].clone()\n",
    "                xyz_NDC = get_ndc_coordinate(w2c_ref, intrinsic_ref, xyz_coarse_sampled, inv_scale,\n",
    "                                             near=near_far_source[0], far=near_far_source[1], pad=pad*args.imgScale_test)\n",
    "\n",
    "\n",
    "                # rendering\n",
    "                \n",
    "                ##################\n",
    "                # time mvsnerf 0\n",
    "                ##################\n",
    "                start_mvsnerf.record()\n",
    "                #\n",
    "                rgb, disp, acc, depth_pred, alpha, extras, records_mvsnerf = rendering(args, pose_source, xyz_coarse_sampled,\n",
    "                                                                       xyz_NDC, z_vals, rays_o, rays_d, records_mvsnerf,\n",
    "                                                                       volume_feature,imgs_source, **render_kwargs_train)\n",
    "                #\n",
    "                end_mvsnerf.record()\n",
    "                torch.cuda.synchronize()\n",
    "                records_mvsnerf['0_total'].append(start_mvsnerf.elapsed_time(end_mvsnerf))\n",
    "                ##############################################################\n",
    "                \n",
    "                \n",
    "\n",
    "                rgb, depth_pred = torch.clamp(rgb.cpu(),0,1.0).numpy(), depth_pred.cpu().numpy()\n",
    "                rgb_rays.append(rgb)\n",
    "                depth_rays_preds.append(depth_pred)\n",
    "                \n",
    "                #\n",
    "                end_loop.record()\n",
    "                torch.cuda.synchronize()\n",
    "                records_general['1_loop'].append(start_loop.elapsed_time(end_loop))\n",
    "                ##############################################################\n",
    "            \n",
    "            #\n",
    "            end_loops.record()\n",
    "            torch.cuda.synchronize()\n",
    "            records_general['2_loops'].append(start_loops.elapsed_time(end_loops))\n",
    "            ##############################################################\n",
    "            \n",
    "            #\n",
    "            end_all.record()\n",
    "            torch.cuda.synchronize()\n",
    "            records_general['0_all'].append(start_all.elapsed_time(end_all))\n",
    "            ##############################################################\n",
    "            \n",
    "            \n",
    "            depth_rays_preds = np.concatenate(depth_rays_preds).reshape(H, W)\n",
    "\n",
    "            depth_gt, _ =  read_depth(f'/mnt/hdd/mvsnerf_data/dtu/Depths/scan{scene}/depth_map_{val_idx[i]:04d}.pfm')\n",
    "\n",
    "            # commented out because prediction and gt shape mismatch\n",
    "#             mask_gt = depth_gt>0\n",
    "#             abs_err = abs_error(depth_rays_preds, depth_gt/200, mask_gt)\n",
    "\n",
    "#             eval_metric = [0.01,0.05, 0.1]\n",
    "#             depth_acc[f'abs_err'][f'{scene}'] = np.mean(abs_err)\n",
    "#             depth_acc[f'acc_l_{eval_metric[0]}'][f'{scene}'] = acc_threshold(abs_err,eval_metric[0]).mean()\n",
    "#             depth_acc[f'acc_l_{eval_metric[1]}'][f'{scene}'] = acc_threshold(abs_err,eval_metric[1]).mean()\n",
    "#             depth_acc[f'acc_l_{eval_metric[2]}'][f'{scene}'] = acc_threshold(abs_err,eval_metric[2]).mean()\n",
    "\n",
    "\n",
    "            depth_rays_preds, _ = visualize_depth_numpy(depth_rays_preds, near_far_source)\n",
    "\n",
    "            rgb_rays = np.concatenate(rgb_rays).reshape(H, W, 3)\n",
    "            img_vis = np.concatenate((img*255,rgb_rays*255,depth_rays_preds),axis=1)\n",
    "            \n",
    "            print(rgb_rays.shape)\n",
    "\n",
    "            if save_as_image:\n",
    "                imageio.imwrite(f'{save_dir}/scan{scene}_{val_idx[i]:03d}.png', img_vis.astype('uint8'))\n",
    "            else:\n",
    "                rgbs.append(img_vis.astype('uint8'))\n",
    "\n",
    "            # quantity\n",
    "            # mask background since they are outside the far boundle\n",
    "            mask = depth==0\n",
    "            imageio.imwrite(f'{save_dir}/scan{scene}_{val_idx[i]:03d}_mask.png', mask.astype('uint8')*255)\n",
    "            rgb_rays[mask],img[mask] = 0.0,0.0\n",
    "            psnr.append( mse2psnr(np.mean((rgb_rays[~mask]-img[~mask])**2)))\n",
    "            ssim.append( structural_similarity(rgb_rays, img, multichannel=True))\n",
    "\n",
    "            img_tensor = torch.from_numpy(rgb_rays)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "            img_gt_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0\n",
    "            LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "        print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "        psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "\n",
    "\n",
    "    if not save_as_image:\n",
    "        imageio.mimwrite(f'{save_dir}/{scene}_spiral.mp4', np.stack(rgbs), fps=20, quality=10)\n",
    "\n",
    "# a = np.mean(list(depth_acc['abs_err'].values()))\n",
    "# b = np.mean(list(depth_acc[f'acc_l_{eval_metric[0]}'].values()))\n",
    "# c = np.mean(list(depth_acc[f'acc_l_{eval_metric[1]}'].values()))\n",
    "# d = np.mean(list(depth_acc[f'acc_l_{eval_metric[2]}'].values()))\n",
    "# print(f'============> abs_err: {a} <=================')\n",
    "# print(f'============> acc_l_{eval_metric[0]}: {b} <=================')\n",
    "# print(f'============> acc_l_{eval_metric[1]}: {c} <=================')\n",
    "# print(f'============> acc_l_{eval_metric[2]}: {d} <=================')\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rendering novel views with fixed 3 source views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate([1,8,21,103,114]):#,\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    cmd = f'--datadir /mnt/data/new_disk/sungx/data/mvs_dataset/DTU/mvs_training/dtu/scan{scene}  \\\n",
    "    --dataset_name dtu_ft  \\\n",
    "    --ckpt ./ckpts//mvsnerf-v0.tar'\n",
    "\n",
    "    args = config_parser(cmd.split())\n",
    "    args.use_viewdirs = True\n",
    "\n",
    "    args.N_samples = 128\n",
    "    args.feat_dim =  8+3*4\n",
    "\n",
    "    # create models\n",
    "    if 0==i_scene:\n",
    "        render_kwargs_train, render_kwargs_test, start, grad_vars = create_nerf_mvs(args, use_mvs=True, dir_embedder=False, pts_embedder=True)\n",
    "        filter_keys(render_kwargs_train)\n",
    "\n",
    "        MVSNet = render_kwargs_train['network_mvs']\n",
    "        render_kwargs_train.pop('network_mvs')\n",
    "\n",
    "\n",
    "    datadir = args.datadir\n",
    "    datatype = 'val'\n",
    "    pad = 24\n",
    "    args.chunk = 5120\n",
    "\n",
    "\n",
    "    print('============> rendering dataset <===================')\n",
    "    dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "    val_idx = dataset.img_idx\n",
    "    \n",
    "    save_as_image = True\n",
    "    save_dir = f'results/test3'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    MVSNet.train()\n",
    "    MVSNet = MVSNet.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)\n",
    "        volume_feature, _, _ = MVSNet(imgs_source, proj_mats, near_far_source, pad=pad)\n",
    "        imgs_source = unpreprocess(imgs_source)\n",
    "\n",
    "        try:\n",
    "            tqdm._instances.clear() \n",
    "        except Exception:     \n",
    "            pass\n",
    "        \n",
    "        for i, batch in enumerate(tqdm(dataset)):\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            rays, img = decode_batch(batch)\n",
    "            rays = rays.squeeze().to(device)  # (H*W, 3)\n",
    "            img = img.squeeze().cpu().numpy()  # (H, W, 3)\n",
    "            depth = batch['depth'].squeeze().numpy()  # (H, W)\n",
    "        \n",
    "            N_rays_all = rays.shape[0]\n",
    "            rgb_rays, depth_rays_preds = [],[]\n",
    "            for chunk_idx in range(N_rays_all//args.chunk + int(N_rays_all%args.chunk>0)):\n",
    "\n",
    "                xyz_coarse_sampled, rays_o, rays_d, z_vals = ray_marcher(rays[chunk_idx*args.chunk:(chunk_idx+1)*args.chunk],\n",
    "                                                    N_samples=args.N_samples)\n",
    "\n",
    "                # Converting world coordinate to ndc coordinate\n",
    "                H, W = img.shape[:2]\n",
    "                inv_scale = torch.tensor([W - 1, H - 1]).to(device)\n",
    "                w2c_ref, intrinsic_ref = pose_source['w2cs'][0], pose_source['intrinsics'][0].clone()\n",
    "                xyz_NDC = get_ndc_coordinate(w2c_ref, intrinsic_ref, xyz_coarse_sampled, inv_scale,\n",
    "                                             near=near_far_source[0], far=near_far_source[1], pad=pad*args.imgScale_test)\n",
    "\n",
    "\n",
    "                # rendering\n",
    "                rgb, disp, acc, depth_pred, alpha, extras = rendering(args, pose_source, xyz_coarse_sampled,\n",
    "                                                                       xyz_NDC, z_vals, rays_o, rays_d,\n",
    "                                                                       volume_feature,imgs_source, **render_kwargs_train)\n",
    "    \n",
    "                \n",
    "                rgb, depth_pred = torch.clamp(rgb.cpu(),0,1.0).numpy(), depth_pred.cpu().numpy()\n",
    "                rgb_rays.append(rgb)\n",
    "                depth_rays_preds.append(depth_pred)\n",
    "\n",
    "            \n",
    "            depth_rays_preds = np.concatenate(depth_rays_preds).reshape(H, W)\n",
    "            depth_rays_preds, _ = visualize_depth_numpy(depth_rays_preds, near_far_source)\n",
    "            \n",
    "            rgb_rays = np.concatenate(rgb_rays).reshape(H, W, 3)\n",
    "            img_vis = np.concatenate((img*255,rgb_rays*255,depth_rays_preds),axis=1)\n",
    "            \n",
    "            if save_as_image:\n",
    "                imageio.imwrite(f'{save_dir}/scan{scene}_{val_idx[i]:03d}.png', img_vis.astype('uint8'))\n",
    "            else:\n",
    "                rgbs.append(img_vis.astype('uint8'))\n",
    "                \n",
    "            # quantity\n",
    "            # mask background since they are outside the far boundle\n",
    "            mask = depth==0\n",
    "            imageio.imwrite(f'{save_dir}/scan{scene}_{val_idx[i]:03d}_mask.png', mask.astype('uint8')*255)\n",
    "            rgb_rays[mask],img[mask] = 0.0,0.0\n",
    "            psnr.append( mse2psnr(np.mean((rgb_rays[~mask]-img[~mask])**2)))\n",
    "            ssim.append( structural_similarity(rgb_rays, img, multichannel=True))\n",
    "            \n",
    "            img_tensor = torch.from_numpy(rgb_rays)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "            img_gt_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0\n",
    "            LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "        print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "        psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "\n",
    "    if not save_as_image:\n",
    "        imageio.mimwrite(f'{save_dir}/{scene}_spiral.mp4', np.stack(rgbs), fps=20, quality=10)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairs generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,torch\n",
    "import sys,os\n",
    "import numpy as np\n",
    "root = '/home/hengfei/Desktop/research/mvsnerf'\n",
    "os.chdir(root)\n",
    "sys.path.append(root)\n",
    "pairs = torch.load('./configs/pairs.th')\n",
    "\n",
    "# llff\n",
    "root_dir = '/home/hengfei/Desktop/research/mvsnerf/xgaze/'\n",
    "for scene in ['xgaze_11images_cropped_colmapCODE']:#\n",
    "    poses_bounds = np.load(os.path.join(root_dir, scene, 'poses_bounds.npy'))  # (N_images, 11)\n",
    "    poses = poses_bounds[:, :15].reshape(-1, 3, 5)  # (N_images, 3, 5)\n",
    "    poses = np.concatenate([poses[..., 1:2], - poses[..., :1], poses[..., 2:4]], -1)\n",
    "\n",
    "    ref_position = np.mean(poses[..., 3],axis=0, keepdims=True)\n",
    "    dist = np.sum(np.abs(poses[..., 3] - ref_position), axis=-1)\n",
    "    pair_idx = np.argsort(dist)[:11]\n",
    "#     pair_idx = torch.randperm(len(poses))[:20].tolist()\n",
    "\n",
    "    pairs[f'{scene}_test'] = pair_idx[::6]\n",
    "    pairs[f'{scene}_val'] = pair_idx[::6]\n",
    "    pairs[f'{scene}_train'] = np.delete(pair_idx, range(0,11,6))\n",
    "\n",
    "torch.save(pairs,'/home/hengfei/Desktop/research/mvsnerf/configs/pairs.th')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nerf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> scene: chair mean psnr 31.070870959993407 ssim: 0.970869913728838 lpips: 0.05512496456503868\n",
      "=====> scene: drums mean psnr 25.464383523724557 ssim: 0.9430287321705997 lpips: 0.1010842639952898\n",
      "=====> scene: ficus mean psnr 29.72717081186501 ssim: 0.9688198661712594 lpips: 0.04721927270293236\n",
      "=====> scene: hotdog mean psnr 34.63162021512352 ssim: 0.9798700143526428 lpips: 0.0885334312915802\n",
      "=====> scene: lego mean psnr 32.65761069614622 ssim: 0.9751430050524844 lpips: 0.05375238787382841\n",
      "=====> scene: materials mean psnr 30.220202654922936 ssim: 0.9677394226502894 lpips: 0.1052329633384943\n",
      "=====> scene: mic mean psnr 31.810551677509977 ssim: 0.9810118386928188 lpips: 0.03268271638080478\n",
      "=====> scene: ship mean psnr 29.487980342358682 ssim: 0.9079920156014059 lpips: 0.2625834122300148\n",
      "=====> all mean psnr 30.633798860205538 ssim: 0.9618093510525423 lpips: 0.09327667654724792\n",
      "=====> scene: fern mean psnr 23.87081932481545 ssim: 0.828319405500272 lpips: 0.29106350988149643\n",
      "=====> scene: flower mean psnr 26.84248375485232 ssim: 0.8972176342834637 lpips: 0.175886869430542\n",
      "=====> scene: fortress mean psnr 31.368287455491632 ssim: 0.9454387223441942 lpips: 0.14687807857990265\n",
      "=====> scene: horns mean psnr 25.957297279910254 ssim: 0.9002932801372645 lpips: 0.24746065214276314\n",
      "=====> scene: leaves mean psnr 21.209230306630403 ssim: 0.7924383925048237 lpips: 0.3013022392988205\n",
      "=====> scene: orchids mean psnr 19.805083676014405 ssim: 0.7216119459229409 lpips: 0.3210122361779213\n",
      "=====> scene: room mean psnr 33.538893189163886 ssim: 0.9782926576909046 lpips: 0.15664737671613693\n",
      "=====> scene: trex mean psnr 25.191124840989897 ssim: 0.8986906362165991 lpips: 0.24522138014435768\n",
      "=====> all mean psnr 25.972902478483533 ssim: 0.8702878343250579 lpips: 0.23568404279649258\n",
      "=====> scene: 1 mean psnr 26.621467948629075 ssim: 0.9015109955368615 lpips: 0.26542308926582336\n",
      "=====> scene: 8 mean psnr 28.331380911917446 ssim: 0.8758834032089559 lpips: 0.32112571597099304\n",
      "=====> scene: 21 mean psnr 23.238617049537122 ssim: 0.8736486067576469 lpips: 0.2456555962562561\n",
      "=====> scene: 103 mean psnr 30.40125554666663 ssim: 0.9442875531229784 lpips: 0.25619567558169365\n",
      "=====> scene: 114 mean psnr 26.46890004323232 ssim: 0.9134870027630088 lpips: 0.2253187969326973\n",
      "=====> all mean psnr 27.01232429999652 ssim: 0.9017635122778904 lpips: 0.2627437748014927\n"
     ]
    }
   ],
   "source": [
    "root = '/mnt/new_disk_2/anpei/code/nerf/logs/'\n",
    "root_gt = '/mnt/new_disk2/anpei/code/MVS-NeRF/runs_fine_tuning/'\n",
    "pairs = torch.load('/mnt/new_disk_2/anpei/code/MVS-NeRF/configs/pairs.th')\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['chair','drums','ficus','hotdog','lego','materials','mic','ship']):#\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    files = sorted(glob.glob(f'{root}/{scene}_test/testset_200000/*.png'))\n",
    "    for j, file in enumerate(files):\n",
    "\n",
    "        idx = pairs[f'{scene}_val'][j]\n",
    "        img = cv2.imread(file)[...,::-1]\n",
    "        gt = cv2.imread(f'{root_gt}/{scene}/{scene}/{scene}_00009999_{j:02d}.png')[...,::-1]\n",
    "        gt, img = gt[:,:800].astype('float')/255.0, img.astype('float')/255.0\n",
    "\n",
    "#         H_crop, W_crop = np.array(gt.shape[:2])//10\n",
    "#         img = img[H_crop:-H_crop,W_crop:-W_crop]\n",
    "#         gt = gt[H_crop:-H_crop,W_crop:-W_crop]\n",
    "\n",
    "        psnr.append( mse2psnr(np.mean((gt-img)**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') \n",
    "\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['fern','flower','fortress','horns','leaves', 'orchids', 'room',  'trex']):#,\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    files = sorted(glob.glob(f'{root}/{scene}_test/testset_200000/*.png'))\n",
    "    for j, file in enumerate(files):\n",
    "\n",
    "        idx = pairs[f'{scene}_val'][j]\n",
    "        img = cv2.resize(cv2.imread(file)[...,::-1],(960,640))\n",
    "        gt =  cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/{scene}_{idx:03d}.png')[...,::-1]\n",
    "        gt, img = gt[:,:960].astype('float')/255.0, img.astype('float')/255.0\n",
    "\n",
    "\n",
    "        psnr.append( mse2psnr(np.mean((gt-img)**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') \n",
    "\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate([1,8,21,103,114]):#,\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    files = sorted(glob.glob(f'{root}/scan{scene}_test/testset_200000/*.png'))\n",
    "    for j, file in enumerate(files):\n",
    "\n",
    "        idx = pairs[f'dtu_val'][j]\n",
    "        img = cv2.imread(file)[...,::-1]\n",
    "        gt = cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/scan{scene}_{idx:03d}.png')[...,::-1]\n",
    "        mask = cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/scan{scene}_{idx:03d}_mask.png')==255\n",
    "        gt, img = gt[:,:640].astype('float')/255.0, img.astype('float')/255.0\n",
    "\n",
    "        gt[mask],img[mask] = 0.0,0.0\n",
    "        psnr.append( mse2psnr(np.mean((gt[~mask]-img[~mask])**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> scene: chair mean psnr 26.746873810513947 ssim: 0.9321234340308168 lpips: 0.15475058555603027\n",
      "=====> scene: drums mean psnr 22.28406117543553 ssim: 0.8964614019632969 lpips: 0.21542910858988762\n",
      "=====> scene: ficus mean psnr 26.365789669973488 ssim: 0.9438413276712496 lpips: 0.15914445742964745\n",
      "=====> scene: hotdog mean psnr 32.489636248742805 ssim: 0.9699785599978382 lpips: 0.11295554973185062\n",
      "=====> scene: lego mean psnr 26.79832336361502 ssim: 0.9245065827858229 lpips: 0.18708691000938416\n",
      "=====> scene: materials mean psnr 24.957611270986945 ssim: 0.9249186604651752 lpips: 0.1740873008966446\n",
      "=====> scene: mic mean psnr 29.449610622444368 ssim: 0.9693072200690339 lpips: 0.092950988560915\n",
      "=====> scene: ship mean psnr 26.60832366062154 ssim: 0.8780999869891254 lpips: 0.28621142730116844\n",
      "=====> all mean psnr 26.962528727791707 ssim: 0.9299046467465449 lpips: 0.17282704100944102\n",
      "=====> scene: fern mean psnr 22.61357364768159 ssim: 0.77000724312094 lpips: 0.2827577739953995\n",
      "=====> scene: flower mean psnr 25.52052448547126 ssim: 0.8816617628340061 lpips: 0.19497620686888695\n",
      "=====> scene: fortress mean psnr 28.010539767191638 ssim: 0.8958878940094137 lpips: 0.19440287351608276\n",
      "=====> scene: horns mean psnr 24.99889079936664 ssim: 0.8806854873247154 lpips: 0.2329558990895748\n",
      "=====> scene: leaves mean psnr 21.228759476443667 ssim: 0.8063488185998933 lpips: 0.24219803884625435\n",
      "=====> scene: orchids mean psnr 19.460068266904813 ssim: 0.7087825176683923 lpips: 0.3091204948723316\n",
      "=====> scene: room mean psnr 29.150237317214977 ssim: 0.9570667630128395 lpips: 0.16276488453149796\n",
      "=====> scene: trex mean psnr 24.079128145103503 ssim: 0.8873396265555692 lpips: 0.2028873674571514\n",
      "=====> all mean psnr 24.38271523817226 ssim: 0.8484725141407212 lpips: 0.22775794239714742\n",
      "=====> scene: 1 mean psnr 28.046498782015014 ssim: 0.9341022735743112 lpips: 0.17119702696800232\n",
      "=====> scene: 8 mean psnr 28.875841987497292 ssim: 0.8999499891440654 lpips: 0.26092011481523514\n",
      "=====> scene: 21 mean psnr 24.870061827644722 ssim: 0.9215085855864712 lpips: 0.14212623238563538\n",
      "=====> scene: 103 mean psnr 32.226849694259215 ssim: 0.9638182025610383 lpips: 0.16953438520431519\n",
      "=====> scene: 114 mean psnr 28.46625266665073 ssim: 0.9451965364044997 lpips: 0.15322893112897873\n",
      "=====> all mean psnr 28.497100991613394 ssim: 0.9329151174540771 lpips: 0.17940133810043335\n"
     ]
    }
   ],
   "source": [
    "root = '/mnt/new_disk2/anpei/code/MVS-NeRF/runs_fine_tuning/'\n",
    "pairs = torch.load('/mnt/new_disk_2/anpei/code/MVS-NeRF/configs/pairs.th')\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['chair','drums','ficus','hotdog','lego','materials','mic','ship']):\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    files = sorted(glob.glob(f'{root}/{scene}/{scene}/{scene}_00009999_*'))\n",
    "    for j, file in enumerate(files):\n",
    "\n",
    "        idx = pairs[f'{scene}_val'][j]\n",
    "        img = cv2.imread(file).astype('float')[...,::-1]\n",
    "        gt, img = img[:,:800]/255.0, img[:,800:1600]/255.0\n",
    "\n",
    "#         H_crop, W_crop = np.array(gt.shape[:2])//10\n",
    "#         img = img[H_crop:-H_crop,W_crop:-W_crop]\n",
    "#         gt = gt[H_crop:-H_crop,W_crop:-W_crop]\n",
    "\n",
    "        psnr.append( mse2psnr(np.mean((gt-img)**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') \n",
    "\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['fern','flower','fortress','horns','leaves', 'orchids', 'room',  'trex']):#,\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    files = sorted(glob.glob(f'/mnt/new_disk2/anpei/code/MVS-NeRF/runs_fine_tuning/{scene}/{scene}/{scene}_00009999_*'))\n",
    "    for j, file in enumerate(files):\n",
    "\n",
    "        idx = pairs[f'{scene}_val'][j]\n",
    "        img = cv2.imread(file)[...,::-1]\n",
    "        gt, img = img[:,:960].astype('float')/255.0, img[:,960:960*2].astype('float')/255.0\n",
    "\n",
    "        H_crop, W_crop = np.array(gt.shape[:2])//10\n",
    "        img = img[H_crop:-H_crop,W_crop:-W_crop]\n",
    "        gt = gt[H_crop:-H_crop,W_crop:-W_crop]\n",
    "\n",
    "\n",
    "        psnr.append( mse2psnr(np.mean((gt-img)**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') \n",
    "\n",
    "root = '/mnt/new_disk2/anpei/code/MVS-NeRF/runs_fine_tuning/'\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate([1,8,21,103,114]):#,\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    files = sorted(glob.glob(f'{root}/dtu_scan{scene}_1h/dtu_scan{scene}_1h/00010239_*'))\n",
    "    for j, file in enumerate(files):\n",
    "\n",
    "        idx = pairs[f'dtu_val'][j]\n",
    "        img = cv2.imread(file)[...,::-1][:,640:1280]\n",
    "        gt = cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/scan{scene}_{idx:03d}.png')[...,::-1]\n",
    "        mask = cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/scan{scene}_{idx:03d}_mask.png')==255\n",
    "        gt, img = gt[:,:640].astype('float')/255.0, img.astype('float')/255.0\n",
    "\n",
    "        gt[mask],img[mask] = 0.0,0.0\n",
    "        psnr.append( mse2psnr(np.mean((gt[~mask]-img[~mask])**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ibrnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> scene: fern mean psnr 22.64474646040451 ssim: 0.7736232480476191 lpips: 0.26588304713368416\n",
      "=====> scene: flower mean psnr 26.553349019087786 ssim: 0.9092690161984827 lpips: 0.14575103670358658\n",
      "=====> scene: fortress mean psnr 30.338842953903075 ssim: 0.9368867837660259 lpips: 0.13289865292608738\n",
      "=====> scene: horns mean psnr 25.01290939681414 ssim: 0.9040335882553917 lpips: 0.1899307444691658\n",
      "=====> scene: leaves mean psnr 22.076508076698556 ssim: 0.8430354849586478 lpips: 0.17987846583127975\n",
      "=====> scene: orchids mean psnr 19.007830032899616 ssim: 0.7045611776629173 lpips: 0.2861044891178608\n",
      "=====> scene: room mean psnr 31.05473820815669 ssim: 0.9723299877991765 lpips: 0.08911459799855947\n",
      "=====> scene: trex mean psnr 22.339864946223464 ssim: 0.8421255627008343 lpips: 0.22207806631922722\n",
      "=====> all mean psnr 24.878598636773482 ssim: 0.8607331061736369 lpips: 0.1889548875624314\n",
      "=====> scene: 1 mean psnr 30.99564992655386 ssim: 0.9548394719193786 lpips: 0.1285402663052082\n",
      "=====> scene: 8 mean psnr 32.46173840309124 ssim: 0.9445782574823819 lpips: 0.16979694738984108\n",
      "=====> scene: 21 mean psnr 27.88178725277648 ssim: 0.9467770144187784 lpips: 0.1040295660495758\n",
      "=====> scene: 103 mean psnr 34.399916992709706 ssim: 0.9675776122666799 lpips: 0.15563546121120453\n",
      "=====> scene: 114 mean psnr 31.00119644953211 ssim: 0.9638488318305859 lpips: 0.09874588809907436\n",
      "=====> all mean psnr 31.348057804932676 ssim: 0.955524237583561 lpips: 0.1313496258109808\n"
     ]
    }
   ],
   "source": [
    "# root = '/mnt/new_disk2/anpei/code/IBRNet/logs'\n",
    "pairs = torch.load('/mnt/new_disk_2/anpei/code/MVS-NeRF/configs/pairs.th')\n",
    "# psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "# for i_scene, scene in enumerate(['chair','drums','ficus','hotdog','lego','materials','mic','ship']):#,\n",
    "#     psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "#     files = sorted(glob.glob(f'{root}/nerf-3view-finetuning-nearest-{scene}/010000_*'))\n",
    "#     for j, file in enumerate(files):\n",
    "\n",
    "#         idx = pairs[f'{scene}_val'][j]\n",
    "#         img = cv2.imread(file).astype('float')[...,::-1]\n",
    "#         gt, img = img[:,800:800*2]/255.0, img[:,800*3:800*4]/255.0\n",
    "\n",
    "# #         H_crop, W_crop = np.array(gt.shape[:2])//10\n",
    "# #         img = img[H_crop:-H_crop,W_crop:-W_crop]\n",
    "# #         gt = gt[H_crop:-H_crop,W_crop:-W_crop]\n",
    "\n",
    "#         psnr.append( mse2psnr(np.mean((gt-img)**2)))\n",
    "#         ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "#         img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "#         img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "#         LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "#     print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "#     psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "# print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') \n",
    "\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['fern','flower','fortress','horns','leaves', 'orchids', 'room',  'trex']):#\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    files = sorted(glob.glob(f'/mnt/new_disk2/anpei/code/IBRNet/logs/llff-3view-finetuning-nearest-{scene}/010000_*'))\n",
    "    for j, file in enumerate(files):\n",
    "\n",
    "        idx = pairs[f'{scene}_val'][j]\n",
    "        img = cv2.imread(file)[...,::-1]\n",
    "        gt, img = img[:,1008:1008*2].astype('float')/255.0, img[:,1008*3:1008*4].astype('float')/255.0\n",
    "        img, gt = cv2.resize(img,(960,640)), cv2.resize(gt,(960,640))\n",
    "\n",
    "        H_crop, W_crop = np.array(gt.shape[:2])//10\n",
    "        img = img[H_crop:-H_crop,W_crop:-W_crop]\n",
    "        gt = gt[H_crop:-H_crop,W_crop:-W_crop]\n",
    "\n",
    "        \n",
    "        psnr.append( mse2psnr(np.mean((gt-img)**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') \n",
    "\n",
    "root = '/mnt/new_disk2/anpei/code/IBRNet/logs'\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate([1,8,21,103,114]):#\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    files = sorted(glob.glob(f'{root}/dtu-3view-finetuning-nearest-scan{scene}/010000_*'))\n",
    "    for j, file in enumerate(files):\n",
    "\n",
    "        idx = pairs[f'dtu_val'][j]\n",
    "        img = cv2.imread(file)[...,::-1][:,3*640:4*640]\n",
    "        gt = cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/scan{scene}_{idx:03d}.png')[...,::-1]\n",
    "        mask = cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/scan{scene}_{idx:03d}_mask.png')==255\n",
    "        gt, img = gt[:,:640].astype('float')/255.0, img.astype('float')/255.0\n",
    "\n",
    "        gt[mask],img[mask] = 0.0,0.0\n",
    "        psnr.append( mse2psnr(np.mean((gt[~mask]-img[~mask])**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pixel nerf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> scene: chair mean psnr 7.175962813343725 ssim: 0.6243642351905847 lpips: 0.38591109961271286\n",
      "=====> scene: drums mean psnr 8.148548711878252 ssim: 0.6701584468514097 lpips: 0.42121122032403946\n",
      "=====> scene: ficus mean psnr 6.608732738834844 ssim: 0.668716265099144 lpips: 0.3350602239370346\n",
      "=====> scene: hotdog mean psnr 6.799387670799135 ssim: 0.6689815218041557 lpips: 0.43327029794454575\n",
      "=====> scene: lego mean psnr 7.740217521658803 ssim: 0.6710903029993184 lpips: 0.42670799791812897\n",
      "=====> scene: materials mean psnr 7.609290420358684 ssim: 0.6441046576733512 lpips: 0.43245941400527954\n",
      "=====> scene: mic mean psnr 7.707203698223274 ssim: 0.7294597852809476 lpips: 0.32929887622594833\n",
      "=====> scene: ship mean psnr 7.295484760785579 ssim: 0.5836685948507447 lpips: 0.5257005095481873\n",
      "=====> all mean psnr 7.385603541985287 ssim: 0.657567976218707 lpips: 0.4112024549394846\n",
      "=====> scene: fern mean psnr 12.397648684821284 ssim: 0.5312397318110376 lpips: 0.6500117480754852\n",
      "=====> scene: flower mean psnr 9.99675489427281 ssim: 0.43323656344453193 lpips: 0.7075561136007309\n",
      "=====> scene: fortress mean psnr 14.073488262986546 ssim: 0.6736649368929569 lpips: 0.6075489073991776\n",
      "=====> scene: horns mean psnr 11.71002161685521 ssim: 0.5157559834106309 lpips: 0.705190509557724\n",
      "=====> scene: leaves mean psnr 9.847068575637605 ssim: 0.2681360856716045 lpips: 0.6947661340236664\n",
      "=====> scene: orchids mean psnr 9.624184850140201 ssim: 0.3168018322171233 lpips: 0.7207814902067184\n",
      "=====> scene: room mean psnr 11.750716240417157 ssim: 0.6906632306577324 lpips: 0.6112178564071655\n",
      "=====> scene: trex mean psnr 10.55211637013632 ssim: 0.45770187915676297 lpips: 0.6672322899103165\n",
      "=====> all mean psnr 11.243999936908391 ssim: 0.48590003040779756 lpips: 0.6705381311476231\n",
      "=====> scene: 1 mean psnr 21.64330896303546 ssim: 0.8268906240371169 lpips: 0.3728666678071022\n",
      "=====> scene: 8 mean psnr 23.69860813191727 ssim: 0.8291247579664462 lpips: 0.38369619846343994\n",
      "=====> scene: 21 mean psnr 16.03916045790063 ssim: 0.6905614284959526 lpips: 0.4074021577835083\n",
      "=====> scene: 103 mean psnr 16.75554527345504 ssim: 0.8356182752116726 lpips: 0.3762983977794647\n",
      "=====> scene: 114 mean psnr 18.403335481905653 ssim: 0.7632868039029544 lpips: 0.371926873922348\n",
      "=====> all mean psnr 19.30799166164281 ssim: 0.7890963779228286 lpips: 0.3824380591511726\n"
     ]
    }
   ],
   "source": [
    "root = '/mnt/new_disk2/anpei/code/pixel-nerf/visuals/dtu'\n",
    "root_gt = '/mnt/new_disk2/anpei/code/MVS-NeRF/runs_fine_tuning/'\n",
    "pairs = torch.load('/mnt/new_disk_2/anpei/code/MVS-NeRF/configs/pairs.th')\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['chair','drums','ficus','hotdog','lego','materials','mic','ship']):#,\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    pairs_idx = pairs[f'{scene}_val']\n",
    "    for j, file in enumerate(pairs_idx):\n",
    "\n",
    "        idx = pairs_idx[j]\n",
    "        img = cv2.imread(f'{root}/{scene}_{idx:03d}.png')[...,::-1]\n",
    "        gt = cv2.imread(f'{root_gt}/{scene}/{scene}/{scene}_00009999_{j:02d}.png')[...,::-1]\n",
    "        gt, img = gt[:,:800].astype('float')/255.0, img.astype('float')/255.0\n",
    "\n",
    "#         H_crop, W_crop = np.array(gt.shape[:2])//10\n",
    "#         img = img[H_crop:-H_crop,W_crop:-W_crop]\n",
    "#         gt = gt[H_crop:-H_crop,W_crop:-W_crop]\n",
    "\n",
    "        psnr.append( mse2psnr(np.mean((gt-img)**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') \n",
    "\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['fern','flower','fortress','horns','leaves', 'orchids', 'room',  'trex']):#,\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    pairs_idx = pairs[f'{scene}_val']\n",
    "    for j, file in enumerate(pairs_idx):\n",
    "\n",
    "        idx = pairs_idx[j]\n",
    "        img = cv2.resize(cv2.imread(f'{root}/{scene}_{idx:03d}.png')[...,::-1],(960,640))\n",
    "        gt =  cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/{scene}_{idx:03d}.png')[...,::-1]\n",
    "        gt, img = gt[:,:960].astype('float')/255.0, img.astype('float')/255.0\n",
    "\n",
    "\n",
    "        psnr.append( mse2psnr(np.mean((gt-img)**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') \n",
    "\n",
    "psnr_all,ssim_all,LPIPS_vgg_all,depth_acc = [],[],[],[]\n",
    "for i_scene, scene in enumerate([1,8,21,103,114]):#,\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    pairs_idx = pairs[f'dtu_val']\n",
    "    for j, file in enumerate(pairs_idx):\n",
    "\n",
    "        idx = pairs_idx[j]\n",
    "        img = cv2.imread(f'{root}/scan{scene}_{idx:03d}.png')[...,::-1]\n",
    "        \n",
    "        gt = cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/scan{scene}_{idx:03d}.png')[...,::-1]\n",
    "        mask = cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/scan{scene}_{idx:03d}_mask.png')==255\n",
    "        gt, img = gt[:,:640].astype('float')/255.0, img.astype('float')/255.0\n",
    "\n",
    "        gt[mask],img[mask] = 0.0,0.0\n",
    "        psnr.append( mse2psnr(np.mean((gt[~mask]-img[~mask])**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "        \n",
    "        # depth\n",
    "#         depth_pred = torch.load(f'{root}/scan{scene}_{idx:03d}_depth.th')\n",
    "#         depth_gt,_ =  read_depth(f'/mnt/data/new_disk/sungx/data/mvs_dataset/DTU/mvs_training/dtu/Depths/scan{scene}/depth_map_{idx:04d}.pfm')\n",
    "        \n",
    "#         mask_gt = depth_gt>0\n",
    "#         abs_err = abs_error(depth_pred*1.5, depth_gt/200, mask_gt).numpy()\n",
    "\n",
    "#         eval_metric = [0.01,0.05, 0.1]\n",
    "#         depth_acc[f'abs_err'][f'{scene}'] = np.mean(abs_err)\n",
    "#         depth_acc[f'acc_l_{eval_metric[0]}'][f'{scene}'] = acc_threshold(abs_err,eval_metric[0]).mean()\n",
    "#         depth_acc[f'acc_l_{eval_metric[1]}'][f'{scene}'] = acc_threshold(abs_err,eval_metric[1]).mean()\n",
    "#         depth_acc[f'acc_l_{eval_metric[2]}'][f'{scene}'] = acc_threshold(abs_err,eval_metric[2]).mean()\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
