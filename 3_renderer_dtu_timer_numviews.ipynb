{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os,imageio,lpips\n",
    "root = '/home/youngsun/documents/mvs/mvsnerf_timing'\n",
    "os.chdir(root)\n",
    "sys.path.append(root)\n",
    "\n",
    "from opt import config_parser\n",
    "from data import dataset_dict\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# models\n",
    "from models_timer import *\n",
    "from renderer_timer import *\n",
    "from data.ray_utils import get_rays\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from skimage.metrics import structural_similarity\n",
    "\n",
    "# pytorch-lightning\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning import LightningModule, Trainer, loggers\n",
    "\n",
    "\n",
    "from data.ray_utils import ray_marcher\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/envs/mvsnerf/lib/python3.8/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    }
   ],
   "source": [
    "def decode_batch(batch):\n",
    "    rays = batch['rays']  # (B, 8)\n",
    "    rgbs = batch['rgbs']  # (B, 3)\n",
    "    return rays, rgbs\n",
    "\n",
    "def unpreprocess(data, shape=(1,1,3,1,1)):\n",
    "    # to unnormalize image for visualization\n",
    "    # data N V C H W\n",
    "    device = data.device\n",
    "    mean = torch.tensor([-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225]).view(*shape).to(device)\n",
    "    std = torch.tensor([1 / 0.229, 1 / 0.224, 1 / 0.225]).view(*shape).to(device)\n",
    "\n",
    "    return (data - mean) / std\n",
    "\n",
    "def read_depth(filename):\n",
    "    depth_h = np.array(read_pfm(filename)[0], dtype=np.float32) # (800, 800)\n",
    "    depth_h = cv2.resize(depth_h, None, fx=0.5, fy=0.5,\n",
    "                       interpolation=cv2.INTER_NEAREST)  # (600, 800)\n",
    "    depth_h = depth_h[44:556, 80:720]  # (512, 640)\n",
    "#     depth = cv2.resize(depth_h, None, fx=0.5, fy=0.5,interpolation=cv2.INTER_NEAREST)#!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    mask = depth>0\n",
    "    return depth_h,mask\n",
    "\n",
    "loss_fn_vgg = lpips.LPIPS(net='vgg') \n",
    "mse2psnr = lambda x : -10. * np.log(x) / np.log(10.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quantity evauation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os,imageio,lpips,cv2,torch,glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/envs/mvsnerf/lib/python3.8/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    }
   ],
   "source": [
    "loss_fn_vgg = lpips.LPIPS(net='vgg') \n",
    "mse2psnr = lambda x : -10. * np.log(x) / np.log(10.)\n",
    "\n",
    "\n",
    "\n",
    "def acc_threshold(abs_err, threshold):\n",
    "    \"\"\"\n",
    "    computes the percentage of pixels whose depth error is less than @threshold\n",
    "    \"\"\"\n",
    "    acc_mask = abs_err < threshold\n",
    "    return  acc_mask.astype('float') if type(abs_err) is np.ndarray else acc_mask.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTU no fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rendering novel views with nearest 3 views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_and_record(records_processed, name='test', scenes=[1], num_src=4, img_scale=1.0, save_as_image=True):\n",
    "    \n",
    "    psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "    \n",
    "    for i_scene, scene in enumerate(scenes):#,8,21,103,114\n",
    "        \n",
    "        psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "        \n",
    "        # create timing variables\n",
    "\n",
    "        # measure time - all processes\n",
    "        start_all = torch.cuda.Event(enable_timing=True)\n",
    "        end_all = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        # measure time - Feature extraction and neural volume encoding\n",
    "        start_mvsnet = torch.cuda.Event(enable_timing=True)\n",
    "        end_mvsnet = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        # measure time - MVSNeRF and volume rendering\n",
    "        start_mvsnerf = torch.cuda.Event(enable_timing=True)\n",
    "        end_mvsnerf = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        # measure time - MVSNeRF and volume rendering loop\n",
    "        start_loop = torch.cuda.Event(enable_timing=True)\n",
    "        end_loop = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        # measure time - MVSNeRF and volume rendering all loops\n",
    "        start_loops = torch.cuda.Event(enable_timing=True)\n",
    "        end_loops = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        # for combining all recorded time\n",
    "        records = []\n",
    "        records_general = {\"0_all\" : [], \"1_loop\" : [], \"2_loops\" : []}\n",
    "        records_mvsnet = {\"0_total\" : [], \"1_feat\" : [], \"2_costvol\" : [], \"3_3dcnn\" : []}\n",
    "        records_mvsnerf = {\"0_total\" : [],\"1_sample\" : [], \"2_nerf\" : [], \"3_rend\" : []}\n",
    "\n",
    "        cmd = f'--datadir /mnt/hdd/mvsnerf_data/dtu/scan{scene}  \\\n",
    "         --dataset_name dtu_ft  \\\n",
    "         --net_type v0 --ckpt ./ckpts/mvsnerf-v0.tar \\\n",
    "         --imgScale_train {img_scale} --imgScale_test {img_scale} --img_downscale {img_scale}'\n",
    "\n",
    "        args = config_parser(cmd.split())\n",
    "        args.use_viewdirs = True\n",
    "\n",
    "        args.N_samples = 128\n",
    "        args.feat_dim =  8+4*3\n",
    "\n",
    "        # create models\n",
    "        if 0==i_scene:\n",
    "            render_kwargs_train, render_kwargs_test, start, grad_vars = create_nerf_mvs(args, use_mvs=True, dir_embedder=False, pts_embedder=True)\n",
    "            filter_keys(render_kwargs_train)\n",
    "\n",
    "            MVSNet = render_kwargs_train['network_mvs']\n",
    "            render_kwargs_train.pop('network_mvs')\n",
    "\n",
    "\n",
    "        datadir = args.datadir\n",
    "        datatype = 'train'\n",
    "        pad = 16\n",
    "        args.chunk = 5120\n",
    "\n",
    "        dataset_train = dataset_dict[args.dataset_name](args, split='train')\n",
    "        dataset_val = dataset_dict[args.dataset_name](args, split='val')\n",
    "        val_idx = dataset_val.img_idx\n",
    "\n",
    "        save_as_image = True\n",
    "        save_dir = f'results/test_dtu_{name}'\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        MVSNet.train()\n",
    "        MVSNet = MVSNet.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "#             try:\n",
    "#                 tqdm._instances.clear() \n",
    "#             except Exception:     \n",
    "#                 pass       \n",
    "\n",
    "#             for i, batch in enumerate(tqdm(dataset_val)):\n",
    "            for i, batch in enumerate(dataset_val):\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                ##################\n",
    "                # time everything\n",
    "                ##################\n",
    "                start_all.record()\n",
    "                #\n",
    "\n",
    "                rays, img = decode_batch(batch)\n",
    "                rays = rays.squeeze().to(device)  # (H*W, 3)\n",
    "                img = img.squeeze().cpu().numpy()  # (H, W, 3)\n",
    "                depth = batch['depth'].squeeze().numpy()  # (H, W)\n",
    "\n",
    "                # find nearest image idx from training views\n",
    "                positions = dataset_train.poses[:,:3,3]\n",
    "                dis = np.sum(np.abs(positions - dataset_val.poses[[i],:3,3]), axis=-1)\n",
    "                pair_idx = np.argsort(dis)[:num_src]\n",
    "                pair_idx = [dataset_train.img_idx[item] for item in pair_idx]\n",
    "\n",
    "                imgs_source, proj_mats, near_far_source, pose_source = dataset_train.read_source_views(pair_idx=pair_idx,device=device)\n",
    "                \n",
    "                ##################\n",
    "                # time mvsnet 0\n",
    "                ##################\n",
    "                start_mvsnet.record()\n",
    "                #\n",
    "                volume_feature, _, _, records_mvsnet = MVSNet(imgs_source, proj_mats, near_far_source, records=records_mvsnet, pad=pad)\n",
    "                #\n",
    "                end_mvsnet.record()\n",
    "                torch.cuda.synchronize()\n",
    "                records_mvsnet['0_total'].append(start_mvsnet.elapsed_time(end_mvsnet))\n",
    "                ##############################################################\n",
    "\n",
    "\n",
    "                imgs_source = unpreprocess(imgs_source)\n",
    "\n",
    "                N_rays_all = rays.shape[0]\n",
    "                rgb_rays, depth_rays_preds = [],[]\n",
    "\n",
    "                ##################\n",
    "                # time loops\n",
    "                ##################\n",
    "                start_loops.record()\n",
    "                #\n",
    "\n",
    "                for chunk_idx in range(N_rays_all//args.chunk + int(N_rays_all%args.chunk>0)):\n",
    "\n",
    "                    # for loop timing\n",
    "                    ##################\n",
    "                    # time loop\n",
    "                    ##################\n",
    "                    start_loop.record()\n",
    "                    #\n",
    "                    xyz_coarse_sampled, rays_o, rays_d, z_vals = ray_marcher(rays[chunk_idx*args.chunk:(chunk_idx+1)*args.chunk],\n",
    "                                                        N_samples=args.N_samples)\n",
    "\n",
    "                    # Converting world coordinate to ndc coordinate\n",
    "                    H, W = img.shape[:2]\n",
    "                    inv_scale = torch.tensor([W - 1, H - 1]).to(device)\n",
    "                    w2c_ref, intrinsic_ref = pose_source['w2cs'][0], pose_source['intrinsics'][0].clone()\n",
    "                    xyz_NDC = get_ndc_coordinate(w2c_ref, intrinsic_ref, xyz_coarse_sampled, inv_scale,\n",
    "                                                 near=near_far_source[0], far=near_far_source[1], pad=pad*args.imgScale_test)\n",
    "\n",
    "\n",
    "                    # rendering\n",
    "\n",
    "                    ##################\n",
    "                    # time mvsnerf 0\n",
    "                    ##################\n",
    "                    start_mvsnerf.record()\n",
    "                    #\n",
    "                    rgb, disp, acc, depth_pred, alpha, extras, records_mvsnerf = rendering(args, pose_source, xyz_coarse_sampled,\n",
    "                                                                           xyz_NDC, z_vals, rays_o, rays_d, records_mvsnerf,\n",
    "                                                                           volume_feature,imgs_source, **render_kwargs_train)\n",
    "                    #\n",
    "                    end_mvsnerf.record()\n",
    "                    torch.cuda.synchronize()\n",
    "                    records_mvsnerf['0_total'].append(start_mvsnerf.elapsed_time(end_mvsnerf))\n",
    "                    ##############################################################\n",
    "\n",
    "\n",
    "\n",
    "                    rgb, depth_pred = torch.clamp(rgb.cpu(),0,1.0).numpy(), depth_pred.cpu().numpy()\n",
    "                    rgb_rays.append(rgb)\n",
    "                    depth_rays_preds.append(depth_pred)\n",
    "\n",
    "                    #\n",
    "                    end_loop.record()\n",
    "                    torch.cuda.synchronize()\n",
    "                    records_general['1_loop'].append(start_loop.elapsed_time(end_loop))\n",
    "                    ##############################################################\n",
    "\n",
    "                #\n",
    "                end_loops.record()\n",
    "                torch.cuda.synchronize()\n",
    "                records_general['2_loops'].append(start_loops.elapsed_time(end_loops))\n",
    "                ##############################################################\n",
    "\n",
    "                #\n",
    "                end_all.record()\n",
    "                torch.cuda.synchronize()\n",
    "                records_general['0_all'].append(start_all.elapsed_time(end_all))\n",
    "                ##############################################################\n",
    "                \n",
    "                depth_rays_preds = np.concatenate(depth_rays_preds).reshape(H, W)\n",
    "                depth_rays_preds, _ = visualize_depth_numpy(depth_rays_preds, near_far_source)\n",
    "\n",
    "                rgb_rays = np.concatenate(rgb_rays).reshape(H, W, 3)\n",
    "                img_vis = np.concatenate((img*255,rgb_rays*255,depth_rays_preds),axis=1)\n",
    "\n",
    "                if save_as_image:\n",
    "                    imageio.imwrite(f'{save_dir}/scan{scene}_{val_idx[i]:03d}.png', img_vis.astype('uint8'))\n",
    "                \n",
    "                # quantity\n",
    "                # mask background since they are outside the far boundle\n",
    "                mask = depth==0\n",
    "                imageio.imwrite(f'{save_dir}/scan{scene}_{val_idx[i]:03d}_mask.png', mask.astype('uint8')*255)\n",
    "                rgb_rays[mask],img[mask] = 0.0,0.0\n",
    "                psnr.append( mse2psnr(np.mean((rgb_rays[~mask]-img[~mask])**2)))\n",
    "                ssim.append( structural_similarity(rgb_rays, img, multichannel=True))\n",
    "\n",
    "                img_tensor = torch.from_numpy(rgb_rays)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "                img_gt_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0\n",
    "                LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "            psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "\n",
    "    records_processed = append_records(records_processed, records_general, records_mvsnet, records_mvsnerf, \n",
    "                                       scenes, num_src, img_scale,\n",
    "                                       np.mean(psnr_all), np.mean(ssim_all), np.mean(LPIPS_vgg_all), \n",
    "                                       name)\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return records_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_records():\n",
    "    return {'experiment name':[],\n",
    "            'novel scenes synthesized':[],\n",
    "            'number of source images':[],\n",
    "            'image scale':[],\n",
    "            'psnr':[],\n",
    "            'ssim':[],\n",
    "            'lpips':[],\n",
    "            'total time':[],\n",
    "            'mvsnet total time':[],\n",
    "            'mvsnet feature extraction':[],\n",
    "            'mvsnet cost volume':[],\n",
    "            'mvsnet 3D-CNN':[],\n",
    "            'mvsnerf total time':[],\n",
    "            'mvsnerf volume sampling':[],\n",
    "            'mvsnerf nerf':[],\n",
    "            'mvsnerf rendering':[]\n",
    "           }\n",
    "\n",
    "def append_records(records_processed, records_general, records_mvsnet, records_mvsnerf, \n",
    "                   scenes, num_src, img_scale,\n",
    "                   psnr, ssim, lpips,\n",
    "                   name='this experiment wants a name'):\n",
    "\n",
    "    records_processed['experiment name'] += [f'{name}']\n",
    "    \n",
    "    ###\n",
    "    records_processed['novel scenes synthesized'] += [scenes]\n",
    "    records_processed['number of source images'] += [num_src]\n",
    "    records_processed['image scale'] += [img_scale]\n",
    "\n",
    "    records_processed['psnr'] += [psnr]\n",
    "    records_processed['ssim'] += [ssim]\n",
    "    records_processed['lpips'] += [lpips]\n",
    "    \n",
    "    records_processed['total time'] += [np.mean(records_general['0_all'])]\n",
    "\n",
    "    records_processed['mvsnet total time'] += [np.mean(records_mvsnet['0_total'])]\n",
    "    records_processed['mvsnet feature extraction'] += [np.mean(records_mvsnet['1_feat'])]\n",
    "    records_processed['mvsnet cost volume'] += [np.mean(records_mvsnet['2_costvol'])]\n",
    "    records_processed['mvsnet 3D-CNN'] += [np.mean(records_mvsnet['3_3dcnn'])]\n",
    "    \n",
    "    records_processed['mvsnerf total time'] += [np.mean(records_mvsnerf['0_total']) * len (records_mvsnerf['0_total']) / len(records_general['0_all'])]\n",
    "    records_processed['mvsnerf volume sampling'] += [np.mean(records_mvsnerf['1_sample']) * len (records_mvsnerf['1_sample']) / len(records_general['0_all'])]\n",
    "    records_processed['mvsnerf nerf'] += [np.mean(records_mvsnerf['2_nerf']) * len (records_mvsnerf['2_nerf']) / len(records_general['0_all'])]\n",
    "    records_processed['mvsnerf rendering'] += [np.mean(records_mvsnerf['3_rend']) * len (records_mvsnerf['3_rend']) / len(records_general['0_all'])]\n",
    "\n",
    "    return records_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ckpts ['./ckpts/mvsnerf-v0.tar']\n",
      "Reloading from ./ckpts/mvsnerf-v0.tar\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mvsnerf/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_752/3174675028.py:200: FutureWarning: `multichannel` is a deprecated argument name for `structural_similarity`. It will be removed in version 1.0.Please use `channel_axis` instead.\n",
      "  ssim.append( structural_similarity(rgb_rays, img, multichannel=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "Found ckpts ['./ckpts/mvsnerf-v0.tar']\n",
      "Reloading from ./ckpts/mvsnerf-v0.tar\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "Found ckpts ['./ckpts/mvsnerf-v0.tar']\n",
      "Reloading from ./ckpts/mvsnerf-v0.tar\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "Found ckpts ['./ckpts/mvsnerf-v0.tar']\n",
      "Reloading from ./ckpts/mvsnerf-v0.tar\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "Found ckpts ['./ckpts/mvsnerf-v0.tar']\n",
      "Reloading from ./ckpts/mvsnerf-v0.tar\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "Found ckpts ['./ckpts/mvsnerf-v0.tar']\n",
      "Reloading from ./ckpts/mvsnerf-v0.tar\n",
      "==> image down scale: 0.8\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 0.8\n",
      "===> valing index: [32, 24, 23, 44]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 409, 512]' is invalid for input of size 839680",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m records_processed \u001b[38;5;241m=\u001b[39m experiment_and_record(records_processed, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_src=12\u001b[39m\u001b[38;5;124m'\u001b[39m, scenes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m21\u001b[39m,\u001b[38;5;241m30\u001b[39m,\u001b[38;5;241m31\u001b[39m,\u001b[38;5;241m34\u001b[39m,\u001b[38;5;241m63\u001b[39m,\u001b[38;5;241m82\u001b[39m,\u001b[38;5;241m103\u001b[39m,\u001b[38;5;241m114\u001b[39m], num_src\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, img_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, save_as_image\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# decreasing image scale\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m records_processed \u001b[38;5;241m=\u001b[39m \u001b[43mexperiment_and_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecords_processed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimg_scale=0.8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscenes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m21\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m31\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m34\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m63\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m82\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m103\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m114\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_src\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_as_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m records_processed \u001b[38;5;241m=\u001b[39m experiment_and_record(records_processed, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_scale=0.6\u001b[39m\u001b[38;5;124m'\u001b[39m, scenes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m21\u001b[39m,\u001b[38;5;241m30\u001b[39m,\u001b[38;5;241m31\u001b[39m,\u001b[38;5;241m34\u001b[39m,\u001b[38;5;241m63\u001b[39m,\u001b[38;5;241m82\u001b[39m,\u001b[38;5;241m103\u001b[39m,\u001b[38;5;241m114\u001b[39m], num_src\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, img_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m, save_as_image\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m records_processed \u001b[38;5;241m=\u001b[39m experiment_and_record(records_processed, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_scale=0.4\u001b[39m\u001b[38;5;124m'\u001b[39m, scenes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m21\u001b[39m,\u001b[38;5;241m30\u001b[39m,\u001b[38;5;241m31\u001b[39m,\u001b[38;5;241m34\u001b[39m,\u001b[38;5;241m63\u001b[39m,\u001b[38;5;241m82\u001b[39m,\u001b[38;5;241m103\u001b[39m,\u001b[38;5;241m114\u001b[39m], num_src\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, img_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m, save_as_image\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mexperiment_and_record\u001b[0;34m(records_processed, name, scenes, num_src, img_scale, save_as_image)\u001b[0m\n\u001b[1;32m     60\u001b[0m args\u001b[38;5;241m.\u001b[39mchunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5120\u001b[39m\n\u001b[1;32m     62\u001b[0m dataset_train \u001b[38;5;241m=\u001b[39m dataset_dict[args\u001b[38;5;241m.\u001b[39mdataset_name](args, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m dataset_val \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m val_idx \u001b[38;5;241m=\u001b[39m dataset_val\u001b[38;5;241m.\u001b[39mimg_idx\n\u001b[1;32m     66\u001b[0m save_as_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/documents/mvs/mvsnerf_timing/data/dtu_ft.py:37\u001b[0m, in \u001b[0;36mDTU_ft.__init__\u001b[0;34m(self, args, split, load_ref)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnear_far \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m2.125\u001b[39m, \u001b[38;5;241m4.525\u001b[39m]\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m load_ref:\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_meta\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/documents/mvs/mvsnerf_timing/data/dtu_ft.py:192\u001b[0m, in \u001b[0;36mDTU_ft.read_meta\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_rays \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_rays, \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# (len(self.meta['frames]),h*w, 3)\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_rgbs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_rgbs, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_wh[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# (len(self.meta['frames]),h,w,3)\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_depth \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_wh\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 409, 512]' is invalid for input of size 839680"
     ]
    }
   ],
   "source": [
    "## 이걸로 계속 돌리고 쌓으면 됨!\n",
    "\n",
    "records_processed = init_records()\n",
    "records_processed = experiment_and_record(records_processed, name='base', scenes=[1,8,21,103,114], num_src=3, img_scale=1.0, save_as_image=True)\n",
    "records_processed = experiment_and_record(records_processed, name='base+more_scenes', scenes=[1,8,21,30,31,34,63,82,103,114], num_src=3, img_scale=1.0, save_as_image=True)\n",
    "\n",
    "# number of source views \n",
    "records_processed = experiment_and_record(records_processed, name='num_src=4', scenes=[1,8,21,30,31,34,63,82,103,114], num_src=4, img_scale=1.0, save_as_image=True)\n",
    "records_processed = experiment_and_record(records_processed, name='num_src=8', scenes=[1,8,21,30,31,34,63,82,103,114], num_src=8, img_scale=1.0, save_as_image=True)\n",
    "records_processed = experiment_and_record(records_processed, name='num_src=12', scenes=[1,8,21,30,31,34,63,82,103,114], num_src=12, img_scale=1.0, save_as_image=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ckpts ['./ckpts/mvsnerf-v0.tar']\n",
      "Reloading from ./ckpts/mvsnerf-v0.tar\n",
      "==> image down scale: 0.75\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 0.75\n",
      "===> valing index: [32, 24, 23, 44]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# decreasing image scale 1.00 = (512, 640, 3) (384, 480, 3) (256, 320, 3) (128, 160, 3)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m records_processed \u001b[38;5;241m=\u001b[39m \u001b[43mexperiment_and_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecords_processed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimg_scale=0.75\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscenes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m21\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m31\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m34\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m63\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m82\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m103\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m114\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_src\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.75\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_as_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m records_processed \u001b[38;5;241m=\u001b[39m experiment_and_record(records_processed, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_scale=0.50\u001b[39m\u001b[38;5;124m'\u001b[39m, scenes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m21\u001b[39m,\u001b[38;5;241m30\u001b[39m,\u001b[38;5;241m31\u001b[39m,\u001b[38;5;241m34\u001b[39m,\u001b[38;5;241m63\u001b[39m,\u001b[38;5;241m82\u001b[39m,\u001b[38;5;241m103\u001b[39m,\u001b[38;5;241m114\u001b[39m], num_src\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, img_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.50\u001b[39m, save_as_image\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m records_processed \u001b[38;5;241m=\u001b[39m experiment_and_record(records_processed, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_scale=0.25\u001b[39m\u001b[38;5;124m'\u001b[39m, scenes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m21\u001b[39m,\u001b[38;5;241m30\u001b[39m,\u001b[38;5;241m31\u001b[39m,\u001b[38;5;241m34\u001b[39m,\u001b[38;5;241m63\u001b[39m,\u001b[38;5;241m82\u001b[39m,\u001b[38;5;241m103\u001b[39m,\u001b[38;5;241m114\u001b[39m], num_src\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, img_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m, save_as_image\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mexperiment_and_record\u001b[0;34m(records_processed, name, scenes, num_src, img_scale, save_as_image)\u001b[0m\n\u001b[1;32m    150\u001b[0m start_mvsnerf\u001b[38;5;241m.\u001b[39mrecord()\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m rgb, disp, acc, depth_pred, alpha, extras, records_mvsnerf \u001b[38;5;241m=\u001b[39m \u001b[43mrendering\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpose_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxyz_coarse_sampled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mxyz_NDC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrays_o\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrays_d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecords_mvsnerf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mvolume_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimgs_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrender_kwargs_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    156\u001b[0m end_mvsnerf\u001b[38;5;241m.\u001b[39mrecord()\n",
      "File \u001b[0;32m~/documents/mvs/mvsnerf_timing/renderer_timer.py:184\u001b[0m, in \u001b[0;36mrendering\u001b[0;34m(args, pose_ref, rays_pts, rays_ndc, depth_candidates, rays_o, rays_dir, records, volume_feature, imgs, network_fn, img_feat, network_query_fn, white_bkgd, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    183\u001b[0m end_2\u001b[38;5;241m.\u001b[39mrecord()\n\u001b[0;32m--> 184\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m records[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2_nerf\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(start_2\u001b[38;5;241m.\u001b[39melapsed_time(end_2))\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m##############################################################\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/mvsnerf/lib/python3.8/site-packages/torch/cuda/__init__.py:493\u001b[0m, in \u001b[0;36msynchronize\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    491\u001b[0m _lazy_init()\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[0;32m--> 493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_synchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# decreasing image scale \n",
    "# 1.00 = (512, 640, 3) \n",
    "# 0.75 = (384, 480, 3) \n",
    "# 0.50 = (256, 320, 3) \n",
    "# 0.25 = (128, 160, 3)\n",
    "records_processed = experiment_and_record(records_processed, name='img_scale=0.75', scenes=[1,8,21,30,31,34,63,82,103,114], num_src=3, img_scale=0.75, save_as_image=True)\n",
    "records_processed = experiment_and_record(records_processed, name='img_scale=0.50', scenes=[1,8,21,30,31,34,63,82,103,114], num_src=3, img_scale=0.50, save_as_image=True)\n",
    "records_processed = experiment_and_record(records_processed, name='img_scale=0.25', scenes=[1,8,21,30,31,34,63,82,103,114], num_src=3, img_scale=0.25, save_as_image=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_df = pd.DataFrame(records_processed).round({'psnr':1, 'ssim':2, 'lpips':2, \n",
    "                                       'total time':0, 'mvsnet total time':0, 'mvsnerf total time':0, \n",
    "                                       'mvsnet feature extraction' : 1, 'mvsnet cost volume':1, 'mvsnet 3D-CNN':1,\n",
    "                                       'mvsnerf volume sampling': 1, 'mvsnerf nerf' : 1, 'mvsnerf rendering' : 1\n",
    "                                      })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_df['experiment name'] = records_df['experiment name'].replace(['img_scale=0.8', 'img_scale=0.6', 'img_scale=0.4'], ['img_scale=0.75', 'img_scale=0.50', 'img_scale=0.25'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment name</th>\n",
       "      <th>novel scenes synthesized</th>\n",
       "      <th>number of source images</th>\n",
       "      <th>image scale</th>\n",
       "      <th>psnr</th>\n",
       "      <th>ssim</th>\n",
       "      <th>lpips</th>\n",
       "      <th>total time</th>\n",
       "      <th>mvsnet total time</th>\n",
       "      <th>mvsnet feature extraction</th>\n",
       "      <th>mvsnet cost volume</th>\n",
       "      <th>mvsnet 3D-CNN</th>\n",
       "      <th>mvsnerf total time</th>\n",
       "      <th>mvsnerf volume sampling</th>\n",
       "      <th>mvsnerf nerf</th>\n",
       "      <th>mvsnerf rendering</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>base</td>\n",
       "      <td>[1, 8, 21, 103, 114]</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.17</td>\n",
       "      <td>2693.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>33.9</td>\n",
       "      <td>65.8</td>\n",
       "      <td>2497.0</td>\n",
       "      <td>129.2</td>\n",
       "      <td>2333.0</td>\n",
       "      <td>19.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>base+more_scenes</td>\n",
       "      <td>[1, 8, 21, 30, 31, 34, 63, 82, 103, 114]</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>25.8</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2718.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>33.9</td>\n",
       "      <td>65.4</td>\n",
       "      <td>2521.0</td>\n",
       "      <td>131.8</td>\n",
       "      <td>2354.9</td>\n",
       "      <td>19.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>num_src=4</td>\n",
       "      <td>[1, 8, 21, 30, 31, 34, 63, 82, 103, 114]</td>\n",
       "      <td>4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>25.9</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2741.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>39.1</td>\n",
       "      <td>65.2</td>\n",
       "      <td>2524.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>2356.8</td>\n",
       "      <td>19.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>num_src=8</td>\n",
       "      <td>[1, 8, 21, 30, 31, 34, 63, 82, 103, 114]</td>\n",
       "      <td>8</td>\n",
       "      <td>1.00</td>\n",
       "      <td>25.8</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2821.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>63.3</td>\n",
       "      <td>65.9</td>\n",
       "      <td>2520.0</td>\n",
       "      <td>131.2</td>\n",
       "      <td>2354.0</td>\n",
       "      <td>19.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>num_src=12</td>\n",
       "      <td>[1, 8, 21, 30, 31, 34, 63, 82, 103, 114]</td>\n",
       "      <td>12</td>\n",
       "      <td>1.00</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2919.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>93.5</td>\n",
       "      <td>66.1</td>\n",
       "      <td>2520.0</td>\n",
       "      <td>131.7</td>\n",
       "      <td>2353.5</td>\n",
       "      <td>19.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>img_scale=0.75</td>\n",
       "      <td>[1, 8, 21, 30, 31, 34, 63, 82, 103, 114]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.75</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1563.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>18.1</td>\n",
       "      <td>41.2</td>\n",
       "      <td>1418.0</td>\n",
       "      <td>73.3</td>\n",
       "      <td>1323.3</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>img_scale=0.50</td>\n",
       "      <td>[1, 8, 21, 30, 31, 34, 63, 82, 103, 114]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.50</td>\n",
       "      <td>20.6</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.21</td>\n",
       "      <td>716.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>20.9</td>\n",
       "      <td>622.0</td>\n",
       "      <td>31.3</td>\n",
       "      <td>581.6</td>\n",
       "      <td>4.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>img_scale=0.25</td>\n",
       "      <td>[1, 8, 21, 30, 31, 34, 63, 82, 103, 114]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.25</td>\n",
       "      <td>17.3</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.23</td>\n",
       "      <td>221.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>9.5</td>\n",
       "      <td>155.0</td>\n",
       "      <td>7.6</td>\n",
       "      <td>144.7</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    experiment name                  novel scenes synthesized  \\\n",
       "0              base                      [1, 8, 21, 103, 114]   \n",
       "1  base+more_scenes  [1, 8, 21, 30, 31, 34, 63, 82, 103, 114]   \n",
       "2         num_src=4  [1, 8, 21, 30, 31, 34, 63, 82, 103, 114]   \n",
       "3         num_src=8  [1, 8, 21, 30, 31, 34, 63, 82, 103, 114]   \n",
       "4        num_src=12  [1, 8, 21, 30, 31, 34, 63, 82, 103, 114]   \n",
       "5    img_scale=0.75  [1, 8, 21, 30, 31, 34, 63, 82, 103, 114]   \n",
       "6    img_scale=0.50  [1, 8, 21, 30, 31, 34, 63, 82, 103, 114]   \n",
       "7    img_scale=0.25  [1, 8, 21, 30, 31, 34, 63, 82, 103, 114]   \n",
       "\n",
       "   number of source images  image scale  psnr  ssim  lpips  total time  \\\n",
       "0                        3         1.00  26.2  0.93   0.17      2693.0   \n",
       "1                        3         1.00  25.8  0.93   0.16      2718.0   \n",
       "2                        4         1.00  25.9  0.94   0.16      2741.0   \n",
       "3                        8         1.00  25.8  0.94   0.16      2821.0   \n",
       "4                       12         1.00  25.6  0.93   0.16      2919.0   \n",
       "5                        3         0.75  24.0  0.91   0.17      1563.0   \n",
       "6                        3         0.50  20.6  0.87   0.21       716.0   \n",
       "7                        3         0.25  17.3  0.82   0.23       221.0   \n",
       "\n",
       "   mvsnet total time  mvsnet feature extraction  mvsnet cost volume  \\\n",
       "0              105.0                        4.9                33.9   \n",
       "1              105.0                        5.1                33.9   \n",
       "2              111.0                        6.2                39.1   \n",
       "3              140.0                       10.3                63.3   \n",
       "4              175.0                       15.3                93.5   \n",
       "5               63.0                        3.9                18.1   \n",
       "6               35.0                        3.5                 9.9   \n",
       "7               19.0                        4.0                 4.9   \n",
       "\n",
       "   mvsnet 3D-CNN  mvsnerf total time  mvsnerf volume sampling  mvsnerf nerf  \\\n",
       "0           65.8              2497.0                    129.2        2333.0   \n",
       "1           65.4              2521.0                    131.8        2354.9   \n",
       "2           65.2              2524.0                    132.0        2356.8   \n",
       "3           65.9              2520.0                    131.2        2354.0   \n",
       "4           66.1              2520.0                    131.7        2353.5   \n",
       "5           41.2              1418.0                     73.3        1323.3   \n",
       "6           20.9               622.0                     31.3         581.6   \n",
       "7            9.5               155.0                      7.6         144.7   \n",
       "\n",
       "   mvsnerf rendering  \n",
       "0               19.8  \n",
       "1               19.7  \n",
       "2               19.8  \n",
       "3               19.3  \n",
       "4               19.8  \n",
       "5               11.9  \n",
       "6                4.9  \n",
       "7                1.3  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_df.to_csv('./csv/3_renderer_dtu_timer_numviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment name</th>\n",
       "      <th>novel scenes synthesized</th>\n",
       "      <th>number of source images</th>\n",
       "      <th>image scale</th>\n",
       "      <th>psnr</th>\n",
       "      <th>ssim</th>\n",
       "      <th>lpips</th>\n",
       "      <th>total time</th>\n",
       "      <th>mvsnet total time</th>\n",
       "      <th>mvsnet feature extraction</th>\n",
       "      <th>mvsnet cost volume</th>\n",
       "      <th>mvsnet 3D-CNN</th>\n",
       "      <th>mvsnerf total time</th>\n",
       "      <th>mvsnerf volume sampling</th>\n",
       "      <th>mvsnerf nerf</th>\n",
       "      <th>mvsnerf rendering</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>base</td>\n",
       "      <td>[1, 8, 21, 103, 114]</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.17</td>\n",
       "      <td>2693.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>33.9</td>\n",
       "      <td>65.8</td>\n",
       "      <td>2497.0</td>\n",
       "      <td>129.2</td>\n",
       "      <td>2333.0</td>\n",
       "      <td>19.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>base+more_scenes</td>\n",
       "      <td>[1, 8, 21, 30, 31, 34, 63, 82, 103, 114]</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>25.8</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2718.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>33.9</td>\n",
       "      <td>65.4</td>\n",
       "      <td>2521.0</td>\n",
       "      <td>131.8</td>\n",
       "      <td>2354.9</td>\n",
       "      <td>19.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>num_src=4</td>\n",
       "      <td>[1, 8, 21, 30, 31, 34, 63, 82, 103, 114]</td>\n",
       "      <td>4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>25.9</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2741.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>39.1</td>\n",
       "      <td>65.2</td>\n",
       "      <td>2524.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>2356.8</td>\n",
       "      <td>19.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>num_src=8</td>\n",
       "      <td>[1, 8, 21, 30, 31, 34, 63, 82, 103, 114]</td>\n",
       "      <td>8</td>\n",
       "      <td>1.00</td>\n",
       "      <td>25.8</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2821.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>63.3</td>\n",
       "      <td>65.9</td>\n",
       "      <td>2520.0</td>\n",
       "      <td>131.2</td>\n",
       "      <td>2354.0</td>\n",
       "      <td>19.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>num_src=12</td>\n",
       "      <td>[1, 8, 21, 30, 31, 34, 63, 82, 103, 114]</td>\n",
       "      <td>12</td>\n",
       "      <td>1.00</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2919.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>93.5</td>\n",
       "      <td>66.1</td>\n",
       "      <td>2520.0</td>\n",
       "      <td>131.7</td>\n",
       "      <td>2353.5</td>\n",
       "      <td>19.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>img_scale=0.75</td>\n",
       "      <td>[1, 8, 21, 30, 31, 34, 63, 82, 103, 114]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.75</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1563.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>18.1</td>\n",
       "      <td>41.2</td>\n",
       "      <td>1418.0</td>\n",
       "      <td>73.3</td>\n",
       "      <td>1323.3</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>img_scale=0.50</td>\n",
       "      <td>[1, 8, 21, 30, 31, 34, 63, 82, 103, 114]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.50</td>\n",
       "      <td>20.6</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.21</td>\n",
       "      <td>716.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>20.9</td>\n",
       "      <td>622.0</td>\n",
       "      <td>31.3</td>\n",
       "      <td>581.6</td>\n",
       "      <td>4.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>img_scale=0.25</td>\n",
       "      <td>[1, 8, 21, 30, 31, 34, 63, 82, 103, 114]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.25</td>\n",
       "      <td>17.3</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.23</td>\n",
       "      <td>221.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>9.5</td>\n",
       "      <td>155.0</td>\n",
       "      <td>7.6</td>\n",
       "      <td>144.7</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    experiment name                  novel scenes synthesized  \\\n",
       "0              base                      [1, 8, 21, 103, 114]   \n",
       "1  base+more_scenes  [1, 8, 21, 30, 31, 34, 63, 82, 103, 114]   \n",
       "2         num_src=4  [1, 8, 21, 30, 31, 34, 63, 82, 103, 114]   \n",
       "3         num_src=8  [1, 8, 21, 30, 31, 34, 63, 82, 103, 114]   \n",
       "4        num_src=12  [1, 8, 21, 30, 31, 34, 63, 82, 103, 114]   \n",
       "5    img_scale=0.75  [1, 8, 21, 30, 31, 34, 63, 82, 103, 114]   \n",
       "6    img_scale=0.50  [1, 8, 21, 30, 31, 34, 63, 82, 103, 114]   \n",
       "7    img_scale=0.25  [1, 8, 21, 30, 31, 34, 63, 82, 103, 114]   \n",
       "\n",
       "   number of source images  image scale  psnr  ssim  lpips  total time  \\\n",
       "0                        3         1.00  26.2  0.93   0.17      2693.0   \n",
       "1                        3         1.00  25.8  0.93   0.16      2718.0   \n",
       "2                        4         1.00  25.9  0.94   0.16      2741.0   \n",
       "3                        8         1.00  25.8  0.94   0.16      2821.0   \n",
       "4                       12         1.00  25.6  0.93   0.16      2919.0   \n",
       "5                        3         0.75  24.0  0.91   0.17      1563.0   \n",
       "6                        3         0.50  20.6  0.87   0.21       716.0   \n",
       "7                        3         0.25  17.3  0.82   0.23       221.0   \n",
       "\n",
       "   mvsnet total time  mvsnet feature extraction  mvsnet cost volume  \\\n",
       "0              105.0                        4.9                33.9   \n",
       "1              105.0                        5.1                33.9   \n",
       "2              111.0                        6.2                39.1   \n",
       "3              140.0                       10.3                63.3   \n",
       "4              175.0                       15.3                93.5   \n",
       "5               63.0                        3.9                18.1   \n",
       "6               35.0                        3.5                 9.9   \n",
       "7               19.0                        4.0                 4.9   \n",
       "\n",
       "   mvsnet 3D-CNN  mvsnerf total time  mvsnerf volume sampling  mvsnerf nerf  \\\n",
       "0           65.8              2497.0                    129.2        2333.0   \n",
       "1           65.4              2521.0                    131.8        2354.9   \n",
       "2           65.2              2524.0                    132.0        2356.8   \n",
       "3           65.9              2520.0                    131.2        2354.0   \n",
       "4           66.1              2520.0                    131.7        2353.5   \n",
       "5           41.2              1418.0                     73.3        1323.3   \n",
       "6           20.9               622.0                     31.3         581.6   \n",
       "7            9.5               155.0                      7.6         144.7   \n",
       "\n",
       "   mvsnerf rendering  \n",
       "0               19.8  \n",
       "1               19.7  \n",
       "2               19.8  \n",
       "3               19.3  \n",
       "4               19.8  \n",
       "5               11.9  \n",
       "6                4.9  \n",
       "7                1.3  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('./csv/3_renderer_dtu_timer_numviews.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = torch.load('./configs/pairs.th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'horns_train': array([23, 32, 24, 41, 34, 22, 25, 35, 42, 12, 21, 51, 13, 39, 50, 26]),\n",
       " 'horns_test': array([33, 40, 31, 59]),\n",
       " 'horns_val': array([33, 40, 31, 59]),\n",
       " 'leaves_train': array([12, 18,  8, 17, 14,  7,  3, 22,  2, 19, 23,  9, 10,  6, 15, 21]),\n",
       " 'leaves_test': array([13, 11, 16,  4]),\n",
       " 'leaves_val': array([13, 11, 16,  4]),\n",
       " 'room_train': array([14, 39, 34, 13, 20, 36, 30, 31,  8,  7, 33, 25, 12, 29, 40, 19]),\n",
       " 'room_test': array([35, 15, 38, 21]),\n",
       " 'room_val': array([35, 15, 38, 21]),\n",
       " 'fortress_train': array([15, 20, 26, 14, 22, 27, 33, 39, 16, 32, 19,  8, 10,  3, 13, 28]),\n",
       " 'fortress_test': array([21,  9, 40, 25]),\n",
       " 'fortress_val': array([21,  9, 40, 25]),\n",
       " 'trex_train': array([52, 19, 47, 12, 46, 28, 18, 51, 27, 13, 48, 43, 29, 11, 45, 42]),\n",
       " 'trex_test': array([20, 21, 53, 22]),\n",
       " 'trex_val': array([20, 21, 53, 22]),\n",
       " 'orchids_train': array([ 8, 13, 11, 17,  7,  9, 14, 18, 23,  6,  2, 15,  5,  3, 22, 24]),\n",
       " 'orchids_test': array([12, 10, 16, 19]),\n",
       " 'orchids_val': array([12, 10, 16, 19]),\n",
       " 'fern_train': array([17,  2,  7,  6, 11,  1,  8, 18,  3, 16,  4, 14,  9, 10,  0, 15]),\n",
       " 'fern_test': array([12, 13,  5, 19]),\n",
       " 'fern_val': array([12, 13,  5, 19]),\n",
       " 'flower_train': array([12, 29, 11,  4, 31, 13, 15, 30, 10, 27, 14, 19, 28,  3, 18, 21]),\n",
       " 'flower_test': array([20,  6, 22,  5]),\n",
       " 'flower_val': array([20,  6, 22,  5]),\n",
       " 'lego_train': [6, 43, 33, 13, 17, 19, 20, 25, 30, 37, 46, 48, 49, 55, 59, 65],\n",
       " 'lego_test': [63, 70, 18, 28],\n",
       " 'lego_val': [63, 70, 18, 28],\n",
       " 'chair_train': array([62, 56, 26, 67, 92, 31, 63, 77, 85, 82, 47, 41, 55, 61, 99, 25]),\n",
       " 'chair_test': array([ 8, 24, 32, 78]),\n",
       " 'chair_val': array([ 8, 24, 32, 78]),\n",
       " 'ship_train': array([12, 32, 44, 17, 47,  3, 19,  2, 33, 77, 95, 54, 11, 98, 67, 87]),\n",
       " 'ship_test': array([80, 86, 22, 20]),\n",
       " 'ship_val': array([80, 86, 22, 20]),\n",
       " 'drums_train': [43, 81, 14, 3, 9, 11, 20, 21, 22, 40, 41, 42, 46, 51, 52, 55],\n",
       " 'drums_test': [79, 74, 91, 68],\n",
       " 'drums_val': [79, 74, 91, 68],\n",
       " 'materials_train': array([34, 73, 94, 97, 47, 19, 58, 16, 21, 13, 82, 61, 18, 79, 78, 37]),\n",
       " 'materials_test': array([36, 63, 46, 96]),\n",
       " 'materials_val': array([36, 63, 46, 96]),\n",
       " 'ficus_train': array([92, 69, 56, 98,  3, 64, 61, 45,  8, 62, 83, 17,  7, 44, 32, 90]),\n",
       " 'ficus_test': array([38, 23,  0,  5]),\n",
       " 'ficus_val': array([38, 23,  0,  5]),\n",
       " 'hotdog_train': array([48, 61,  0,  3, 33, 73, 78, 58, 28, 63, 71, 38, 22, 30,  9, 31]),\n",
       " 'hotdog_test': array([26, 60, 13, 47]),\n",
       " 'hotdog_val': array([26, 60, 13, 47]),\n",
       " 'mic_train': array([61, 80, 64,  2, 85, 15, 97, 93, 53, 44, 71, 68, 32, 90, 99,  6]),\n",
       " 'mic_test': array([20, 49, 55, 72]),\n",
       " 'mic_val': array([20, 49, 55, 72]),\n",
       " 'dtu_train': [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36],\n",
       " 'dtu_test': [32, 24, 23, 44],\n",
       " 'dtu_val': [32, 24, 23, 44],\n",
       " 'horns_teaser_train': [0,\n",
       "  6,\n",
       "  33,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  52,\n",
       "  53,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61],\n",
       " 'horns_teaser_val': [34],\n",
       " 'horns_teaser_test': [34],\n",
       " 'xgaze_11images_cropped_colmapCODE_test': array([0, 5]),\n",
       " 'xgaze_11images_cropped_colmapCODE_val': array([0, 5]),\n",
       " 'xgaze_11images_cropped_colmapCODE_train': array([ 1,  7,  6, 10,  8,  2,  9,  4,  3])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of source views \n",
    "records_processed = experiment_and_record(records_processed, name='num_src=4', scenes=[1,8,21,30,31,34,63,82,103,114], num_src=4, img_scale=1.0, save_as_image=True)\n",
    "records_processed = experiment_and_record(records_processed, name='num_src=8', scenes=[1,8,21,30,31,34,63,82,103,114], num_src=8, img_scale=1.0, save_as_image=True)\n",
    "records_processed = experiment_and_record(records_processed, name='num_src=12', scenes=[1,8,21,30,31,34,63,82,103,114], num_src=12, img_scale=1.0, save_as_image=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decreasing image scale\n",
    "records_processed = experiment_and_record(records_processed, name='img_scale=0.8', scenes=[1,8,21,30,31,34,63,82,103,114], num_src=3, img_scale=0.8, save_as_image=True)\n",
    "records_processed = experiment_and_record(records_processed, name='img_scale=0.6', scenes=[1,8,21,30,31,34,63,82,103,114], num_src=3, img_scale=0.6, save_as_image=True)\n",
    "records_processed = experiment_and_record(records_processed, name='img_scale=0.4', scenes=[1,8,21,30,31,34,63,82,103,114], num_src=3, img_scale=0.4, save_as_image=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ckpts ['./ckpts/mvsnerf-v0.tar']\n",
      "Reloading from ./ckpts/mvsnerf-v0.tar\n",
      "============> rendering dataset <===================\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                           | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                           | 0/4 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/mvsnerf/lib/python3.8/site-packages/PIL/ImageFile.py:495\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 495\u001b[0m     fh \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileno\u001b[49m()\n\u001b[1;32m    496\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28mprint\u001b[39m(rgb_rays\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_as_image:\n\u001b[0;32m--> 212\u001b[0m     \u001b[43mimageio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msave_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/scan\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mscene\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mval_idx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m03d\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_vis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muint8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     rgbs\u001b[38;5;241m.\u001b[39mappend(img_vis\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muint8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/opt/conda/envs/mvsnerf/lib/python3.8/site-packages/imageio/core/functions.py:196\u001b[0m, in \u001b[0;36mimwrite\u001b[0;34m(uri, im, format, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage must be 2D (grayscale, RGB, or RGBA).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m imopen(uri, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwi\u001b[39m\u001b[38;5;124m\"\u001b[39m, plugin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/mvsnerf/lib/python3.8/site-packages/imageio/core/legacy_plugin_wrapper.py:172\u001b[0m, in \u001b[0;36mLegacyPlugin.write\u001b[0;34m(self, image, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlegacy_get_writer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;241m.\u001b[39mimage_mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miv\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 172\u001b[0m         \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(image) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/envs/mvsnerf/lib/python3.8/site-packages/imageio/core/format.py:559\u001b[0m, in \u001b[0;36mFormat.Writer.append_data\u001b[0;34m(self, im, meta)\u001b[0m\n\u001b[1;32m    557\u001b[0m im \u001b[38;5;241m=\u001b[39m asarray(im)\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# Call\u001b[39;00m\n\u001b[0;32m--> 559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_append_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_meta\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/mvsnerf/lib/python3.8/site-packages/imageio/plugins/pillow_legacy.py:460\u001b[0m, in \u001b[0;36mPNGFormat.Writer._append_data\u001b[0;34m(self, im, meta)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m     im \u001b[38;5;241m=\u001b[39m image_as_uint(im, bitdepth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m--> 460\u001b[0m \u001b[43mPillowFormat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_append_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/mvsnerf/lib/python3.8/site-packages/imageio/plugins/pillow_legacy.py:382\u001b[0m, in \u001b[0;36mPillowFormat.Writer._append_data\u001b[0;34m(self, im, meta)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbits\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_meta:\n\u001b[1;32m    381\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mquantize()  \u001b[38;5;66;03m# Make it a P image, so bits arg is used\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplugin_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_meta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m save_pillow_close(img)\n",
      "File \u001b[0;32m/opt/conda/envs/mvsnerf/lib/python3.8/site-packages/PIL/Image.py:2212\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2209\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2211\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2212\u001b[0m     \u001b[43msave_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2213\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# do what we can to clean up\u001b[39;00m\n\u001b[1;32m   2215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m open_fp:\n",
      "File \u001b[0;32m/opt/conda/envs/mvsnerf/lib/python3.8/site-packages/PIL/PngImagePlugin.py:1348\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[1;32m   1346\u001b[0m     _write_multiple_frames(im, fp, chunk, rawmode)\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1348\u001b[0m     \u001b[43mImageFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_idat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrawmode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info:\n\u001b[1;32m   1351\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m info_chunk \u001b[38;5;129;01min\u001b[39;00m info\u001b[38;5;241m.\u001b[39mchunks:\n",
      "File \u001b[0;32m/opt/conda/envs/mvsnerf/lib/python3.8/site-packages/PIL/ImageFile.py:509\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 509\u001b[0m         l, s, d \u001b[38;5;241m=\u001b[39m \u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m         fp\u001b[38;5;241m.\u001b[39mwrite(d)\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m s:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "depth_acc = {}\n",
    "eval_metric = [0.1,0.05,0.01]\n",
    "depth_acc[f'abs_err'],depth_acc[f'acc_l_{eval_metric[0]}'],depth_acc[f'acc_l_{eval_metric[1]}'],depth_acc[f'acc_l_{eval_metric[2]}'] = {},{},{},{}\n",
    "   \n",
    "\n",
    "for i_scene, scene in enumerate([1]):#,8,21,103,114\n",
    "\n",
    "    # create timing variables\n",
    "\n",
    "    # measure time - all processes\n",
    "    start_all = torch.cuda.Event(enable_timing=True)\n",
    "    end_all = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # measure time - Feature extraction and neural volume encoding\n",
    "    start_mvsnet = torch.cuda.Event(enable_timing=True)\n",
    "    end_mvsnet = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    # measure time - MVSNeRF and volume rendering\n",
    "    start_mvsnerf = torch.cuda.Event(enable_timing=True)\n",
    "    end_mvsnerf = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    # measure time - MVSNeRF and volume rendering loop\n",
    "    start_loop = torch.cuda.Event(enable_timing=True)\n",
    "    end_loop = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # measure time - MVSNeRF and volume rendering all loops\n",
    "    start_loops = torch.cuda.Event(enable_timing=True)\n",
    "    end_loops = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    # for combining all recorded time\n",
    "    records = []\n",
    "    records_general = {\"0_all\" : [], \"1_loop\" : [], \"2_loops\" : []}\n",
    "    records_mvsnet = {\"0_total\" : [], \"1_feat\" : [], \"2_costvol\" : [], \"3_3dcnn\" : []}\n",
    "    records_mvsnerf = {\"0_total\" : [],\"1_sample\" : [], \"2_nerf\" : [], \"3_rend\" : []}\n",
    "    \n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    cmd = f'--datadir /mnt/hdd/mvsnerf_data/dtu/scan{scene}  \\\n",
    "     --dataset_name dtu_ft  \\\n",
    "     --net_type v0 --ckpt ./ckpts/mvsnerf-v0.tar \\\n",
    "     --imgScale_train 1 --imgScale_test 1 --img_downscale 1'\n",
    "\n",
    "    args = config_parser(cmd.split())\n",
    "    args.use_viewdirs = True\n",
    "\n",
    "    args.N_samples = 128\n",
    "    args.feat_dim =  8+4*3\n",
    "\n",
    "    # create models\n",
    "    if 0==i_scene:\n",
    "        render_kwargs_train, render_kwargs_test, start, grad_vars = create_nerf_mvs(args, use_mvs=True, dir_embedder=False, pts_embedder=True)\n",
    "        filter_keys(render_kwargs_train)\n",
    "\n",
    "        MVSNet = render_kwargs_train['network_mvs']\n",
    "        render_kwargs_train.pop('network_mvs')\n",
    "\n",
    "\n",
    "    datadir = args.datadir\n",
    "    datatype = 'train'\n",
    "    pad = 16\n",
    "    args.chunk = 5120\n",
    "\n",
    "\n",
    "    print('============> rendering dataset <===================')\n",
    "    dataset_train = dataset_dict[args.dataset_name](args, split='train')\n",
    "    dataset_val = dataset_dict[args.dataset_name](args, split='val')\n",
    "    val_idx = dataset_val.img_idx\n",
    "\n",
    "    save_as_image = True\n",
    "    save_dir = f'results/test_dtu'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    MVSNet.train()\n",
    "    MVSNet = MVSNet.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        try:\n",
    "            tqdm._instances.clear() \n",
    "        except Exception:     \n",
    "            pass       \n",
    "\n",
    "        for i, batch in enumerate(tqdm(dataset_val)):\n",
    "                        \n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            ##################\n",
    "            # time everything\n",
    "            ##################\n",
    "            start_all.record()\n",
    "            #\n",
    "\n",
    "            rays, img = decode_batch(batch)\n",
    "            rays = rays.squeeze().to(device)  # (H*W, 3)\n",
    "            img = img.squeeze().cpu().numpy()  # (H, W, 3)\n",
    "            depth = batch['depth'].squeeze().numpy()  # (H, W)\n",
    "\n",
    "            # find nearest image idx from training views\n",
    "            positions = dataset_train.poses[:,:3,3]\n",
    "            dis = np.sum(np.abs(positions - dataset_val.poses[[i],:3,3]), axis=-1)\n",
    "            pair_idx = np.argsort(dis)[:3]\n",
    "            pair_idx = [dataset_train.img_idx[item] for item in pair_idx]\n",
    "\n",
    "            imgs_source, proj_mats, near_far_source, pose_source = dataset_train.read_source_views(pair_idx=pair_idx,device=device)\n",
    "            \n",
    "            ##################\n",
    "            # time mvsnet 0\n",
    "            ##################\n",
    "            start_mvsnet.record()\n",
    "            #\n",
    "            volume_feature, _, _, records_mvsnet = MVSNet(imgs_source, proj_mats, near_far_source, records=records_mvsnet, pad=pad)\n",
    "            #\n",
    "            end_mvsnet.record()\n",
    "            torch.cuda.synchronize()\n",
    "            records_mvsnet['0_total'].append(start_mvsnet.elapsed_time(end_mvsnet))\n",
    "            ##############################################################\n",
    "            \n",
    "                \n",
    "            imgs_source = unpreprocess(imgs_source)\n",
    "\n",
    "            N_rays_all = rays.shape[0]\n",
    "            rgb_rays, depth_rays_preds = [],[]\n",
    "            \n",
    "            ##################\n",
    "            # time loops\n",
    "            ##################\n",
    "            start_loops.record()\n",
    "            #\n",
    "            \n",
    "            for chunk_idx in range(N_rays_all//args.chunk + int(N_rays_all%args.chunk>0)):\n",
    "                \n",
    "                # for loop timing\n",
    "                ##################\n",
    "                # time loop\n",
    "                ##################\n",
    "                start_loop.record()\n",
    "                #\n",
    "                xyz_coarse_sampled, rays_o, rays_d, z_vals = ray_marcher(rays[chunk_idx*args.chunk:(chunk_idx+1)*args.chunk],\n",
    "                                                    N_samples=args.N_samples)\n",
    "\n",
    "                # Converting world coordinate to ndc coordinate\n",
    "                H, W = img.shape[:2]\n",
    "                inv_scale = torch.tensor([W - 1, H - 1]).to(device)\n",
    "                w2c_ref, intrinsic_ref = pose_source['w2cs'][0], pose_source['intrinsics'][0].clone()\n",
    "                xyz_NDC = get_ndc_coordinate(w2c_ref, intrinsic_ref, xyz_coarse_sampled, inv_scale,\n",
    "                                             near=near_far_source[0], far=near_far_source[1], pad=pad*args.imgScale_test)\n",
    "\n",
    "\n",
    "                # rendering\n",
    "                \n",
    "                ##################\n",
    "                # time mvsnerf 0\n",
    "                ##################\n",
    "                start_mvsnerf.record()\n",
    "                #\n",
    "                rgb, disp, acc, depth_pred, alpha, extras, records_mvsnerf = rendering(args, pose_source, xyz_coarse_sampled,\n",
    "                                                                       xyz_NDC, z_vals, rays_o, rays_d, records_mvsnerf,\n",
    "                                                                       volume_feature,imgs_source, **render_kwargs_train)\n",
    "                #\n",
    "                end_mvsnerf.record()\n",
    "                torch.cuda.synchronize()\n",
    "                records_mvsnerf['0_total'].append(start_mvsnerf.elapsed_time(end_mvsnerf))\n",
    "                ##############################################################\n",
    "                \n",
    "                \n",
    "\n",
    "                rgb, depth_pred = torch.clamp(rgb.cpu(),0,1.0).numpy(), depth_pred.cpu().numpy()\n",
    "                rgb_rays.append(rgb)\n",
    "                depth_rays_preds.append(depth_pred)\n",
    "                \n",
    "                #\n",
    "                end_loop.record()\n",
    "                torch.cuda.synchronize()\n",
    "                records_general['1_loop'].append(start_loop.elapsed_time(end_loop))\n",
    "                ##############################################################\n",
    "            \n",
    "            #\n",
    "            end_loops.record()\n",
    "            torch.cuda.synchronize()\n",
    "            records_general['2_loops'].append(start_loops.elapsed_time(end_loops))\n",
    "            ##############################################################\n",
    "            \n",
    "            #\n",
    "            end_all.record()\n",
    "            torch.cuda.synchronize()\n",
    "            records_general['0_all'].append(start_all.elapsed_time(end_all))\n",
    "            ##############################################################\n",
    "            \n",
    "            \n",
    "            depth_rays_preds = np.concatenate(depth_rays_preds).reshape(H, W)\n",
    "\n",
    "            depth_gt, _ =  read_depth(f'/mnt/hdd/mvsnerf_data/dtu/Depths/scan{scene}/depth_map_{val_idx[i]:04d}.pfm')\n",
    "\n",
    "            # commented out because prediction and gt shape mismatch\n",
    "#             mask_gt = depth_gt>0\n",
    "#             abs_err = abs_error(depth_rays_preds, depth_gt/200, mask_gt)\n",
    "\n",
    "#             eval_metric = [0.01,0.05, 0.1]\n",
    "#             depth_acc[f'abs_err'][f'{scene}'] = np.mean(abs_err)\n",
    "#             depth_acc[f'acc_l_{eval_metric[0]}'][f'{scene}'] = acc_threshold(abs_err,eval_metric[0]).mean()\n",
    "#             depth_acc[f'acc_l_{eval_metric[1]}'][f'{scene}'] = acc_threshold(abs_err,eval_metric[1]).mean()\n",
    "#             depth_acc[f'acc_l_{eval_metric[2]}'][f'{scene}'] = acc_threshold(abs_err,eval_metric[2]).mean()\n",
    "\n",
    "\n",
    "            depth_rays_preds, _ = visualize_depth_numpy(depth_rays_preds, near_far_source)\n",
    "\n",
    "            rgb_rays = np.concatenate(rgb_rays).reshape(H, W, 3)\n",
    "            img_vis = np.concatenate((img*255,rgb_rays*255,depth_rays_preds),axis=1)\n",
    "            \n",
    "            print(rgb_rays.shape)\n",
    "\n",
    "            if save_as_image:\n",
    "                imageio.imwrite(f'{save_dir}/scan{scene}_{val_idx[i]:03d}.png', img_vis.astype('uint8'))\n",
    "            else:\n",
    "                rgbs.append(img_vis.astype('uint8'))\n",
    "\n",
    "            # quantity\n",
    "            # mask background since they are outside the far boundle\n",
    "            mask = depth==0\n",
    "            imageio.imwrite(f'{save_dir}/scan{scene}_{val_idx[i]:03d}_mask.png', mask.astype('uint8')*255)\n",
    "            rgb_rays[mask],img[mask] = 0.0,0.0\n",
    "            psnr.append( mse2psnr(np.mean((rgb_rays[~mask]-img[~mask])**2)))\n",
    "            ssim.append( structural_similarity(rgb_rays, img, multichannel=True))\n",
    "\n",
    "            img_tensor = torch.from_numpy(rgb_rays)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "            img_gt_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0\n",
    "            LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "        print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "        psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "\n",
    "\n",
    "    if not save_as_image:\n",
    "        imageio.mimwrite(f'{save_dir}/{scene}_spiral.mp4', np.stack(rgbs), fps=20, quality=10)\n",
    "\n",
    "# a = np.mean(list(depth_acc['abs_err'].values()))\n",
    "# b = np.mean(list(depth_acc[f'acc_l_{eval_metric[0]}'].values()))\n",
    "# c = np.mean(list(depth_acc[f'acc_l_{eval_metric[1]}'].values()))\n",
    "# d = np.mean(list(depth_acc[f'acc_l_{eval_metric[2]}'].values()))\n",
    "# print(f'============> abs_err: {a} <=================')\n",
    "# print(f'============> acc_l_{eval_metric[0]}: {b} <=================')\n",
    "# print(f'============> acc_l_{eval_metric[1]}: {c} <=================')\n",
    "# print(f'============> acc_l_{eval_metric[2]}: {d} <=================')\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rendering novel views with fixed 3 source views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate([1,8,21,103,114]):#,\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    cmd = f'--datadir /mnt/data/new_disk/sungx/data/mvs_dataset/DTU/mvs_training/dtu/scan{scene}  \\\n",
    "    --dataset_name dtu_ft  \\\n",
    "    --ckpt ./ckpts//mvsnerf-v0.tar'\n",
    "\n",
    "    args = config_parser(cmd.split())\n",
    "    args.use_viewdirs = True\n",
    "\n",
    "    args.N_samples = 128\n",
    "    args.feat_dim =  8+3*4\n",
    "\n",
    "    # create models\n",
    "    if 0==i_scene:\n",
    "        render_kwargs_train, render_kwargs_test, start, grad_vars = create_nerf_mvs(args, use_mvs=True, dir_embedder=False, pts_embedder=True)\n",
    "        filter_keys(render_kwargs_train)\n",
    "\n",
    "        MVSNet = render_kwargs_train['network_mvs']\n",
    "        render_kwargs_train.pop('network_mvs')\n",
    "\n",
    "\n",
    "    datadir = args.datadir\n",
    "    datatype = 'val'\n",
    "    pad = 24\n",
    "    args.chunk = 5120\n",
    "\n",
    "\n",
    "    print('============> rendering dataset <===================')\n",
    "    dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "    val_idx = dataset.img_idx\n",
    "    \n",
    "    save_as_image = True\n",
    "    save_dir = f'results/test3'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    MVSNet.train()\n",
    "    MVSNet = MVSNet.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)\n",
    "        volume_feature, _, _ = MVSNet(imgs_source, proj_mats, near_far_source, pad=pad)\n",
    "        imgs_source = unpreprocess(imgs_source)\n",
    "\n",
    "        try:\n",
    "            tqdm._instances.clear() \n",
    "        except Exception:     \n",
    "            pass\n",
    "        \n",
    "        for i, batch in enumerate(tqdm(dataset)):\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            rays, img = decode_batch(batch)\n",
    "            rays = rays.squeeze().to(device)  # (H*W, 3)\n",
    "            img = img.squeeze().cpu().numpy()  # (H, W, 3)\n",
    "            depth = batch['depth'].squeeze().numpy()  # (H, W)\n",
    "        \n",
    "            N_rays_all = rays.shape[0]\n",
    "            rgb_rays, depth_rays_preds = [],[]\n",
    "            for chunk_idx in range(N_rays_all//args.chunk + int(N_rays_all%args.chunk>0)):\n",
    "\n",
    "                xyz_coarse_sampled, rays_o, rays_d, z_vals = ray_marcher(rays[chunk_idx*args.chunk:(chunk_idx+1)*args.chunk],\n",
    "                                                    N_samples=args.N_samples)\n",
    "\n",
    "                # Converting world coordinate to ndc coordinate\n",
    "                H, W = img.shape[:2]\n",
    "                inv_scale = torch.tensor([W - 1, H - 1]).to(device)\n",
    "                w2c_ref, intrinsic_ref = pose_source['w2cs'][0], pose_source['intrinsics'][0].clone()\n",
    "                xyz_NDC = get_ndc_coordinate(w2c_ref, intrinsic_ref, xyz_coarse_sampled, inv_scale,\n",
    "                                             near=near_far_source[0], far=near_far_source[1], pad=pad*args.imgScale_test)\n",
    "\n",
    "\n",
    "                # rendering\n",
    "                rgb, disp, acc, depth_pred, alpha, extras = rendering(args, pose_source, xyz_coarse_sampled,\n",
    "                                                                       xyz_NDC, z_vals, rays_o, rays_d,\n",
    "                                                                       volume_feature,imgs_source, **render_kwargs_train)\n",
    "    \n",
    "                \n",
    "                rgb, depth_pred = torch.clamp(rgb.cpu(),0,1.0).numpy(), depth_pred.cpu().numpy()\n",
    "                rgb_rays.append(rgb)\n",
    "                depth_rays_preds.append(depth_pred)\n",
    "\n",
    "            \n",
    "            depth_rays_preds = np.concatenate(depth_rays_preds).reshape(H, W)\n",
    "            depth_rays_preds, _ = visualize_depth_numpy(depth_rays_preds, near_far_source)\n",
    "            \n",
    "            rgb_rays = np.concatenate(rgb_rays).reshape(H, W, 3)\n",
    "            img_vis = np.concatenate((img*255,rgb_rays*255,depth_rays_preds),axis=1)\n",
    "            \n",
    "            if save_as_image:\n",
    "                imageio.imwrite(f'{save_dir}/scan{scene}_{val_idx[i]:03d}.png', img_vis.astype('uint8'))\n",
    "            else:\n",
    "                rgbs.append(img_vis.astype('uint8'))\n",
    "                \n",
    "            # quantity\n",
    "            # mask background since they are outside the far boundle\n",
    "            mask = depth==0\n",
    "            imageio.imwrite(f'{save_dir}/scan{scene}_{val_idx[i]:03d}_mask.png', mask.astype('uint8')*255)\n",
    "            rgb_rays[mask],img[mask] = 0.0,0.0\n",
    "            psnr.append( mse2psnr(np.mean((rgb_rays[~mask]-img[~mask])**2)))\n",
    "            ssim.append( structural_similarity(rgb_rays, img, multichannel=True))\n",
    "            \n",
    "            img_tensor = torch.from_numpy(rgb_rays)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "            img_gt_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0\n",
    "            LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "        print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "        psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "\n",
    "    if not save_as_image:\n",
    "        imageio.mimwrite(f'{save_dir}/{scene}_spiral.mp4', np.stack(rgbs), fps=20, quality=10)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairs generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,torch\n",
    "import sys,os\n",
    "import numpy as np\n",
    "root = '/home/hengfei/Desktop/research/mvsnerf'\n",
    "os.chdir(root)\n",
    "sys.path.append(root)\n",
    "pairs = torch.load('./configs/pairs.th')\n",
    "\n",
    "# llff\n",
    "root_dir = '/home/hengfei/Desktop/research/mvsnerf/xgaze/'\n",
    "for scene in ['xgaze_11images_cropped_colmapCODE']:#\n",
    "    poses_bounds = np.load(os.path.join(root_dir, scene, 'poses_bounds.npy'))  # (N_images, 11)\n",
    "    poses = poses_bounds[:, :15].reshape(-1, 3, 5)  # (N_images, 3, 5)\n",
    "    poses = np.concatenate([poses[..., 1:2], - poses[..., :1], poses[..., 2:4]], -1)\n",
    "\n",
    "    ref_position = np.mean(poses[..., 3],axis=0, keepdims=True)\n",
    "    dist = np.sum(np.abs(poses[..., 3] - ref_position), axis=-1)\n",
    "    pair_idx = np.argsort(dist)[:11]\n",
    "#     pair_idx = torch.randperm(len(poses))[:20].tolist()\n",
    "\n",
    "    pairs[f'{scene}_test'] = pair_idx[::6]\n",
    "    pairs[f'{scene}_val'] = pair_idx[::6]\n",
    "    pairs[f'{scene}_train'] = np.delete(pair_idx, range(0,11,6))\n",
    "\n",
    "torch.save(pairs,'/home/hengfei/Desktop/research/mvsnerf/configs/pairs.th')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nerf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> scene: chair mean psnr 31.070870959993407 ssim: 0.970869913728838 lpips: 0.05512496456503868\n",
      "=====> scene: drums mean psnr 25.464383523724557 ssim: 0.9430287321705997 lpips: 0.1010842639952898\n",
      "=====> scene: ficus mean psnr 29.72717081186501 ssim: 0.9688198661712594 lpips: 0.04721927270293236\n",
      "=====> scene: hotdog mean psnr 34.63162021512352 ssim: 0.9798700143526428 lpips: 0.0885334312915802\n",
      "=====> scene: lego mean psnr 32.65761069614622 ssim: 0.9751430050524844 lpips: 0.05375238787382841\n",
      "=====> scene: materials mean psnr 30.220202654922936 ssim: 0.9677394226502894 lpips: 0.1052329633384943\n",
      "=====> scene: mic mean psnr 31.810551677509977 ssim: 0.9810118386928188 lpips: 0.03268271638080478\n",
      "=====> scene: ship mean psnr 29.487980342358682 ssim: 0.9079920156014059 lpips: 0.2625834122300148\n",
      "=====> all mean psnr 30.633798860205538 ssim: 0.9618093510525423 lpips: 0.09327667654724792\n",
      "=====> scene: fern mean psnr 23.87081932481545 ssim: 0.828319405500272 lpips: 0.29106350988149643\n",
      "=====> scene: flower mean psnr 26.84248375485232 ssim: 0.8972176342834637 lpips: 0.175886869430542\n",
      "=====> scene: fortress mean psnr 31.368287455491632 ssim: 0.9454387223441942 lpips: 0.14687807857990265\n",
      "=====> scene: horns mean psnr 25.957297279910254 ssim: 0.9002932801372645 lpips: 0.24746065214276314\n",
      "=====> scene: leaves mean psnr 21.209230306630403 ssim: 0.7924383925048237 lpips: 0.3013022392988205\n",
      "=====> scene: orchids mean psnr 19.805083676014405 ssim: 0.7216119459229409 lpips: 0.3210122361779213\n",
      "=====> scene: room mean psnr 33.538893189163886 ssim: 0.9782926576909046 lpips: 0.15664737671613693\n",
      "=====> scene: trex mean psnr 25.191124840989897 ssim: 0.8986906362165991 lpips: 0.24522138014435768\n",
      "=====> all mean psnr 25.972902478483533 ssim: 0.8702878343250579 lpips: 0.23568404279649258\n",
      "=====> scene: 1 mean psnr 26.621467948629075 ssim: 0.9015109955368615 lpips: 0.26542308926582336\n",
      "=====> scene: 8 mean psnr 28.331380911917446 ssim: 0.8758834032089559 lpips: 0.32112571597099304\n",
      "=====> scene: 21 mean psnr 23.238617049537122 ssim: 0.8736486067576469 lpips: 0.2456555962562561\n",
      "=====> scene: 103 mean psnr 30.40125554666663 ssim: 0.9442875531229784 lpips: 0.25619567558169365\n",
      "=====> scene: 114 mean psnr 26.46890004323232 ssim: 0.9134870027630088 lpips: 0.2253187969326973\n",
      "=====> all mean psnr 27.01232429999652 ssim: 0.9017635122778904 lpips: 0.2627437748014927\n"
     ]
    }
   ],
   "source": [
    "root = '/mnt/new_disk_2/anpei/code/nerf/logs/'\n",
    "root_gt = '/mnt/new_disk2/anpei/code/MVS-NeRF/runs_fine_tuning/'\n",
    "pairs = torch.load('/mnt/new_disk_2/anpei/code/MVS-NeRF/configs/pairs.th')\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['chair','drums','ficus','hotdog','lego','materials','mic','ship']):#\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    files = sorted(glob.glob(f'{root}/{scene}_test/testset_200000/*.png'))\n",
    "    for j, file in enumerate(files):\n",
    "\n",
    "        idx = pairs[f'{scene}_val'][j]\n",
    "        img = cv2.imread(file)[...,::-1]\n",
    "        gt = cv2.imread(f'{root_gt}/{scene}/{scene}/{scene}_00009999_{j:02d}.png')[...,::-1]\n",
    "        gt, img = gt[:,:800].astype('float')/255.0, img.astype('float')/255.0\n",
    "\n",
    "#         H_crop, W_crop = np.array(gt.shape[:2])//10\n",
    "#         img = img[H_crop:-H_crop,W_crop:-W_crop]\n",
    "#         gt = gt[H_crop:-H_crop,W_crop:-W_crop]\n",
    "\n",
    "        psnr.append( mse2psnr(np.mean((gt-img)**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') \n",
    "\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['fern','flower','fortress','horns','leaves', 'orchids', 'room',  'trex']):#,\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    files = sorted(glob.glob(f'{root}/{scene}_test/testset_200000/*.png'))\n",
    "    for j, file in enumerate(files):\n",
    "\n",
    "        idx = pairs[f'{scene}_val'][j]\n",
    "        img = cv2.resize(cv2.imread(file)[...,::-1],(960,640))\n",
    "        gt =  cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/{scene}_{idx:03d}.png')[...,::-1]\n",
    "        gt, img = gt[:,:960].astype('float')/255.0, img.astype('float')/255.0\n",
    "\n",
    "\n",
    "        psnr.append( mse2psnr(np.mean((gt-img)**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') \n",
    "\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate([1,8,21,103,114]):#,\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    files = sorted(glob.glob(f'{root}/scan{scene}_test/testset_200000/*.png'))\n",
    "    for j, file in enumerate(files):\n",
    "\n",
    "        idx = pairs[f'dtu_val'][j]\n",
    "        img = cv2.imread(file)[...,::-1]\n",
    "        gt = cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/scan{scene}_{idx:03d}.png')[...,::-1]\n",
    "        mask = cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/scan{scene}_{idx:03d}_mask.png')==255\n",
    "        gt, img = gt[:,:640].astype('float')/255.0, img.astype('float')/255.0\n",
    "\n",
    "        gt[mask],img[mask] = 0.0,0.0\n",
    "        psnr.append( mse2psnr(np.mean((gt[~mask]-img[~mask])**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> scene: chair mean psnr 26.746873810513947 ssim: 0.9321234340308168 lpips: 0.15475058555603027\n",
      "=====> scene: drums mean psnr 22.28406117543553 ssim: 0.8964614019632969 lpips: 0.21542910858988762\n",
      "=====> scene: ficus mean psnr 26.365789669973488 ssim: 0.9438413276712496 lpips: 0.15914445742964745\n",
      "=====> scene: hotdog mean psnr 32.489636248742805 ssim: 0.9699785599978382 lpips: 0.11295554973185062\n",
      "=====> scene: lego mean psnr 26.79832336361502 ssim: 0.9245065827858229 lpips: 0.18708691000938416\n",
      "=====> scene: materials mean psnr 24.957611270986945 ssim: 0.9249186604651752 lpips: 0.1740873008966446\n",
      "=====> scene: mic mean psnr 29.449610622444368 ssim: 0.9693072200690339 lpips: 0.092950988560915\n",
      "=====> scene: ship mean psnr 26.60832366062154 ssim: 0.8780999869891254 lpips: 0.28621142730116844\n",
      "=====> all mean psnr 26.962528727791707 ssim: 0.9299046467465449 lpips: 0.17282704100944102\n",
      "=====> scene: fern mean psnr 22.61357364768159 ssim: 0.77000724312094 lpips: 0.2827577739953995\n",
      "=====> scene: flower mean psnr 25.52052448547126 ssim: 0.8816617628340061 lpips: 0.19497620686888695\n",
      "=====> scene: fortress mean psnr 28.010539767191638 ssim: 0.8958878940094137 lpips: 0.19440287351608276\n",
      "=====> scene: horns mean psnr 24.99889079936664 ssim: 0.8806854873247154 lpips: 0.2329558990895748\n",
      "=====> scene: leaves mean psnr 21.228759476443667 ssim: 0.8063488185998933 lpips: 0.24219803884625435\n",
      "=====> scene: orchids mean psnr 19.460068266904813 ssim: 0.7087825176683923 lpips: 0.3091204948723316\n",
      "=====> scene: room mean psnr 29.150237317214977 ssim: 0.9570667630128395 lpips: 0.16276488453149796\n",
      "=====> scene: trex mean psnr 24.079128145103503 ssim: 0.8873396265555692 lpips: 0.2028873674571514\n",
      "=====> all mean psnr 24.38271523817226 ssim: 0.8484725141407212 lpips: 0.22775794239714742\n",
      "=====> scene: 1 mean psnr 28.046498782015014 ssim: 0.9341022735743112 lpips: 0.17119702696800232\n",
      "=====> scene: 8 mean psnr 28.875841987497292 ssim: 0.8999499891440654 lpips: 0.26092011481523514\n",
      "=====> scene: 21 mean psnr 24.870061827644722 ssim: 0.9215085855864712 lpips: 0.14212623238563538\n",
      "=====> scene: 103 mean psnr 32.226849694259215 ssim: 0.9638182025610383 lpips: 0.16953438520431519\n",
      "=====> scene: 114 mean psnr 28.46625266665073 ssim: 0.9451965364044997 lpips: 0.15322893112897873\n",
      "=====> all mean psnr 28.497100991613394 ssim: 0.9329151174540771 lpips: 0.17940133810043335\n"
     ]
    }
   ],
   "source": [
    "root = '/mnt/new_disk2/anpei/code/MVS-NeRF/runs_fine_tuning/'\n",
    "pairs = torch.load('/mnt/new_disk_2/anpei/code/MVS-NeRF/configs/pairs.th')\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['chair','drums','ficus','hotdog','lego','materials','mic','ship']):\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    files = sorted(glob.glob(f'{root}/{scene}/{scene}/{scene}_00009999_*'))\n",
    "    for j, file in enumerate(files):\n",
    "\n",
    "        idx = pairs[f'{scene}_val'][j]\n",
    "        img = cv2.imread(file).astype('float')[...,::-1]\n",
    "        gt, img = img[:,:800]/255.0, img[:,800:1600]/255.0\n",
    "\n",
    "#         H_crop, W_crop = np.array(gt.shape[:2])//10\n",
    "#         img = img[H_crop:-H_crop,W_crop:-W_crop]\n",
    "#         gt = gt[H_crop:-H_crop,W_crop:-W_crop]\n",
    "\n",
    "        psnr.append( mse2psnr(np.mean((gt-img)**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') \n",
    "\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['fern','flower','fortress','horns','leaves', 'orchids', 'room',  'trex']):#,\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    files = sorted(glob.glob(f'/mnt/new_disk2/anpei/code/MVS-NeRF/runs_fine_tuning/{scene}/{scene}/{scene}_00009999_*'))\n",
    "    for j, file in enumerate(files):\n",
    "\n",
    "        idx = pairs[f'{scene}_val'][j]\n",
    "        img = cv2.imread(file)[...,::-1]\n",
    "        gt, img = img[:,:960].astype('float')/255.0, img[:,960:960*2].astype('float')/255.0\n",
    "\n",
    "        H_crop, W_crop = np.array(gt.shape[:2])//10\n",
    "        img = img[H_crop:-H_crop,W_crop:-W_crop]\n",
    "        gt = gt[H_crop:-H_crop,W_crop:-W_crop]\n",
    "\n",
    "\n",
    "        psnr.append( mse2psnr(np.mean((gt-img)**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') \n",
    "\n",
    "root = '/mnt/new_disk2/anpei/code/MVS-NeRF/runs_fine_tuning/'\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate([1,8,21,103,114]):#,\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    files = sorted(glob.glob(f'{root}/dtu_scan{scene}_1h/dtu_scan{scene}_1h/00010239_*'))\n",
    "    for j, file in enumerate(files):\n",
    "\n",
    "        idx = pairs[f'dtu_val'][j]\n",
    "        img = cv2.imread(file)[...,::-1][:,640:1280]\n",
    "        gt = cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/scan{scene}_{idx:03d}.png')[...,::-1]\n",
    "        mask = cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/scan{scene}_{idx:03d}_mask.png')==255\n",
    "        gt, img = gt[:,:640].astype('float')/255.0, img.astype('float')/255.0\n",
    "\n",
    "        gt[mask],img[mask] = 0.0,0.0\n",
    "        psnr.append( mse2psnr(np.mean((gt[~mask]-img[~mask])**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ibrnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> scene: fern mean psnr 22.64474646040451 ssim: 0.7736232480476191 lpips: 0.26588304713368416\n",
      "=====> scene: flower mean psnr 26.553349019087786 ssim: 0.9092690161984827 lpips: 0.14575103670358658\n",
      "=====> scene: fortress mean psnr 30.338842953903075 ssim: 0.9368867837660259 lpips: 0.13289865292608738\n",
      "=====> scene: horns mean psnr 25.01290939681414 ssim: 0.9040335882553917 lpips: 0.1899307444691658\n",
      "=====> scene: leaves mean psnr 22.076508076698556 ssim: 0.8430354849586478 lpips: 0.17987846583127975\n",
      "=====> scene: orchids mean psnr 19.007830032899616 ssim: 0.7045611776629173 lpips: 0.2861044891178608\n",
      "=====> scene: room mean psnr 31.05473820815669 ssim: 0.9723299877991765 lpips: 0.08911459799855947\n",
      "=====> scene: trex mean psnr 22.339864946223464 ssim: 0.8421255627008343 lpips: 0.22207806631922722\n",
      "=====> all mean psnr 24.878598636773482 ssim: 0.8607331061736369 lpips: 0.1889548875624314\n",
      "=====> scene: 1 mean psnr 30.99564992655386 ssim: 0.9548394719193786 lpips: 0.1285402663052082\n",
      "=====> scene: 8 mean psnr 32.46173840309124 ssim: 0.9445782574823819 lpips: 0.16979694738984108\n",
      "=====> scene: 21 mean psnr 27.88178725277648 ssim: 0.9467770144187784 lpips: 0.1040295660495758\n",
      "=====> scene: 103 mean psnr 34.399916992709706 ssim: 0.9675776122666799 lpips: 0.15563546121120453\n",
      "=====> scene: 114 mean psnr 31.00119644953211 ssim: 0.9638488318305859 lpips: 0.09874588809907436\n",
      "=====> all mean psnr 31.348057804932676 ssim: 0.955524237583561 lpips: 0.1313496258109808\n"
     ]
    }
   ],
   "source": [
    "# root = '/mnt/new_disk2/anpei/code/IBRNet/logs'\n",
    "pairs = torch.load('/mnt/new_disk_2/anpei/code/MVS-NeRF/configs/pairs.th')\n",
    "# psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "# for i_scene, scene in enumerate(['chair','drums','ficus','hotdog','lego','materials','mic','ship']):#,\n",
    "#     psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "#     files = sorted(glob.glob(f'{root}/nerf-3view-finetuning-nearest-{scene}/010000_*'))\n",
    "#     for j, file in enumerate(files):\n",
    "\n",
    "#         idx = pairs[f'{scene}_val'][j]\n",
    "#         img = cv2.imread(file).astype('float')[...,::-1]\n",
    "#         gt, img = img[:,800:800*2]/255.0, img[:,800*3:800*4]/255.0\n",
    "\n",
    "# #         H_crop, W_crop = np.array(gt.shape[:2])//10\n",
    "# #         img = img[H_crop:-H_crop,W_crop:-W_crop]\n",
    "# #         gt = gt[H_crop:-H_crop,W_crop:-W_crop]\n",
    "\n",
    "#         psnr.append( mse2psnr(np.mean((gt-img)**2)))\n",
    "#         ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "#         img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "#         img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "#         LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "#     print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "#     psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "# print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') \n",
    "\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['fern','flower','fortress','horns','leaves', 'orchids', 'room',  'trex']):#\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    files = sorted(glob.glob(f'/mnt/new_disk2/anpei/code/IBRNet/logs/llff-3view-finetuning-nearest-{scene}/010000_*'))\n",
    "    for j, file in enumerate(files):\n",
    "\n",
    "        idx = pairs[f'{scene}_val'][j]\n",
    "        img = cv2.imread(file)[...,::-1]\n",
    "        gt, img = img[:,1008:1008*2].astype('float')/255.0, img[:,1008*3:1008*4].astype('float')/255.0\n",
    "        img, gt = cv2.resize(img,(960,640)), cv2.resize(gt,(960,640))\n",
    "\n",
    "        H_crop, W_crop = np.array(gt.shape[:2])//10\n",
    "        img = img[H_crop:-H_crop,W_crop:-W_crop]\n",
    "        gt = gt[H_crop:-H_crop,W_crop:-W_crop]\n",
    "\n",
    "        \n",
    "        psnr.append( mse2psnr(np.mean((gt-img)**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') \n",
    "\n",
    "root = '/mnt/new_disk2/anpei/code/IBRNet/logs'\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate([1,8,21,103,114]):#\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    files = sorted(glob.glob(f'{root}/dtu-3view-finetuning-nearest-scan{scene}/010000_*'))\n",
    "    for j, file in enumerate(files):\n",
    "\n",
    "        idx = pairs[f'dtu_val'][j]\n",
    "        img = cv2.imread(file)[...,::-1][:,3*640:4*640]\n",
    "        gt = cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/scan{scene}_{idx:03d}.png')[...,::-1]\n",
    "        mask = cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/scan{scene}_{idx:03d}_mask.png')==255\n",
    "        gt, img = gt[:,:640].astype('float')/255.0, img.astype('float')/255.0\n",
    "\n",
    "        gt[mask],img[mask] = 0.0,0.0\n",
    "        psnr.append( mse2psnr(np.mean((gt[~mask]-img[~mask])**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pixel nerf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> scene: chair mean psnr 7.175962813343725 ssim: 0.6243642351905847 lpips: 0.38591109961271286\n",
      "=====> scene: drums mean psnr 8.148548711878252 ssim: 0.6701584468514097 lpips: 0.42121122032403946\n",
      "=====> scene: ficus mean psnr 6.608732738834844 ssim: 0.668716265099144 lpips: 0.3350602239370346\n",
      "=====> scene: hotdog mean psnr 6.799387670799135 ssim: 0.6689815218041557 lpips: 0.43327029794454575\n",
      "=====> scene: lego mean psnr 7.740217521658803 ssim: 0.6710903029993184 lpips: 0.42670799791812897\n",
      "=====> scene: materials mean psnr 7.609290420358684 ssim: 0.6441046576733512 lpips: 0.43245941400527954\n",
      "=====> scene: mic mean psnr 7.707203698223274 ssim: 0.7294597852809476 lpips: 0.32929887622594833\n",
      "=====> scene: ship mean psnr 7.295484760785579 ssim: 0.5836685948507447 lpips: 0.5257005095481873\n",
      "=====> all mean psnr 7.385603541985287 ssim: 0.657567976218707 lpips: 0.4112024549394846\n",
      "=====> scene: fern mean psnr 12.397648684821284 ssim: 0.5312397318110376 lpips: 0.6500117480754852\n",
      "=====> scene: flower mean psnr 9.99675489427281 ssim: 0.43323656344453193 lpips: 0.7075561136007309\n",
      "=====> scene: fortress mean psnr 14.073488262986546 ssim: 0.6736649368929569 lpips: 0.6075489073991776\n",
      "=====> scene: horns mean psnr 11.71002161685521 ssim: 0.5157559834106309 lpips: 0.705190509557724\n",
      "=====> scene: leaves mean psnr 9.847068575637605 ssim: 0.2681360856716045 lpips: 0.6947661340236664\n",
      "=====> scene: orchids mean psnr 9.624184850140201 ssim: 0.3168018322171233 lpips: 0.7207814902067184\n",
      "=====> scene: room mean psnr 11.750716240417157 ssim: 0.6906632306577324 lpips: 0.6112178564071655\n",
      "=====> scene: trex mean psnr 10.55211637013632 ssim: 0.45770187915676297 lpips: 0.6672322899103165\n",
      "=====> all mean psnr 11.243999936908391 ssim: 0.48590003040779756 lpips: 0.6705381311476231\n",
      "=====> scene: 1 mean psnr 21.64330896303546 ssim: 0.8268906240371169 lpips: 0.3728666678071022\n",
      "=====> scene: 8 mean psnr 23.69860813191727 ssim: 0.8291247579664462 lpips: 0.38369619846343994\n",
      "=====> scene: 21 mean psnr 16.03916045790063 ssim: 0.6905614284959526 lpips: 0.4074021577835083\n",
      "=====> scene: 103 mean psnr 16.75554527345504 ssim: 0.8356182752116726 lpips: 0.3762983977794647\n",
      "=====> scene: 114 mean psnr 18.403335481905653 ssim: 0.7632868039029544 lpips: 0.371926873922348\n",
      "=====> all mean psnr 19.30799166164281 ssim: 0.7890963779228286 lpips: 0.3824380591511726\n"
     ]
    }
   ],
   "source": [
    "root = '/mnt/new_disk2/anpei/code/pixel-nerf/visuals/dtu'\n",
    "root_gt = '/mnt/new_disk2/anpei/code/MVS-NeRF/runs_fine_tuning/'\n",
    "pairs = torch.load('/mnt/new_disk_2/anpei/code/MVS-NeRF/configs/pairs.th')\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['chair','drums','ficus','hotdog','lego','materials','mic','ship']):#,\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    pairs_idx = pairs[f'{scene}_val']\n",
    "    for j, file in enumerate(pairs_idx):\n",
    "\n",
    "        idx = pairs_idx[j]\n",
    "        img = cv2.imread(f'{root}/{scene}_{idx:03d}.png')[...,::-1]\n",
    "        gt = cv2.imread(f'{root_gt}/{scene}/{scene}/{scene}_00009999_{j:02d}.png')[...,::-1]\n",
    "        gt, img = gt[:,:800].astype('float')/255.0, img.astype('float')/255.0\n",
    "\n",
    "#         H_crop, W_crop = np.array(gt.shape[:2])//10\n",
    "#         img = img[H_crop:-H_crop,W_crop:-W_crop]\n",
    "#         gt = gt[H_crop:-H_crop,W_crop:-W_crop]\n",
    "\n",
    "        psnr.append( mse2psnr(np.mean((gt-img)**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') \n",
    "\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['fern','flower','fortress','horns','leaves', 'orchids', 'room',  'trex']):#,\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    pairs_idx = pairs[f'{scene}_val']\n",
    "    for j, file in enumerate(pairs_idx):\n",
    "\n",
    "        idx = pairs_idx[j]\n",
    "        img = cv2.resize(cv2.imread(f'{root}/{scene}_{idx:03d}.png')[...,::-1],(960,640))\n",
    "        gt =  cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/{scene}_{idx:03d}.png')[...,::-1]\n",
    "        gt, img = gt[:,:960].astype('float')/255.0, img.astype('float')/255.0\n",
    "\n",
    "\n",
    "        psnr.append( mse2psnr(np.mean((gt-img)**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') \n",
    "\n",
    "psnr_all,ssim_all,LPIPS_vgg_all,depth_acc = [],[],[],[]\n",
    "for i_scene, scene in enumerate([1,8,21,103,114]):#,\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    pairs_idx = pairs[f'dtu_val']\n",
    "    for j, file in enumerate(pairs_idx):\n",
    "\n",
    "        idx = pairs_idx[j]\n",
    "        img = cv2.imread(f'{root}/scan{scene}_{idx:03d}.png')[...,::-1]\n",
    "        \n",
    "        gt = cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/scan{scene}_{idx:03d}.png')[...,::-1]\n",
    "        mask = cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/scan{scene}_{idx:03d}_mask.png')==255\n",
    "        gt, img = gt[:,:640].astype('float')/255.0, img.astype('float')/255.0\n",
    "\n",
    "        gt[mask],img[mask] = 0.0,0.0\n",
    "        psnr.append( mse2psnr(np.mean((gt[~mask]-img[~mask])**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "        \n",
    "        # depth\n",
    "#         depth_pred = torch.load(f'{root}/scan{scene}_{idx:03d}_depth.th')\n",
    "#         depth_gt,_ =  read_depth(f'/mnt/data/new_disk/sungx/data/mvs_dataset/DTU/mvs_training/dtu/Depths/scan{scene}/depth_map_{idx:04d}.pfm')\n",
    "        \n",
    "#         mask_gt = depth_gt>0\n",
    "#         abs_err = abs_error(depth_pred*1.5, depth_gt/200, mask_gt).numpy()\n",
    "\n",
    "#         eval_metric = [0.01,0.05, 0.1]\n",
    "#         depth_acc[f'abs_err'][f'{scene}'] = np.mean(abs_err)\n",
    "#         depth_acc[f'acc_l_{eval_metric[0]}'][f'{scene}'] = acc_threshold(abs_err,eval_metric[0]).mean()\n",
    "#         depth_acc[f'acc_l_{eval_metric[1]}'][f'{scene}'] = acc_threshold(abs_err,eval_metric[1]).mean()\n",
    "#         depth_acc[f'acc_l_{eval_metric[2]}'][f'{scene}'] = acc_threshold(abs_err,eval_metric[2]).mean()\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
