{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys,os,imageio,lpips\n",
    "root = '/home/youngsun/documents/mvs/mvsnerf_timing'\n",
    "os.chdir(root)\n",
    "sys.path.append(root)\n",
    "\n",
    "from opt import config_parser\n",
    "from data import dataset_dict\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# models\n",
    "from models_timer import *\n",
    "from renderer_timer import *\n",
    "from data.ray_utils import get_rays\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from skimage.metrics import structural_similarity\n",
    "\n",
    "# pytorch-lightning\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning import LightningModule, Trainer, loggers\n",
    "\n",
    "\n",
    "from data.ray_utils import ray_marcher\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "torch.cuda.set_device(2)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/envs/mvsnerf/lib/python3.8/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    }
   ],
   "source": [
    "def decode_batch(batch):\n",
    "    rays = batch['rays']  # (B, 8)\n",
    "    rgbs = batch['rgbs']  # (B, 3)\n",
    "    return rays, rgbs\n",
    "\n",
    "def unpreprocess(data, shape=(1,1,3,1,1)):\n",
    "    # to unnormalize image for visualization\n",
    "    # data N V C H W\n",
    "    device = data.device\n",
    "    mean = torch.tensor([-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225]).view(*shape).to(device)\n",
    "    std = torch.tensor([1 / 0.229, 1 / 0.224, 1 / 0.225]).view(*shape).to(device)\n",
    "\n",
    "    return (data - mean) / std\n",
    "\n",
    "def read_depth(filename):\n",
    "    depth_h = np.array(read_pfm(filename)[0], dtype=np.float32) # (800, 800)\n",
    "    depth_h = cv2.resize(depth_h, None, fx=0.5, fy=0.5,\n",
    "                       interpolation=cv2.INTER_NEAREST)  # (600, 800)\n",
    "    depth_h = depth_h[44:556, 80:720]  # (512, 640)\n",
    "#     depth = cv2.resize(depth_h, None, fx=0.5, fy=0.5,interpolation=cv2.INTER_NEAREST)#!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    mask = depth>0\n",
    "    return depth_h,mask\n",
    "\n",
    "loss_fn_vgg = lpips.LPIPS(net='vgg') \n",
    "mse2psnr = lambda x : -10. * np.log(x) / np.log(10.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quantity evauation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os,imageio,lpips,cv2,torch,glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/envs/mvsnerf/lib/python3.8/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    }
   ],
   "source": [
    "loss_fn_vgg = lpips.LPIPS(net='vgg') \n",
    "mse2psnr = lambda x : -10. * np.log(x) / np.log(10.)\n",
    "\n",
    "\n",
    "\n",
    "def acc_threshold(abs_err, threshold):\n",
    "    \"\"\"\n",
    "    computes the percentage of pixels whose depth error is less than @threshold\n",
    "    \"\"\"\n",
    "    acc_mask = abs_err < threshold\n",
    "    return  acc_mask.astype('float') if type(abs_err) is np.ndarray else acc_mask.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTU no fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rendering novel views with nearest 3 views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ckpts ['./ckpts/mvsnerf-v0.tar']\n",
      "Reloading from ./ckpts/mvsnerf-v0.tar\n",
      "============> rendering dataset <===================\n",
      "==> image down scale: 0.75\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 0.75\n",
      "===> valing index: [32, 24, 23, 44]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                   | 0/4 [00:00<?, ?it/s]/tmp/ipykernel_13139/2640702470.py:220: FutureWarning: `multichannel` is a deprecated argument name for `structural_similarity`. It will be removed in version 1.0.Please use `channel_axis` instead.\n",
      "  ssim.append( structural_similarity(rgb_rays, img, multichannel=True))\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:11<00:00,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> scene: 1 mean psnr 23.73473304455396 ssim: 0.8938922882080078 lpips: 0.1892389878630638\n",
      "=====> all mean psnr 23.73473304455396 ssim: 0.8938922882080078 lpips: 0.1892389878630638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "depth_acc = {}\n",
    "eval_metric = [0.1,0.05,0.01]\n",
    "depth_acc[f'abs_err'],depth_acc[f'acc_l_{eval_metric[0]}'],depth_acc[f'acc_l_{eval_metric[1]}'],depth_acc[f'acc_l_{eval_metric[2]}'] = {},{},{},{}\n",
    "   \n",
    "\n",
    "for i_scene, scene in enumerate([1]):#,8,21,103,114\n",
    "\n",
    "    # create timing variables\n",
    "\n",
    "    # measure time - all processes\n",
    "    start_all = torch.cuda.Event(enable_timing=True)\n",
    "    end_all = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # measure time - Feature extraction and neural volume encoding\n",
    "    start_mvsnet = torch.cuda.Event(enable_timing=True)\n",
    "    end_mvsnet = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    # measure time - MVSNeRF and volume rendering\n",
    "    start_mvsnerf = torch.cuda.Event(enable_timing=True)\n",
    "    end_mvsnerf = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    # measure time - MVSNeRF and volume rendering loop\n",
    "    start_loop = torch.cuda.Event(enable_timing=True)\n",
    "    end_loop = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # measure time - MVSNeRF and volume rendering all loops\n",
    "    start_loops = torch.cuda.Event(enable_timing=True)\n",
    "    end_loops = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    # for combining all recorded time\n",
    "    records = []\n",
    "    records_general = {\"0_all\" : [], \"1_loop\" : [], \"2_loops\" : []}\n",
    "    records_mvsnet = {\"0_total\" : [], \"1_feat\" : [], \"2_costvol\" : [], \"3_3dcnn\" : []}\n",
    "    records_mvsnerf = {\"0_total\" : [],\"1_sample\" : [], \"2_nerf\" : [], \"3_rend\" : []}\n",
    "    \n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    cmd = f'--datadir /mnt/hdd/mvsnerf_data/dtu/scan{scene}  \\\n",
    "     --dataset_name dtu_ft  \\\n",
    "     --net_type v0 --ckpt ./ckpts/mvsnerf-v0.tar \\\n",
    "     --imgScale_train 0.75 --imgScale_test 0.75 --img_downscale 0.75'\n",
    "\n",
    "    args = config_parser(cmd.split())\n",
    "    args.use_viewdirs = True\n",
    "\n",
    "    args.N_samples = 128\n",
    "    args.feat_dim =  8+4*3\n",
    "\n",
    "    # create models\n",
    "    if 0==i_scene:\n",
    "        render_kwargs_train, render_kwargs_test, start, grad_vars = create_nerf_mvs(args, use_mvs=True, dir_embedder=False, pts_embedder=True)\n",
    "        filter_keys(render_kwargs_train)\n",
    "\n",
    "        MVSNet = render_kwargs_train['network_mvs']\n",
    "        render_kwargs_train.pop('network_mvs')\n",
    "\n",
    "\n",
    "    datadir = args.datadir\n",
    "    datatype = 'train'\n",
    "    pad = 16\n",
    "    args.chunk = 5120\n",
    "\n",
    "\n",
    "    print('============> rendering dataset <===================')\n",
    "    dataset_train = dataset_dict[args.dataset_name](args, split='train')\n",
    "    dataset_val = dataset_dict[args.dataset_name](args, split='val')\n",
    "    val_idx = dataset_val.img_idx\n",
    "\n",
    "    save_as_image = True\n",
    "    save_dir = f'results/test_dtu'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    MVSNet.train()\n",
    "    MVSNet = MVSNet.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        try:\n",
    "            tqdm._instances.clear() \n",
    "        except Exception:     \n",
    "            pass       \n",
    "\n",
    "        for i, batch in enumerate(tqdm(dataset_val)):\n",
    "                        \n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            ##################\n",
    "            # time everything\n",
    "            ##################\n",
    "            start_all.record()\n",
    "            #\n",
    "\n",
    "            rays, img = decode_batch(batch)\n",
    "            rays = rays.squeeze().to(device)  # (H*W, 3)\n",
    "            img = img.squeeze().cpu().numpy()  # (H, W, 3)\n",
    "            depth = batch['depth'].squeeze().numpy()  # (H, W)\n",
    "\n",
    "            # find nearest image idx from training views\n",
    "            positions = dataset_train.poses[:,:3,3]\n",
    "            dis = np.sum(np.abs(positions - dataset_val.poses[[i],:3,3]), axis=-1)\n",
    "            pair_idx = np.argsort(dis)[:3]\n",
    "            pair_idx = [dataset_train.img_idx[item] for item in pair_idx]\n",
    "\n",
    "            imgs_source, proj_mats, near_far_source, pose_source = dataset_train.read_source_views(pair_idx=pair_idx,device=device)\n",
    "            \n",
    "            ##################\n",
    "            # time mvsnet 0\n",
    "            ##################\n",
    "            start_mvsnet.record()\n",
    "            #\n",
    "            volume_feature, _, _, records_mvsnet = MVSNet(imgs_source, proj_mats, near_far_source, records=records_mvsnet, pad=pad)\n",
    "            #\n",
    "            end_mvsnet.record()\n",
    "            torch.cuda.synchronize()\n",
    "            records_mvsnet['0_total'].append(start_mvsnet.elapsed_time(end_mvsnet))\n",
    "            ##############################################################\n",
    "            \n",
    "                \n",
    "            imgs_source = unpreprocess(imgs_source)\n",
    "\n",
    "            N_rays_all = rays.shape[0]\n",
    "            rgb_rays, depth_rays_preds = [],[]\n",
    "            \n",
    "            ##################\n",
    "            # time loops\n",
    "            ##################\n",
    "            start_loops.record()\n",
    "            #\n",
    "            \n",
    "            for chunk_idx in range(N_rays_all//args.chunk + int(N_rays_all%args.chunk>0)):\n",
    "                \n",
    "                # for loop timing\n",
    "                ##################\n",
    "                # time loop\n",
    "                ##################\n",
    "                start_loop.record()\n",
    "                #\n",
    "                xyz_coarse_sampled, rays_o, rays_d, z_vals = ray_marcher(rays[chunk_idx*args.chunk:(chunk_idx+1)*args.chunk],\n",
    "                                                    N_samples=args.N_samples)\n",
    "\n",
    "                # Converting world coordinate to ndc coordinate\n",
    "                H, W = img.shape[:2]\n",
    "                inv_scale = torch.tensor([W - 1, H - 1]).to(device)\n",
    "                w2c_ref, intrinsic_ref = pose_source['w2cs'][0], pose_source['intrinsics'][0].clone()\n",
    "                xyz_NDC = get_ndc_coordinate(w2c_ref, intrinsic_ref, xyz_coarse_sampled, inv_scale,\n",
    "                                             near=near_far_source[0], far=near_far_source[1], pad=pad*args.imgScale_test)\n",
    "\n",
    "\n",
    "                # rendering\n",
    "                \n",
    "                ##################\n",
    "                # time mvsnerf 0\n",
    "                ##################\n",
    "                start_mvsnerf.record()\n",
    "                #\n",
    "                rgb, disp, acc, depth_pred, alpha, extras, records_mvsnerf = rendering(args, pose_source, xyz_coarse_sampled,\n",
    "                                                                       xyz_NDC, z_vals, rays_o, rays_d, records_mvsnerf,\n",
    "                                                                       volume_feature,imgs_source, **render_kwargs_train)\n",
    "                #\n",
    "                end_mvsnerf.record()\n",
    "                torch.cuda.synchronize()\n",
    "                records_mvsnerf['0_total'].append(start_mvsnerf.elapsed_time(end_mvsnerf))\n",
    "                ##############################################################\n",
    "                \n",
    "                \n",
    "\n",
    "                rgb, depth_pred = torch.clamp(rgb.cpu(),0,1.0).numpy(), depth_pred.cpu().numpy()\n",
    "                rgb_rays.append(rgb)\n",
    "                depth_rays_preds.append(depth_pred)\n",
    "                \n",
    "                #\n",
    "                end_loop.record()\n",
    "                torch.cuda.synchronize()\n",
    "                records_general['1_loop'].append(start_loop.elapsed_time(end_loop))\n",
    "                ##############################################################\n",
    "            \n",
    "            #\n",
    "            end_loops.record()\n",
    "            torch.cuda.synchronize()\n",
    "            records_general['2_loops'].append(start_loops.elapsed_time(end_loops))\n",
    "            ##############################################################\n",
    "            \n",
    "            #\n",
    "            end_all.record()\n",
    "            torch.cuda.synchronize()\n",
    "            records_general['0_all'].append(start_all.elapsed_time(end_all))\n",
    "            ##############################################################\n",
    "            \n",
    "            \n",
    "            depth_rays_preds = np.concatenate(depth_rays_preds).reshape(H, W)\n",
    "\n",
    "            depth_gt, _ =  read_depth(f'/mnt/hdd/mvsnerf_data/dtu/Depths/scan{scene}/depth_map_{val_idx[i]:04d}.pfm')\n",
    "\n",
    "            # commented out because prediction and gt shape mismatch\n",
    "#             mask_gt = depth_gt>0\n",
    "#             abs_err = abs_error(depth_rays_preds, depth_gt/200, mask_gt)\n",
    "\n",
    "#             eval_metric = [0.01,0.05, 0.1]\n",
    "#             depth_acc[f'abs_err'][f'{scene}'] = np.mean(abs_err)\n",
    "#             depth_acc[f'acc_l_{eval_metric[0]}'][f'{scene}'] = acc_threshold(abs_err,eval_metric[0]).mean()\n",
    "#             depth_acc[f'acc_l_{eval_metric[1]}'][f'{scene}'] = acc_threshold(abs_err,eval_metric[1]).mean()\n",
    "#             depth_acc[f'acc_l_{eval_metric[2]}'][f'{scene}'] = acc_threshold(abs_err,eval_metric[2]).mean()\n",
    "\n",
    "\n",
    "            depth_rays_preds, _ = visualize_depth_numpy(depth_rays_preds, near_far_source)\n",
    "\n",
    "            rgb_rays = np.concatenate(rgb_rays).reshape(H, W, 3)\n",
    "            img_vis = np.concatenate((img*255,rgb_rays*255,depth_rays_preds),axis=1)\n",
    "\n",
    "            if save_as_image:\n",
    "                imageio.imwrite(f'{save_dir}/scan{scene}_{val_idx[i]:03d}.png', img_vis.astype('uint8'))\n",
    "            else:\n",
    "                rgbs.append(img_vis.astype('uint8'))\n",
    "\n",
    "            # quantity\n",
    "            # mask background since they are outside the far boundle\n",
    "            mask = depth==0\n",
    "            imageio.imwrite(f'{save_dir}/scan{scene}_{val_idx[i]:03d}_mask.png', mask.astype('uint8')*255)\n",
    "            rgb_rays[mask],img[mask] = 0.0,0.0\n",
    "            psnr.append( mse2psnr(np.mean((rgb_rays[~mask]-img[~mask])**2)))\n",
    "            ssim.append( structural_similarity(rgb_rays, img, multichannel=True))\n",
    "\n",
    "            img_tensor = torch.from_numpy(rgb_rays)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "            img_gt_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0\n",
    "            LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "        print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "        psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "\n",
    "\n",
    "    if not save_as_image:\n",
    "        imageio.mimwrite(f'{save_dir}/{scene}_spiral.mp4', np.stack(rgbs), fps=20, quality=10)\n",
    "\n",
    "# a = np.mean(list(depth_acc['abs_err'].values()))\n",
    "# b = np.mean(list(depth_acc[f'acc_l_{eval_metric[0]}'].values()))\n",
    "# c = np.mean(list(depth_acc[f'acc_l_{eval_metric[1]}'].values()))\n",
    "# d = np.mean(list(depth_acc[f'acc_l_{eval_metric[2]}'].values()))\n",
    "# print(f'============> abs_err: {a} <=================')\n",
    "# print(f'============> acc_l_{eval_metric[0]}: {b} <=================')\n",
    "# print(f'============> acc_l_{eval_metric[1]}: {c} <=================')\n",
    "# print(f'============> acc_l_{eval_metric[2]}: {d} <=================')\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_and_record(records_processed, name='test', scenes=[1], num_src=3, img_scale=1.0, save_as_image=True):\n",
    "    \n",
    "    psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "    \n",
    "    for i_scene, scene in enumerate(scenes):#,8,21,103,114\n",
    "        \n",
    "        psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "        \n",
    "        # create timing variables\n",
    "\n",
    "        # measure time - all processes\n",
    "        start_all = torch.cuda.Event(enable_timing=True)\n",
    "        end_all = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        # measure time - Feature extraction and neural volume encoding\n",
    "        start_mvsnet = torch.cuda.Event(enable_timing=True)\n",
    "        end_mvsnet = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        # measure time - MVSNeRF and volume rendering\n",
    "        start_mvsnerf = torch.cuda.Event(enable_timing=True)\n",
    "        end_mvsnerf = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        # measure time - MVSNeRF and volume rendering loop\n",
    "        start_loop = torch.cuda.Event(enable_timing=True)\n",
    "        end_loop = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        # measure time - MVSNeRF and volume rendering all loops\n",
    "        start_loops = torch.cuda.Event(enable_timing=True)\n",
    "        end_loops = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        # for combining all recorded time\n",
    "        records = []\n",
    "        records_general = {\"0_all\" : [], \"1_loop\" : [], \"2_loops\" : []}\n",
    "        records_mvsnet = {\"0_total\" : [], \"1_feat\" : [], \"2_costvol\" : [], \"3_3dcnn\" : []}\n",
    "        records_mvsnerf = {\"0_total\" : [],\"1_sample\" : [], \"2_nerf\" : [], \"3_rend\" : []}\n",
    "\n",
    "        cmd = f'--datadir /mnt/hdd/mvsnerf_data/dtu/scan{scene}  \\\n",
    "         --dataset_name dtu_ft  \\\n",
    "         --net_type v0 --ckpt ./ckpts/mvsnerf-v0.tar \\\n",
    "         --imgScale_train {img_scale} --imgScale_test {img_scale} --img_downscale {img_scale}'\n",
    "\n",
    "        args = config_parser(cmd.split())\n",
    "        args.use_viewdirs = True\n",
    "\n",
    "        args.N_samples = 128\n",
    "        args.feat_dim =  8+4*num_src\n",
    "\n",
    "        # create models\n",
    "        if 0==i_scene:\n",
    "            render_kwargs_train, render_kwargs_test, start, grad_vars = create_nerf_mvs(args, use_mvs=True, dir_embedder=False, pts_embedder=True)\n",
    "            filter_keys(render_kwargs_train)\n",
    "\n",
    "            MVSNet = render_kwargs_train['network_mvs']\n",
    "            render_kwargs_train.pop('network_mvs')\n",
    "\n",
    "\n",
    "        datadir = args.datadir\n",
    "        datatype = 'train'\n",
    "        pad = 16\n",
    "        args.chunk = 5120\n",
    "\n",
    "\n",
    "        print('============> rendering dataset <===================')\n",
    "        dataset_train = dataset_dict[args.dataset_name](args, split='train')\n",
    "        dataset_val = dataset_dict[args.dataset_name](args, split='val')\n",
    "        val_idx = dataset_val.img_idx\n",
    "\n",
    "        save_as_image = True\n",
    "        save_dir = f'results/test_dtu_{name}'\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        MVSNet.train()\n",
    "        MVSNet = MVSNet.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            try:\n",
    "                tqdm._instances.clear() \n",
    "            except Exception:     \n",
    "                pass       \n",
    "\n",
    "            for i, batch in enumerate(tqdm(dataset_val)):\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                ##################\n",
    "                # time everything\n",
    "                ##################\n",
    "                start_all.record()\n",
    "                #\n",
    "\n",
    "                rays, img = decode_batch(batch)\n",
    "                rays = rays.squeeze().to(device)  # (H*W, 3)\n",
    "                img = img.squeeze().cpu().numpy()  # (H, W, 3)\n",
    "                depth = batch['depth'].squeeze().numpy()  # (H, W)\n",
    "\n",
    "                # find nearest image idx from training views\n",
    "                positions = dataset_train.poses[:,:3,3]\n",
    "                dis = np.sum(np.abs(positions - dataset_val.poses[[i],:3,3]), axis=-1)\n",
    "                pair_idx = np.argsort(dis)[:num_src]\n",
    "                pair_idx = [dataset_train.img_idx[item] for item in pair_idx]\n",
    "\n",
    "                imgs_source, proj_mats, near_far_source, pose_source = dataset_train.read_source_views(pair_idx=pair_idx,device=device)\n",
    "\n",
    "                ##################\n",
    "                # time mvsnet 0\n",
    "                ##################\n",
    "                start_mvsnet.record()\n",
    "                #\n",
    "                volume_feature, _, _, records_mvsnet = MVSNet(imgs_source, proj_mats, near_far_source, records=records_mvsnet, pad=pad)\n",
    "                #\n",
    "                end_mvsnet.record()\n",
    "                torch.cuda.synchronize()\n",
    "                records_mvsnet['0_total'].append(start_mvsnet.elapsed_time(end_mvsnet))\n",
    "                ##############################################################\n",
    "\n",
    "\n",
    "                imgs_source = unpreprocess(imgs_source)\n",
    "\n",
    "                N_rays_all = rays.shape[0]\n",
    "                rgb_rays, depth_rays_preds = [],[]\n",
    "\n",
    "                ##################\n",
    "                # time loops\n",
    "                ##################\n",
    "                start_loops.record()\n",
    "                #\n",
    "\n",
    "                for chunk_idx in range(N_rays_all//args.chunk + int(N_rays_all%args.chunk>0)):\n",
    "\n",
    "                    # for loop timing\n",
    "                    ##################\n",
    "                    # time loop\n",
    "                    ##################\n",
    "                    start_loop.record()\n",
    "                    #\n",
    "                    xyz_coarse_sampled, rays_o, rays_d, z_vals = ray_marcher(rays[chunk_idx*args.chunk:(chunk_idx+1)*args.chunk],\n",
    "                                                        N_samples=args.N_samples)\n",
    "\n",
    "                    # Converting world coordinate to ndc coordinate\n",
    "                    H, W = img.shape[:2]\n",
    "                    inv_scale = torch.tensor([W - 1, H - 1]).to(device)\n",
    "                    w2c_ref, intrinsic_ref = pose_source['w2cs'][0], pose_source['intrinsics'][0].clone()\n",
    "                    xyz_NDC = get_ndc_coordinate(w2c_ref, intrinsic_ref, xyz_coarse_sampled, inv_scale,\n",
    "                                                 near=near_far_source[0], far=near_far_source[1], pad=pad*args.imgScale_test)\n",
    "\n",
    "\n",
    "                    # rendering\n",
    "\n",
    "                    ##################\n",
    "                    # time mvsnerf 0\n",
    "                    ##################\n",
    "                    start_mvsnerf.record()\n",
    "                    #\n",
    "                    rgb, disp, acc, depth_pred, alpha, extras, records_mvsnerf = rendering(args, pose_source, xyz_coarse_sampled,\n",
    "                                                                           xyz_NDC, z_vals, rays_o, rays_d, records_mvsnerf,\n",
    "                                                                           volume_feature,imgs_source, **render_kwargs_train)\n",
    "                    #\n",
    "                    end_mvsnerf.record()\n",
    "                    torch.cuda.synchronize()\n",
    "                    records_mvsnerf['0_total'].append(start_mvsnerf.elapsed_time(end_mvsnerf))\n",
    "                    ##############################################################\n",
    "\n",
    "\n",
    "\n",
    "                    rgb, depth_pred = torch.clamp(rgb.cpu(),0,1.0).numpy(), depth_pred.cpu().numpy()\n",
    "                    rgb_rays.append(rgb)\n",
    "                    depth_rays_preds.append(depth_pred)\n",
    "\n",
    "                    #\n",
    "                    end_loop.record()\n",
    "                    torch.cuda.synchronize()\n",
    "                    records_general['1_loop'].append(start_loop.elapsed_time(end_loop))\n",
    "                    ##############################################################\n",
    "\n",
    "                #\n",
    "                end_loops.record()\n",
    "                torch.cuda.synchronize()\n",
    "                records_general['2_loops'].append(start_loops.elapsed_time(end_loops))\n",
    "                ##############################################################\n",
    "\n",
    "                #\n",
    "                end_all.record()\n",
    "                torch.cuda.synchronize()\n",
    "                records_general['0_all'].append(start_all.elapsed_time(end_all))\n",
    "                ##############################################################\n",
    "                \n",
    "                depth_rays_preds = np.concatenate(depth_rays_preds).reshape(H, W)\n",
    "                depth_rays_preds, _ = visualize_depth_numpy(depth_rays_preds, near_far_source)\n",
    "\n",
    "                rgb_rays = np.concatenate(rgb_rays).reshape(H, W, 3)\n",
    "                img_vis = np.concatenate((img*255,rgb_rays*255,depth_rays_preds),axis=1)\n",
    "\n",
    "                if save_as_image:\n",
    "                    imageio.imwrite(f'{save_dir}/scan{scene}_{val_idx[i]:03d}.png', img_vis.astype('uint8'))\n",
    "                \n",
    "                # quantity\n",
    "                # mask background since they are outside the far boundle\n",
    "                mask = depth==0\n",
    "                imageio.imwrite(f'{save_dir}/scan{scene}_{val_idx[i]:03d}_mask.png', mask.astype('uint8')*255)\n",
    "                rgb_rays[mask],img[mask] = 0.0,0.0\n",
    "                psnr.append( mse2psnr(np.mean((rgb_rays[~mask]-img[~mask])**2)))\n",
    "                ssim.append( structural_similarity(rgb_rays, img, multichannel=True))\n",
    "\n",
    "                img_tensor = torch.from_numpy(rgb_rays)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "                img_gt_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0\n",
    "                LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "            psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "\n",
    "    records_processed = append_records(records_processed, records_general, records_mvsnet, records_mvsnerf, \n",
    "                                       np.mean(psnr_all), np.mean(ssim_all), np.mean(LPIPS_vgg_all), \n",
    "                                       name)\n",
    "            \n",
    "    return records_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_records():\n",
    "    return {'experiment name':[],\n",
    "            'novel scenes synthesized':[],\n",
    "            'psnr':[],\n",
    "            'ssim':[],\n",
    "            'lpips':[],\n",
    "            'total time':[],\n",
    "            'mvsnet total time':[],\n",
    "            'mvsnet feature extraction':[],\n",
    "            'mvsnet cost volume':[],\n",
    "            'mvsnet 3D-CNN':[],\n",
    "            'mvsnerf total time':[],\n",
    "            'mvsnerf volume sampling':[],\n",
    "            'mvsnerf nerf':[],\n",
    "            'mvsnerf rendering':[]\n",
    "           }\n",
    "\n",
    "def append_records(records_processed, records_general, records_mvsnet, records_mvsnerf, \n",
    "                   psnr, ssim, lpips,\n",
    "                   name='this experiment wants a name'):\n",
    "\n",
    "    records_processed['experiment name'] += [f'{name}']\n",
    "    records_processed['novel scenes synthesized'] += [len(records_general['0_all'])]\n",
    "    \n",
    "    records_processed['psnr'] += [psnr]\n",
    "    records_processed['ssim'] += [ssim]\n",
    "    records_processed['lpips'] += [lpips]\n",
    "    \n",
    "    records_processed['total time'] += [np.mean(records_general['0_all'])]\n",
    "\n",
    "    records_processed['mvsnet total time'] += [np.mean(records_mvsnet['0_total'])]\n",
    "    records_processed['mvsnet feature extraction'] += [np.mean(records_mvsnet['1_feat'])]\n",
    "    records_processed['mvsnet cost volume'] += [np.mean(records_mvsnet['2_costvol'])]\n",
    "    records_processed['mvsnet 3D-CNN'] += [np.mean(records_mvsnet['3_3dcnn'])]\n",
    "    \n",
    "    records_processed['mvsnerf total time'] += [np.mean(records_mvsnerf['0_total']) * len (records_mvsnerf['0_total']) / len(records_general['0_all'])]\n",
    "    records_processed['mvsnerf volume sampling'] += [np.mean(records_mvsnerf['1_sample']) * len (records_mvsnerf['1_sample']) / len(records_general['0_all'])]\n",
    "    records_processed['mvsnerf nerf'] += [np.mean(records_mvsnerf['2_nerf']) * len (records_mvsnerf['2_nerf']) / len(records_general['0_all'])]\n",
    "    records_processed['mvsnerf rendering'] += [np.mean(records_mvsnerf['3_rend']) * len (records_mvsnerf['3_rend']) / len(records_general['0_all'])]\n",
    "\n",
    "    return records_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_processed = init_records()\n",
    "\n",
    "records_processed = append_records(records_processed, records_general, records_mvsnet, records_mvsnerf, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_processed = append_records(records_processed, records_general, records_mvsnet, records_mvsnerf, 'test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experiment name': ['test', 'test2'],\n",
       " 'total time': [1601.7471313476562, 1601.7471313476562],\n",
       " 'novel scenes synthesized': [4, 4],\n",
       " 'mvsnet total time': [61.2093448638916, 61.2093448638916],\n",
       " 'mvsnet feature extraction': [4.20684814453125, 4.20684814453125],\n",
       " 'mvsnet cost volume': [17.603840351104736, 17.603840351104736],\n",
       " 'mvsnet 3D-CNN': [39.14752006530762, 39.14752006530762],\n",
       " 'mvsnerf total time': [1451.4196434020996, 1451.4196434020996],\n",
       " 'mvsnerf volume sampling': [77.8173440694809, 77.8173440694809],\n",
       " 'mvsnerf nerf': [1349.6506843566895, 1349.6506843566895],\n",
       " 'mvsnerf rendering': [13.000448025763035, 13.000448025763035]}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ckpts ['./ckpts/mvsnerf-v0.tar']\n",
      "Reloading from ./ckpts/mvsnerf-v0.tar\n",
      "============> rendering dataset <===================\n",
      "==> image down scale: 0.5\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 0.5\n",
      "===> valing index: [32, 24, 23, 44]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                   | 0/4 [00:00<?, ?it/s]/tmp/ipykernel_13139/2670735445.py:202: FutureWarning: `multichannel` is a deprecated argument name for `structural_similarity`. It will be removed in version 1.0.Please use `channel_axis` instead.\n",
      "  ssim.append( structural_similarity(rgb_rays, img, multichannel=True))\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:05<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============> rendering dataset <===================\n",
      "==> image down scale: 0.5\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 0.5\n",
      "===> valing index: [32, 24, 23, 44]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:05<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============> rendering dataset <===================\n",
      "==> image down scale: 0.5\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 0.5\n",
      "===> valing index: [32, 24, 23, 44]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:05<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============> rendering dataset <===================\n",
      "==> image down scale: 0.5\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 0.5\n",
      "===> valing index: [32, 24, 23, 44]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:05<00:00,  1.38s/it]\n"
     ]
    }
   ],
   "source": [
    "## 이걸로 계속 돌리고 쌓으면 됨!\n",
    "\n",
    "records_processed = init_records()\n",
    "records_processed = experiment_and_record(records_processed, name='test3', scenes=[1,2,3,4], num_src=3, img_scale=0.5, save_as_image=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ckpts ['./ckpts/mvsnerf-v0.tar']\n",
      "Reloading from ./ckpts/mvsnerf-v0.tar\n",
      "============> rendering dataset <===================\n",
      "==> image down scale: 0.5\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 0.5\n",
      "===> valing index: [32, 24, 23, 44]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                   | 0/4 [00:00<?, ?it/s]/tmp/ipykernel_13139/674310418.py:202: FutureWarning: `multichannel` is a deprecated argument name for `structural_similarity`. It will be removed in version 1.0.Please use `channel_axis` instead.\n",
      "  ssim.append( structural_similarity(rgb_rays, img, multichannel=True))\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:05<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> scene: 1 mean psnr 19.779164493385384 ssim: 0.8381005525588989 lpips: 0.2410932220518589\n",
      "============> rendering dataset <===================\n",
      "==> image down scale: 0.5\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 0.5\n",
      "===> valing index: [32, 24, 23, 44]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:05<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> scene: 2 mean psnr 17.855236492613876 ssim: 0.7069318890571594 lpips: 0.25165697932243347\n",
      "============> rendering dataset <===================\n",
      "==> image down scale: 0.5\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 0.5\n",
      "===> valing index: [32, 24, 23, 44]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:04<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> scene: 3 mean psnr 20.201428833914655 ssim: 0.8010879158973694 lpips: 0.2353186160326004\n",
      "============> rendering dataset <===================\n",
      "==> image down scale: 0.5\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 0.5\n",
      "===> valing index: [32, 24, 23, 44]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:05<00:00,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> scene: 4 mean psnr 18.070365974106757 ssim: 0.8202603459358215 lpips: 0.19477754086256027\n",
      "=====> all mean psnr 18.97654894850517 ssim: 0.7915952205657959 lpips: 0.23071158956736326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "records_processed = experiment_and_record(records_processed, name='test3', scenes=[1,2,3,4], num_src=3, img_scale=0.5, save_as_image=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment name</th>\n",
       "      <th>novel scenes synthesized</th>\n",
       "      <th>psnr</th>\n",
       "      <th>ssim</th>\n",
       "      <th>lpips</th>\n",
       "      <th>total time</th>\n",
       "      <th>mvsnet total time</th>\n",
       "      <th>mvsnet feature extraction</th>\n",
       "      <th>mvsnet cost volume</th>\n",
       "      <th>mvsnet 3D-CNN</th>\n",
       "      <th>mvsnerf total time</th>\n",
       "      <th>mvsnerf volume sampling</th>\n",
       "      <th>mvsnerf nerf</th>\n",
       "      <th>mvsnerf rendering</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test3</td>\n",
       "      <td>4</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.23</td>\n",
       "      <td>741.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>10.8</td>\n",
       "      <td>20.7</td>\n",
       "      <td>631.0</td>\n",
       "      <td>32.4</td>\n",
       "      <td>588.2</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  experiment name  novel scenes synthesized  psnr  ssim  lpips  total time  \\\n",
       "0           test3                         4  19.0  0.79   0.23       741.0   \n",
       "\n",
       "   mvsnet total time  mvsnet feature extraction  mvsnet cost volume  \\\n",
       "0               36.0                        4.5                10.8   \n",
       "\n",
       "   mvsnet 3D-CNN  mvsnerf total time  mvsnerf volume sampling  mvsnerf nerf  \\\n",
       "0           20.7               631.0                     32.4         588.2   \n",
       "\n",
       "   mvsnerf rendering  \n",
       "0                5.6  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(records_processed).round({'psnr':1, 'ssim':2, 'lpips':2, \n",
    "                                       'total time':0, 'mvsnet total time':0, 'mvsnerf total time':0, \n",
    "                                       'mvsnet feature extraction' : 1, 'mvsnet cost volume':1, 'mvsnet 3D-CNN':1,\n",
    "                                       'mvsnerf volume sampling': 1, 'mvsnerf nerf' : 1, 'mvsnerf rendering' : 1\n",
    "                                      })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rendering novel views with fixed 3 source views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate([1,8,21,103,114]):#,\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    cmd = f'--datadir /mnt/data/new_disk/sungx/data/mvs_dataset/DTU/mvs_training/dtu/scan{scene}  \\\n",
    "    --dataset_name dtu_ft  \\\n",
    "    --ckpt ./ckpts//mvsnerf-v0.tar'\n",
    "\n",
    "    args = config_parser(cmd.split())\n",
    "    args.use_viewdirs = True\n",
    "\n",
    "    args.N_samples = 128\n",
    "    args.feat_dim =  8+3*4\n",
    "\n",
    "    # create models\n",
    "    if 0==i_scene:\n",
    "        render_kwargs_train, render_kwargs_test, start, grad_vars = create_nerf_mvs(args, use_mvs=True, dir_embedder=False, pts_embedder=True)\n",
    "        filter_keys(render_kwargs_train)\n",
    "\n",
    "        MVSNet = render_kwargs_train['network_mvs']\n",
    "        render_kwargs_train.pop('network_mvs')\n",
    "\n",
    "\n",
    "    datadir = args.datadir\n",
    "    datatype = 'val'\n",
    "    pad = 24\n",
    "    args.chunk = 5120\n",
    "\n",
    "\n",
    "    print('============> rendering dataset <===================')\n",
    "    dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "    val_idx = dataset.img_idx\n",
    "    \n",
    "    save_as_image = True\n",
    "    save_dir = f'results/test3'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    MVSNet.train()\n",
    "    MVSNet = MVSNet.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)\n",
    "        volume_feature, _, _ = MVSNet(imgs_source, proj_mats, near_far_source, pad=pad)\n",
    "        imgs_source = unpreprocess(imgs_source)\n",
    "\n",
    "        try:\n",
    "            tqdm._instances.clear() \n",
    "        except Exception:     \n",
    "            pass\n",
    "        \n",
    "        for i, batch in enumerate(tqdm(dataset)):\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            rays, img = decode_batch(batch)\n",
    "            rays = rays.squeeze().to(device)  # (H*W, 3)\n",
    "            img = img.squeeze().cpu().numpy()  # (H, W, 3)\n",
    "            depth = batch['depth'].squeeze().numpy()  # (H, W)\n",
    "        \n",
    "            N_rays_all = rays.shape[0]\n",
    "            rgb_rays, depth_rays_preds = [],[]\n",
    "            for chunk_idx in range(N_rays_all//args.chunk + int(N_rays_all%args.chunk>0)):\n",
    "\n",
    "                xyz_coarse_sampled, rays_o, rays_d, z_vals = ray_marcher(rays[chunk_idx*args.chunk:(chunk_idx+1)*args.chunk],\n",
    "                                                    N_samples=args.N_samples)\n",
    "\n",
    "                # Converting world coordinate to ndc coordinate\n",
    "                H, W = img.shape[:2]\n",
    "                inv_scale = torch.tensor([W - 1, H - 1]).to(device)\n",
    "                w2c_ref, intrinsic_ref = pose_source['w2cs'][0], pose_source['intrinsics'][0].clone()\n",
    "                xyz_NDC = get_ndc_coordinate(w2c_ref, intrinsic_ref, xyz_coarse_sampled, inv_scale,\n",
    "                                             near=near_far_source[0], far=near_far_source[1], pad=pad*args.imgScale_test)\n",
    "\n",
    "\n",
    "                # rendering\n",
    "                rgb, disp, acc, depth_pred, alpha, extras = rendering(args, pose_source, xyz_coarse_sampled,\n",
    "                                                                       xyz_NDC, z_vals, rays_o, rays_d,\n",
    "                                                                       volume_feature,imgs_source, **render_kwargs_train)\n",
    "    \n",
    "                \n",
    "                rgb, depth_pred = torch.clamp(rgb.cpu(),0,1.0).numpy(), depth_pred.cpu().numpy()\n",
    "                rgb_rays.append(rgb)\n",
    "                depth_rays_preds.append(depth_pred)\n",
    "\n",
    "            \n",
    "            depth_rays_preds = np.concatenate(depth_rays_preds).reshape(H, W)\n",
    "            depth_rays_preds, _ = visualize_depth_numpy(depth_rays_preds, near_far_source)\n",
    "            \n",
    "            rgb_rays = np.concatenate(rgb_rays).reshape(H, W, 3)\n",
    "            img_vis = np.concatenate((img*255,rgb_rays*255,depth_rays_preds),axis=1)\n",
    "            \n",
    "            if save_as_image:\n",
    "                imageio.imwrite(f'{save_dir}/scan{scene}_{val_idx[i]:03d}.png', img_vis.astype('uint8'))\n",
    "            else:\n",
    "                rgbs.append(img_vis.astype('uint8'))\n",
    "                \n",
    "            # quantity\n",
    "            # mask background since they are outside the far boundle\n",
    "            mask = depth==0\n",
    "            imageio.imwrite(f'{save_dir}/scan{scene}_{val_idx[i]:03d}_mask.png', mask.astype('uint8')*255)\n",
    "            rgb_rays[mask],img[mask] = 0.0,0.0\n",
    "            psnr.append( mse2psnr(np.mean((rgb_rays[~mask]-img[~mask])**2)))\n",
    "            ssim.append( structural_similarity(rgb_rays, img, multichannel=True))\n",
    "            \n",
    "            img_tensor = torch.from_numpy(rgb_rays)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "            img_gt_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0\n",
    "            LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "        print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "        psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "\n",
    "    if not save_as_image:\n",
    "        imageio.mimwrite(f'{save_dir}/{scene}_spiral.mp4', np.stack(rgbs), fps=20, quality=10)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairs generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,torch\n",
    "import sys,os\n",
    "import numpy as np\n",
    "root = '/home/hengfei/Desktop/research/mvsnerf'\n",
    "os.chdir(root)\n",
    "sys.path.append(root)\n",
    "pairs = torch.load('./configs/pairs.th')\n",
    "\n",
    "# llff\n",
    "root_dir = '/home/hengfei/Desktop/research/mvsnerf/xgaze/'\n",
    "for scene in ['xgaze_11images_cropped_colmapCODE']:#\n",
    "    poses_bounds = np.load(os.path.join(root_dir, scene, 'poses_bounds.npy'))  # (N_images, 11)\n",
    "    poses = poses_bounds[:, :15].reshape(-1, 3, 5)  # (N_images, 3, 5)\n",
    "    poses = np.concatenate([poses[..., 1:2], - poses[..., :1], poses[..., 2:4]], -1)\n",
    "\n",
    "    ref_position = np.mean(poses[..., 3],axis=0, keepdims=True)\n",
    "    dist = np.sum(np.abs(poses[..., 3] - ref_position), axis=-1)\n",
    "    pair_idx = np.argsort(dist)[:11]\n",
    "#     pair_idx = torch.randperm(len(poses))[:20].tolist()\n",
    "\n",
    "    pairs[f'{scene}_test'] = pair_idx[::6]\n",
    "    pairs[f'{scene}_val'] = pair_idx[::6]\n",
    "    pairs[f'{scene}_train'] = np.delete(pair_idx, range(0,11,6))\n",
    "\n",
    "torch.save(pairs,'/home/hengfei/Desktop/research/mvsnerf/configs/pairs.th')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nerf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> scene: chair mean psnr 31.070870959993407 ssim: 0.970869913728838 lpips: 0.05512496456503868\n",
      "=====> scene: drums mean psnr 25.464383523724557 ssim: 0.9430287321705997 lpips: 0.1010842639952898\n",
      "=====> scene: ficus mean psnr 29.72717081186501 ssim: 0.9688198661712594 lpips: 0.04721927270293236\n",
      "=====> scene: hotdog mean psnr 34.63162021512352 ssim: 0.9798700143526428 lpips: 0.0885334312915802\n",
      "=====> scene: lego mean psnr 32.65761069614622 ssim: 0.9751430050524844 lpips: 0.05375238787382841\n",
      "=====> scene: materials mean psnr 30.220202654922936 ssim: 0.9677394226502894 lpips: 0.1052329633384943\n",
      "=====> scene: mic mean psnr 31.810551677509977 ssim: 0.9810118386928188 lpips: 0.03268271638080478\n",
      "=====> scene: ship mean psnr 29.487980342358682 ssim: 0.9079920156014059 lpips: 0.2625834122300148\n",
      "=====> all mean psnr 30.633798860205538 ssim: 0.9618093510525423 lpips: 0.09327667654724792\n",
      "=====> scene: fern mean psnr 23.87081932481545 ssim: 0.828319405500272 lpips: 0.29106350988149643\n",
      "=====> scene: flower mean psnr 26.84248375485232 ssim: 0.8972176342834637 lpips: 0.175886869430542\n",
      "=====> scene: fortress mean psnr 31.368287455491632 ssim: 0.9454387223441942 lpips: 0.14687807857990265\n",
      "=====> scene: horns mean psnr 25.957297279910254 ssim: 0.9002932801372645 lpips: 0.24746065214276314\n",
      "=====> scene: leaves mean psnr 21.209230306630403 ssim: 0.7924383925048237 lpips: 0.3013022392988205\n",
      "=====> scene: orchids mean psnr 19.805083676014405 ssim: 0.7216119459229409 lpips: 0.3210122361779213\n",
      "=====> scene: room mean psnr 33.538893189163886 ssim: 0.9782926576909046 lpips: 0.15664737671613693\n",
      "=====> scene: trex mean psnr 25.191124840989897 ssim: 0.8986906362165991 lpips: 0.24522138014435768\n",
      "=====> all mean psnr 25.972902478483533 ssim: 0.8702878343250579 lpips: 0.23568404279649258\n",
      "=====> scene: 1 mean psnr 26.621467948629075 ssim: 0.9015109955368615 lpips: 0.26542308926582336\n",
      "=====> scene: 8 mean psnr 28.331380911917446 ssim: 0.8758834032089559 lpips: 0.32112571597099304\n",
      "=====> scene: 21 mean psnr 23.238617049537122 ssim: 0.8736486067576469 lpips: 0.2456555962562561\n",
      "=====> scene: 103 mean psnr 30.40125554666663 ssim: 0.9442875531229784 lpips: 0.25619567558169365\n",
      "=====> scene: 114 mean psnr 26.46890004323232 ssim: 0.9134870027630088 lpips: 0.2253187969326973\n",
      "=====> all mean psnr 27.01232429999652 ssim: 0.9017635122778904 lpips: 0.2627437748014927\n"
     ]
    }
   ],
   "source": [
    "root = '/mnt/new_disk_2/anpei/code/nerf/logs/'\n",
    "root_gt = '/mnt/new_disk2/anpei/code/MVS-NeRF/runs_fine_tuning/'\n",
    "pairs = torch.load('/mnt/new_disk_2/anpei/code/MVS-NeRF/configs/pairs.th')\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['chair','drums','ficus','hotdog','lego','materials','mic','ship']):#\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    files = sorted(glob.glob(f'{root}/{scene}_test/testset_200000/*.png'))\n",
    "    for j, file in enumerate(files):\n",
    "\n",
    "        idx = pairs[f'{scene}_val'][j]\n",
    "        img = cv2.imread(file)[...,::-1]\n",
    "        gt = cv2.imread(f'{root_gt}/{scene}/{scene}/{scene}_00009999_{j:02d}.png')[...,::-1]\n",
    "        gt, img = gt[:,:800].astype('float')/255.0, img.astype('float')/255.0\n",
    "\n",
    "#         H_crop, W_crop = np.array(gt.shape[:2])//10\n",
    "#         img = img[H_crop:-H_crop,W_crop:-W_crop]\n",
    "#         gt = gt[H_crop:-H_crop,W_crop:-W_crop]\n",
    "\n",
    "        psnr.append( mse2psnr(np.mean((gt-img)**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') \n",
    "\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['fern','flower','fortress','horns','leaves', 'orchids', 'room',  'trex']):#,\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    files = sorted(glob.glob(f'{root}/{scene}_test/testset_200000/*.png'))\n",
    "    for j, file in enumerate(files):\n",
    "\n",
    "        idx = pairs[f'{scene}_val'][j]\n",
    "        img = cv2.resize(cv2.imread(file)[...,::-1],(960,640))\n",
    "        gt =  cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/{scene}_{idx:03d}.png')[...,::-1]\n",
    "        gt, img = gt[:,:960].astype('float')/255.0, img.astype('float')/255.0\n",
    "\n",
    "\n",
    "        psnr.append( mse2psnr(np.mean((gt-img)**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') \n",
    "\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate([1,8,21,103,114]):#,\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    files = sorted(glob.glob(f'{root}/scan{scene}_test/testset_200000/*.png'))\n",
    "    for j, file in enumerate(files):\n",
    "\n",
    "        idx = pairs[f'dtu_val'][j]\n",
    "        img = cv2.imread(file)[...,::-1]\n",
    "        gt = cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/scan{scene}_{idx:03d}.png')[...,::-1]\n",
    "        mask = cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/scan{scene}_{idx:03d}_mask.png')==255\n",
    "        gt, img = gt[:,:640].astype('float')/255.0, img.astype('float')/255.0\n",
    "\n",
    "        gt[mask],img[mask] = 0.0,0.0\n",
    "        psnr.append( mse2psnr(np.mean((gt[~mask]-img[~mask])**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> scene: chair mean psnr 26.746873810513947 ssim: 0.9321234340308168 lpips: 0.15475058555603027\n",
      "=====> scene: drums mean psnr 22.28406117543553 ssim: 0.8964614019632969 lpips: 0.21542910858988762\n",
      "=====> scene: ficus mean psnr 26.365789669973488 ssim: 0.9438413276712496 lpips: 0.15914445742964745\n",
      "=====> scene: hotdog mean psnr 32.489636248742805 ssim: 0.9699785599978382 lpips: 0.11295554973185062\n",
      "=====> scene: lego mean psnr 26.79832336361502 ssim: 0.9245065827858229 lpips: 0.18708691000938416\n",
      "=====> scene: materials mean psnr 24.957611270986945 ssim: 0.9249186604651752 lpips: 0.1740873008966446\n",
      "=====> scene: mic mean psnr 29.449610622444368 ssim: 0.9693072200690339 lpips: 0.092950988560915\n",
      "=====> scene: ship mean psnr 26.60832366062154 ssim: 0.8780999869891254 lpips: 0.28621142730116844\n",
      "=====> all mean psnr 26.962528727791707 ssim: 0.9299046467465449 lpips: 0.17282704100944102\n",
      "=====> scene: fern mean psnr 22.61357364768159 ssim: 0.77000724312094 lpips: 0.2827577739953995\n",
      "=====> scene: flower mean psnr 25.52052448547126 ssim: 0.8816617628340061 lpips: 0.19497620686888695\n",
      "=====> scene: fortress mean psnr 28.010539767191638 ssim: 0.8958878940094137 lpips: 0.19440287351608276\n",
      "=====> scene: horns mean psnr 24.99889079936664 ssim: 0.8806854873247154 lpips: 0.2329558990895748\n",
      "=====> scene: leaves mean psnr 21.228759476443667 ssim: 0.8063488185998933 lpips: 0.24219803884625435\n",
      "=====> scene: orchids mean psnr 19.460068266904813 ssim: 0.7087825176683923 lpips: 0.3091204948723316\n",
      "=====> scene: room mean psnr 29.150237317214977 ssim: 0.9570667630128395 lpips: 0.16276488453149796\n",
      "=====> scene: trex mean psnr 24.079128145103503 ssim: 0.8873396265555692 lpips: 0.2028873674571514\n",
      "=====> all mean psnr 24.38271523817226 ssim: 0.8484725141407212 lpips: 0.22775794239714742\n",
      "=====> scene: 1 mean psnr 28.046498782015014 ssim: 0.9341022735743112 lpips: 0.17119702696800232\n",
      "=====> scene: 8 mean psnr 28.875841987497292 ssim: 0.8999499891440654 lpips: 0.26092011481523514\n",
      "=====> scene: 21 mean psnr 24.870061827644722 ssim: 0.9215085855864712 lpips: 0.14212623238563538\n",
      "=====> scene: 103 mean psnr 32.226849694259215 ssim: 0.9638182025610383 lpips: 0.16953438520431519\n",
      "=====> scene: 114 mean psnr 28.46625266665073 ssim: 0.9451965364044997 lpips: 0.15322893112897873\n",
      "=====> all mean psnr 28.497100991613394 ssim: 0.9329151174540771 lpips: 0.17940133810043335\n"
     ]
    }
   ],
   "source": [
    "root = '/mnt/new_disk2/anpei/code/MVS-NeRF/runs_fine_tuning/'\n",
    "pairs = torch.load('/mnt/new_disk_2/anpei/code/MVS-NeRF/configs/pairs.th')\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['chair','drums','ficus','hotdog','lego','materials','mic','ship']):\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    files = sorted(glob.glob(f'{root}/{scene}/{scene}/{scene}_00009999_*'))\n",
    "    for j, file in enumerate(files):\n",
    "\n",
    "        idx = pairs[f'{scene}_val'][j]\n",
    "        img = cv2.imread(file).astype('float')[...,::-1]\n",
    "        gt, img = img[:,:800]/255.0, img[:,800:1600]/255.0\n",
    "\n",
    "#         H_crop, W_crop = np.array(gt.shape[:2])//10\n",
    "#         img = img[H_crop:-H_crop,W_crop:-W_crop]\n",
    "#         gt = gt[H_crop:-H_crop,W_crop:-W_crop]\n",
    "\n",
    "        psnr.append( mse2psnr(np.mean((gt-img)**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') \n",
    "\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['fern','flower','fortress','horns','leaves', 'orchids', 'room',  'trex']):#,\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    files = sorted(glob.glob(f'/mnt/new_disk2/anpei/code/MVS-NeRF/runs_fine_tuning/{scene}/{scene}/{scene}_00009999_*'))\n",
    "    for j, file in enumerate(files):\n",
    "\n",
    "        idx = pairs[f'{scene}_val'][j]\n",
    "        img = cv2.imread(file)[...,::-1]\n",
    "        gt, img = img[:,:960].astype('float')/255.0, img[:,960:960*2].astype('float')/255.0\n",
    "\n",
    "        H_crop, W_crop = np.array(gt.shape[:2])//10\n",
    "        img = img[H_crop:-H_crop,W_crop:-W_crop]\n",
    "        gt = gt[H_crop:-H_crop,W_crop:-W_crop]\n",
    "\n",
    "\n",
    "        psnr.append( mse2psnr(np.mean((gt-img)**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') \n",
    "\n",
    "root = '/mnt/new_disk2/anpei/code/MVS-NeRF/runs_fine_tuning/'\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate([1,8,21,103,114]):#,\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    files = sorted(glob.glob(f'{root}/dtu_scan{scene}_1h/dtu_scan{scene}_1h/00010239_*'))\n",
    "    for j, file in enumerate(files):\n",
    "\n",
    "        idx = pairs[f'dtu_val'][j]\n",
    "        img = cv2.imread(file)[...,::-1][:,640:1280]\n",
    "        gt = cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/scan{scene}_{idx:03d}.png')[...,::-1]\n",
    "        mask = cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/scan{scene}_{idx:03d}_mask.png')==255\n",
    "        gt, img = gt[:,:640].astype('float')/255.0, img.astype('float')/255.0\n",
    "\n",
    "        gt[mask],img[mask] = 0.0,0.0\n",
    "        psnr.append( mse2psnr(np.mean((gt[~mask]-img[~mask])**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ibrnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> scene: fern mean psnr 22.64474646040451 ssim: 0.7736232480476191 lpips: 0.26588304713368416\n",
      "=====> scene: flower mean psnr 26.553349019087786 ssim: 0.9092690161984827 lpips: 0.14575103670358658\n",
      "=====> scene: fortress mean psnr 30.338842953903075 ssim: 0.9368867837660259 lpips: 0.13289865292608738\n",
      "=====> scene: horns mean psnr 25.01290939681414 ssim: 0.9040335882553917 lpips: 0.1899307444691658\n",
      "=====> scene: leaves mean psnr 22.076508076698556 ssim: 0.8430354849586478 lpips: 0.17987846583127975\n",
      "=====> scene: orchids mean psnr 19.007830032899616 ssim: 0.7045611776629173 lpips: 0.2861044891178608\n",
      "=====> scene: room mean psnr 31.05473820815669 ssim: 0.9723299877991765 lpips: 0.08911459799855947\n",
      "=====> scene: trex mean psnr 22.339864946223464 ssim: 0.8421255627008343 lpips: 0.22207806631922722\n",
      "=====> all mean psnr 24.878598636773482 ssim: 0.8607331061736369 lpips: 0.1889548875624314\n",
      "=====> scene: 1 mean psnr 30.99564992655386 ssim: 0.9548394719193786 lpips: 0.1285402663052082\n",
      "=====> scene: 8 mean psnr 32.46173840309124 ssim: 0.9445782574823819 lpips: 0.16979694738984108\n",
      "=====> scene: 21 mean psnr 27.88178725277648 ssim: 0.9467770144187784 lpips: 0.1040295660495758\n",
      "=====> scene: 103 mean psnr 34.399916992709706 ssim: 0.9675776122666799 lpips: 0.15563546121120453\n",
      "=====> scene: 114 mean psnr 31.00119644953211 ssim: 0.9638488318305859 lpips: 0.09874588809907436\n",
      "=====> all mean psnr 31.348057804932676 ssim: 0.955524237583561 lpips: 0.1313496258109808\n"
     ]
    }
   ],
   "source": [
    "# root = '/mnt/new_disk2/anpei/code/IBRNet/logs'\n",
    "pairs = torch.load('/mnt/new_disk_2/anpei/code/MVS-NeRF/configs/pairs.th')\n",
    "# psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "# for i_scene, scene in enumerate(['chair','drums','ficus','hotdog','lego','materials','mic','ship']):#,\n",
    "#     psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "#     files = sorted(glob.glob(f'{root}/nerf-3view-finetuning-nearest-{scene}/010000_*'))\n",
    "#     for j, file in enumerate(files):\n",
    "\n",
    "#         idx = pairs[f'{scene}_val'][j]\n",
    "#         img = cv2.imread(file).astype('float')[...,::-1]\n",
    "#         gt, img = img[:,800:800*2]/255.0, img[:,800*3:800*4]/255.0\n",
    "\n",
    "# #         H_crop, W_crop = np.array(gt.shape[:2])//10\n",
    "# #         img = img[H_crop:-H_crop,W_crop:-W_crop]\n",
    "# #         gt = gt[H_crop:-H_crop,W_crop:-W_crop]\n",
    "\n",
    "#         psnr.append( mse2psnr(np.mean((gt-img)**2)))\n",
    "#         ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "#         img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "#         img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "#         LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "#     print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "#     psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "# print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') \n",
    "\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['fern','flower','fortress','horns','leaves', 'orchids', 'room',  'trex']):#\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    files = sorted(glob.glob(f'/mnt/new_disk2/anpei/code/IBRNet/logs/llff-3view-finetuning-nearest-{scene}/010000_*'))\n",
    "    for j, file in enumerate(files):\n",
    "\n",
    "        idx = pairs[f'{scene}_val'][j]\n",
    "        img = cv2.imread(file)[...,::-1]\n",
    "        gt, img = img[:,1008:1008*2].astype('float')/255.0, img[:,1008*3:1008*4].astype('float')/255.0\n",
    "        img, gt = cv2.resize(img,(960,640)), cv2.resize(gt,(960,640))\n",
    "\n",
    "        H_crop, W_crop = np.array(gt.shape[:2])//10\n",
    "        img = img[H_crop:-H_crop,W_crop:-W_crop]\n",
    "        gt = gt[H_crop:-H_crop,W_crop:-W_crop]\n",
    "\n",
    "        \n",
    "        psnr.append( mse2psnr(np.mean((gt-img)**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') \n",
    "\n",
    "root = '/mnt/new_disk2/anpei/code/IBRNet/logs'\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate([1,8,21,103,114]):#\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    files = sorted(glob.glob(f'{root}/dtu-3view-finetuning-nearest-scan{scene}/010000_*'))\n",
    "    for j, file in enumerate(files):\n",
    "\n",
    "        idx = pairs[f'dtu_val'][j]\n",
    "        img = cv2.imread(file)[...,::-1][:,3*640:4*640]\n",
    "        gt = cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/scan{scene}_{idx:03d}.png')[...,::-1]\n",
    "        mask = cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/scan{scene}_{idx:03d}_mask.png')==255\n",
    "        gt, img = gt[:,:640].astype('float')/255.0, img.astype('float')/255.0\n",
    "\n",
    "        gt[mask],img[mask] = 0.0,0.0\n",
    "        psnr.append( mse2psnr(np.mean((gt[~mask]-img[~mask])**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pixel nerf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> scene: chair mean psnr 7.175962813343725 ssim: 0.6243642351905847 lpips: 0.38591109961271286\n",
      "=====> scene: drums mean psnr 8.148548711878252 ssim: 0.6701584468514097 lpips: 0.42121122032403946\n",
      "=====> scene: ficus mean psnr 6.608732738834844 ssim: 0.668716265099144 lpips: 0.3350602239370346\n",
      "=====> scene: hotdog mean psnr 6.799387670799135 ssim: 0.6689815218041557 lpips: 0.43327029794454575\n",
      "=====> scene: lego mean psnr 7.740217521658803 ssim: 0.6710903029993184 lpips: 0.42670799791812897\n",
      "=====> scene: materials mean psnr 7.609290420358684 ssim: 0.6441046576733512 lpips: 0.43245941400527954\n",
      "=====> scene: mic mean psnr 7.707203698223274 ssim: 0.7294597852809476 lpips: 0.32929887622594833\n",
      "=====> scene: ship mean psnr 7.295484760785579 ssim: 0.5836685948507447 lpips: 0.5257005095481873\n",
      "=====> all mean psnr 7.385603541985287 ssim: 0.657567976218707 lpips: 0.4112024549394846\n",
      "=====> scene: fern mean psnr 12.397648684821284 ssim: 0.5312397318110376 lpips: 0.6500117480754852\n",
      "=====> scene: flower mean psnr 9.99675489427281 ssim: 0.43323656344453193 lpips: 0.7075561136007309\n",
      "=====> scene: fortress mean psnr 14.073488262986546 ssim: 0.6736649368929569 lpips: 0.6075489073991776\n",
      "=====> scene: horns mean psnr 11.71002161685521 ssim: 0.5157559834106309 lpips: 0.705190509557724\n",
      "=====> scene: leaves mean psnr 9.847068575637605 ssim: 0.2681360856716045 lpips: 0.6947661340236664\n",
      "=====> scene: orchids mean psnr 9.624184850140201 ssim: 0.3168018322171233 lpips: 0.7207814902067184\n",
      "=====> scene: room mean psnr 11.750716240417157 ssim: 0.6906632306577324 lpips: 0.6112178564071655\n",
      "=====> scene: trex mean psnr 10.55211637013632 ssim: 0.45770187915676297 lpips: 0.6672322899103165\n",
      "=====> all mean psnr 11.243999936908391 ssim: 0.48590003040779756 lpips: 0.6705381311476231\n",
      "=====> scene: 1 mean psnr 21.64330896303546 ssim: 0.8268906240371169 lpips: 0.3728666678071022\n",
      "=====> scene: 8 mean psnr 23.69860813191727 ssim: 0.8291247579664462 lpips: 0.38369619846343994\n",
      "=====> scene: 21 mean psnr 16.03916045790063 ssim: 0.6905614284959526 lpips: 0.4074021577835083\n",
      "=====> scene: 103 mean psnr 16.75554527345504 ssim: 0.8356182752116726 lpips: 0.3762983977794647\n",
      "=====> scene: 114 mean psnr 18.403335481905653 ssim: 0.7632868039029544 lpips: 0.371926873922348\n",
      "=====> all mean psnr 19.30799166164281 ssim: 0.7890963779228286 lpips: 0.3824380591511726\n"
     ]
    }
   ],
   "source": [
    "root = '/mnt/new_disk2/anpei/code/pixel-nerf/visuals/dtu'\n",
    "root_gt = '/mnt/new_disk2/anpei/code/MVS-NeRF/runs_fine_tuning/'\n",
    "pairs = torch.load('/mnt/new_disk_2/anpei/code/MVS-NeRF/configs/pairs.th')\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['chair','drums','ficus','hotdog','lego','materials','mic','ship']):#,\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    pairs_idx = pairs[f'{scene}_val']\n",
    "    for j, file in enumerate(pairs_idx):\n",
    "\n",
    "        idx = pairs_idx[j]\n",
    "        img = cv2.imread(f'{root}/{scene}_{idx:03d}.png')[...,::-1]\n",
    "        gt = cv2.imread(f'{root_gt}/{scene}/{scene}/{scene}_00009999_{j:02d}.png')[...,::-1]\n",
    "        gt, img = gt[:,:800].astype('float')/255.0, img.astype('float')/255.0\n",
    "\n",
    "#         H_crop, W_crop = np.array(gt.shape[:2])//10\n",
    "#         img = img[H_crop:-H_crop,W_crop:-W_crop]\n",
    "#         gt = gt[H_crop:-H_crop,W_crop:-W_crop]\n",
    "\n",
    "        psnr.append( mse2psnr(np.mean((gt-img)**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') \n",
    "\n",
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['fern','flower','fortress','horns','leaves', 'orchids', 'room',  'trex']):#,\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    pairs_idx = pairs[f'{scene}_val']\n",
    "    for j, file in enumerate(pairs_idx):\n",
    "\n",
    "        idx = pairs_idx[j]\n",
    "        img = cv2.resize(cv2.imread(f'{root}/{scene}_{idx:03d}.png')[...,::-1],(960,640))\n",
    "        gt =  cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/{scene}_{idx:03d}.png')[...,::-1]\n",
    "        gt, img = gt[:,:960].astype('float')/255.0, img.astype('float')/255.0\n",
    "\n",
    "\n",
    "        psnr.append( mse2psnr(np.mean((gt-img)**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') \n",
    "\n",
    "psnr_all,ssim_all,LPIPS_vgg_all,depth_acc = [],[],[],[]\n",
    "for i_scene, scene in enumerate([1,8,21,103,114]):#,\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    \n",
    "    pairs_idx = pairs[f'dtu_val']\n",
    "    for j, file in enumerate(pairs_idx):\n",
    "\n",
    "        idx = pairs_idx[j]\n",
    "        img = cv2.imread(f'{root}/scan{scene}_{idx:03d}.png')[...,::-1]\n",
    "        \n",
    "        gt = cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/scan{scene}_{idx:03d}.png')[...,::-1]\n",
    "        mask = cv2.imread(f'/mnt/new_disk2/anpei/code/MVS-NeRF/results/test3/scan{scene}_{idx:03d}_mask.png')==255\n",
    "        gt, img = gt[:,:640].astype('float')/255.0, img.astype('float')/255.0\n",
    "\n",
    "        gt[mask],img[mask] = 0.0,0.0\n",
    "        psnr.append( mse2psnr(np.mean((gt[~mask]-img[~mask])**2)))\n",
    "        ssim.append( structural_similarity(gt, img, multichannel=True))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "        img_gt_tensor = torch.from_numpy(gt)[None].permute(0,3,1,2).float()*2-1.0\n",
    "        LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "        \n",
    "        # depth\n",
    "#         depth_pred = torch.load(f'{root}/scan{scene}_{idx:03d}_depth.th')\n",
    "#         depth_gt,_ =  read_depth(f'/mnt/data/new_disk/sungx/data/mvs_dataset/DTU/mvs_training/dtu/Depths/scan{scene}/depth_map_{idx:04d}.pfm')\n",
    "        \n",
    "#         mask_gt = depth_gt>0\n",
    "#         abs_err = abs_error(depth_pred*1.5, depth_gt/200, mask_gt).numpy()\n",
    "\n",
    "#         eval_metric = [0.01,0.05, 0.1]\n",
    "#         depth_acc[f'abs_err'][f'{scene}'] = np.mean(abs_err)\n",
    "#         depth_acc[f'acc_l_{eval_metric[0]}'][f'{scene}'] = acc_threshold(abs_err,eval_metric[0]).mean()\n",
    "#         depth_acc[f'acc_l_{eval_metric[1]}'][f'{scene}'] = acc_threshold(abs_err,eval_metric[1]).mean()\n",
    "#         depth_acc[f'acc_l_{eval_metric[2]}'][f'{scene}'] = acc_threshold(abs_err,eval_metric[2]).mean()\n",
    "\n",
    "    print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "    psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
