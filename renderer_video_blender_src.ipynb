{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys,os,imageio\n",
    "root = '/home/youngsun/documents/mvs/mvsnerf_timing'\n",
    "os.chdir(root)\n",
    "sys.path.append(root)\n",
    "\n",
    "from opt_src import config_parser\n",
    "from data import dataset_dict\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# models\n",
    "from models_src import *\n",
    "from renderer_src import *\n",
    "from data.ray_utils import get_rays\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from skimage.metrics import structural_similarity\n",
    "\n",
    "# pytorch-lightning\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning import LightningModule, Trainer, loggers\n",
    "\n",
    "\n",
    "from data.ray_utils import ray_marcher\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rendering video from finetuned ckpts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_batch(batch):\n",
    "    rays = batch['rays']  # (B, 8)\n",
    "    rgbs = batch['rgbs']  # (B, 3)\n",
    "    return rays, rgbs\n",
    "\n",
    "def unpreprocess(data, shape=(1,1,3,1,1)):\n",
    "    # to unnormalize image for visualization\n",
    "    # data N V C H W\n",
    "    device = data.device\n",
    "    mean = torch.tensor([-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225]).view(*shape).to(device)\n",
    "    std = torch.tensor([1 / 0.229, 1 / 0.224, 1 / 0.225]).view(*shape).to(device)\n",
    "\n",
    "    return (data - mean) / std\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    return x / np.linalg.norm(x, axis=-1, keepdims=True)\n",
    "\n",
    "def viewmatrix(z, up, pos):\n",
    "    vec2 = normalize(z)\n",
    "    vec1_avg = up\n",
    "    vec0 = normalize(np.cross(vec1_avg, vec2))\n",
    "    vec1 = normalize(np.cross(vec2, vec0))\n",
    "    m = np.eye(4)\n",
    "    m[:3] = np.stack([vec0, vec1, vec2, pos], 1)\n",
    "    return m\n",
    "\n",
    "def ptstocam(pts, c2w):\n",
    "    tt = np.matmul(c2w[:3,:3].T, (pts-c2w[:3,3])[...,np.newaxis])[...,0]\n",
    "    return tt\n",
    "\n",
    "def poses_avg(poses):\n",
    "\n",
    "    center = poses[:, :3, 3].mean(0)\n",
    "    print('center in poses_avg', center)\n",
    "    vec2 = normalize(poses[:, :3, 2].sum(0))\n",
    "    up = poses[:, :3, 1].sum(0)\n",
    "    c2w = viewmatrix(vec2, up, center)\n",
    "    \n",
    "    return c2w\n",
    "\n",
    "def render_path_imgs(c2ws_all, focal):\n",
    "    T = c2ws_all[...,3]\n",
    "\n",
    "    return render_poses\n",
    "\n",
    "def render_path_spiral(c2w, up, rads, focal, zdelta, zrate, N_rots=2, N=120):\n",
    "    render_poses = []\n",
    "    rads = np.array(list(rads) + [1.])\n",
    "    \n",
    "    for theta in np.linspace(0., 2. * np.pi * N_rots, N+1)[:-1]:\n",
    "    # for theta in np.linspace(0., 5, N+1)[:-1]:\n",
    "        # spiral\n",
    "        # c = np.dot(c2w[:3,:4], np.array([np.cos(theta), -np.sin(theta), -np.sin(theta*zrate), 1.]) * rads)\n",
    "\n",
    "        # 关于使用平均姿态的相关问题\n",
    "        # 从训练集推算出来的平均姿态方向基本平行于z轴，因为训练集中大多数图片是正面的，\n",
    "        # 但是存在一个问题，将z和平均姿态相乘后得到的方向也基本上和z平行，所以无论怎么调整看起来都是平行的，\n",
    "        # 别用平均姿态看其他位置的照片，直接用世界坐标系即可！！！！\n",
    "        # 但是需要用别的姿态大致估计一下位置参数\n",
    "        c = np.array([(np.cos(theta)*theta)/10, (-np.sin(theta)*theta)/10, -0.1]) \n",
    "\n",
    "        # 这个是因为作者在读取并规范化相机姿态的时候作了poses*blender2opencv，转换了坐标系，\n",
    "        # 我用的数据无需转换，但是这里加个负号就解决了，目前不影响什么，记住就行\n",
    "        z = -(normalize(c - np.array([0,0,-focal])))\n",
    "        print(\"c\", c)\n",
    "        print(\"z\", z)\n",
    "        render_poses.append(viewmatrix(z, up, c))\n",
    "    return render_poses\n",
    "\n",
    "def get_spiral(c2ws_all, near_far, rads_scale=0.5, N_views=120):\n",
    "\n",
    "    # center pose\n",
    "    c2w = poses_avg(c2ws_all)\n",
    "    print('poses_avg', c2w)\n",
    "    \n",
    "    # Get average pose\n",
    "    up = normalize(c2ws_all[:, :3, 1].sum(0))\n",
    "\n",
    "    # Find a reasonable \"focus depth\" for this dataset\n",
    "    close_depth, inf_depth = near_far\n",
    "    print('near and far bounds', close_depth, inf_depth)\n",
    "    dt = .75\n",
    "    mean_dz = 1./(((1.-dt)/close_depth + dt/inf_depth))\n",
    "    focal = mean_dz\n",
    "    print(focal)\n",
    "\n",
    "    # Get radii for spiral path\n",
    "    shrink_factor = .8\n",
    "    zdelta = close_depth * .2\n",
    "    tt = c2ws_all[:,:3,3] - c2w[:3,3][None]\n",
    "    rads = np.percentile(np.abs(tt), 70, 0)*rads_scale\n",
    "    print(\"rads\",rads)\n",
    "    render_poses = render_path_spiral(c2w, up, rads, focal, zdelta, zrate=.5, N=N_views)\n",
    "    return np.stack(render_poses)\n",
    "\n",
    "def position2angle(position, N_views=16, N_rots = 2):\n",
    "    ''' nx3 '''\n",
    "    position = normalize(position)\n",
    "    theta = np.arccos(position[:,2])/np.pi*180\n",
    "    phi = np.arctan2(position[:,1],position[:,0])/np.pi*180\n",
    "    return [theta,phi]\n",
    "\n",
    "def pose_spherical_nerf(euler, radius=0.01):\n",
    "    c2ws_render = np.eye(4)\n",
    "    c2ws_render[:3,:3] =  R.from_euler('xyz', euler, degrees=True).as_matrix()\n",
    "    # 保留旋转矩阵的最后一列再乘个系数就能当作位置？\n",
    "    c2ws_render[:3,3]  = c2ws_render[:3,:3] @ np.array([0.0,0.0,-radius])\n",
    "    return c2ws_render\n",
    "\n",
    "def create_spheric_poses(radius, n_poses=120):\n",
    "    \"\"\"\n",
    "    Create circular poses around z axis.\n",
    "    Inputs:\n",
    "        radius: the (negative) height and the radius of the circle.\n",
    "    Outputs:\n",
    "        spheric_poses: (n_poses, 3, 4) the poses in the circular path\n",
    "    \"\"\"\n",
    "\n",
    "    def spheric_pose(theta, phi, radius):\n",
    "        trans_t = lambda t: np.array([\n",
    "            [1, 0, 0, 0],\n",
    "            [0, 1, 0, -0.9 * t],\n",
    "            [0, 0, 1, t],\n",
    "            [0, 0, 0, 1],\n",
    "        ])\n",
    "\n",
    "        rot_phi = lambda phi: np.array([\n",
    "            [1, 0, 0, 0],\n",
    "            [0, np.cos(phi), -np.sin(phi), 0],\n",
    "            [0, np.sin(phi), np.cos(phi), 0],\n",
    "            [0, 0, 0, 1],\n",
    "        ])\n",
    "\n",
    "        rot_theta = lambda th: np.array([\n",
    "            [np.cos(th), 0, -np.sin(th), 0],\n",
    "            [0, 1, 0, 0],\n",
    "            [np.sin(th), 0, np.cos(th), 0],\n",
    "            [0, 0, 0, 1],\n",
    "        ])\n",
    "\n",
    "        c2w = rot_theta(theta) @ rot_phi(phi) @ trans_t(radius)\n",
    "        c2w = np.array([[-1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1]]) @ c2w\n",
    "        return c2w[:3]\n",
    "\n",
    "    spheric_poses = []\n",
    "    for th in np.linspace(0, 2 * np.pi, n_poses + 1)[:-1]:\n",
    "        spheric_poses += [spheric_pose(th, -np.pi / 5, radius)]  # 36 degree view downwards\n",
    "    return np.stack(spheric_poses, 0)\n",
    "\n",
    "def nerf_video_path(c2ws, theta_range=10,phi_range=20,N_views=120):\n",
    "    c2ws = torch.tensor(c2ws)\n",
    "    mean_position = torch.mean(c2ws[:,:3, 3],dim=0).reshape(1,3).cpu().numpy()\n",
    "    rotvec = []\n",
    "    for i in range(c2ws.shape[0]):\n",
    "        r = R.from_matrix(c2ws[i, :3, :3])\n",
    "        euler_ange = r.as_euler('xyz', degrees=True).reshape(1, 3)\n",
    "        if i:\n",
    "            mask = np.abs(euler_ange - rotvec[0])>180\n",
    "            euler_ange[mask] += 360.0\n",
    "        rotvec.append(euler_ange)\n",
    "    # 采用欧拉角做平均的方法求旋转矩阵的平均\n",
    "    rotvec = np.mean(np.stack(rotvec), axis=0)\n",
    "#     render_poses = [pose_spherical_nerf(rotvec)]\n",
    "    render_poses = [pose_spherical_nerf(rotvec+np.array([angle,0.0,-phi_range])) for angle in np.linspace(-theta_range,theta_range,N_views//4, endpoint=False)]\n",
    "    render_poses += [pose_spherical_nerf(rotvec+np.array([theta_range,0.0,angle])) for angle in np.linspace(-phi_range,phi_range,N_views//4, endpoint=False)]\n",
    "    render_poses += [pose_spherical_nerf(rotvec+np.array([angle,0.0,phi_range])) for angle in np.linspace(theta_range,-theta_range,N_views//4, endpoint=False)]\n",
    "    render_poses += [pose_spherical_nerf(rotvec+np.array([-theta_range,0.0,angle])) for angle in np.linspace(phi_range,-phi_range,N_views//4, endpoint=False)]\n",
    "    # render_poses = torch.from_numpy(np.stack(render_poses)).float().to(device)\n",
    "    return render_poses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NeRF video rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> valing index: [26 60 13 47]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5400/210140234.py:44: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  dis = np.sum(c2ws_all[:,:3,2] * c2ws_all[[random_selete],:3,2], axis=-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> ref idx: [48 61  0]\n",
      "4\n",
      "> \u001b[0;32m/tmp/ipykernel_5400/210140234.py\u001b[0m(57)\u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     56 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 57 \u001b[0;31m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stop here\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     58 \u001b[0;31m        \u001b[0mhello\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmello\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> dis\n",
      "array([ 0.91647896,  0.34328754,  0.47921478,  0.90541335,  0.18376352,\n",
      "        0.5997059 , -0.26847225,  0.64373387,  0.52590789,  0.69414041,\n",
      "        0.87789948,  0.73418933,  0.13424899,  0.82495021,  0.56329689,\n",
      "        0.68468867, -0.1443277 ,  0.45945567,  0.03074702,  0.57506083,\n",
      "        0.30065404,  0.2356221 ,  0.93591053,  0.5557658 ,  0.07247928,\n",
      "        0.74147542,  0.95912177, -0.02707397,  0.86694834,  0.3223359 ,\n",
      "        0.72212367,  0.64516261,  0.00193229,  0.88830373,  0.69280728,\n",
      "       -0.19492757,  0.5979799 ,  0.51677558,  0.94646367,  0.14871072,\n",
      "        0.75861484,  0.63188129,  0.73225707,  0.07465486, -0.31498767,\n",
      "        0.45845861,  0.25452835,  0.65427501,  0.93225069,  0.92983186,\n",
      "        0.1618977 ,  0.22730706,  0.58434367, -0.01863599,  0.69639438,\n",
      "        0.51696815,  0.18051033,  0.354162  ,  0.87665983,  0.17810984,\n",
      "        0.99999999,  0.98996718,  0.28959983,  0.78627181,  0.5214904 ,\n",
      "        0.7007974 ,  0.30401715,  0.55301456, -0.3115227 ,  0.65428065,\n",
      "        0.77767987,  0.95662701, -0.07470262,  0.99927076,  0.50017379,\n",
      "        0.79464033,  0.6120851 ,  0.76085285,  0.83775901,  0.60973144,\n",
      "        0.23298071,  0.043701  , -0.1631557 ,  0.66725066,  0.58442247,\n",
      "        0.61370725,  0.45264528,  0.76083125,  0.64658154,  0.45924678,\n",
      "        0.40075511,  0.60346012,  0.29547276,  0.87770392,  0.81528987,\n",
      "        0.44332053,  0.51676771, -0.0716902 ,  0.35208192,  0.16239269])\n",
      "ipdb> np.argsort(dis)[::-1]\n",
      "array([60, 73, 61, 26, 71, 38, 22, 48, 49,  0,  3, 33, 10, 93, 58, 28, 78,\n",
      "       13, 94, 75, 63, 70, 77, 87, 40, 25, 11, 42, 30, 65, 54,  9, 34, 15,\n",
      "       83, 69, 47, 88, 31,  7, 41, 85, 76, 79, 91,  5, 36, 84, 52, 19, 14,\n",
      "       23, 67,  8, 64, 55, 37, 96, 74,  2, 17, 89, 45, 86, 95, 90, 57, 98,\n",
      "        1, 29, 66, 20, 92, 62, 46, 21, 80, 51,  4, 56, 59, 99, 50, 39, 12,\n",
      "       43, 24, 81, 18, 32, 53, 27, 97, 72, 16, 82, 35,  6, 68, 44])\n",
      "ipdb> dis[np.argsort(dis)[::-1]]\n",
      "array([ 0.99999999,  0.99927076,  0.98996718,  0.95912177,  0.95662701,\n",
      "        0.94646367,  0.93591053,  0.93225069,  0.92983186,  0.91647896,\n",
      "        0.90541335,  0.88830373,  0.87789948,  0.87770392,  0.87665983,\n",
      "        0.86694834,  0.83775901,  0.82495021,  0.81528987,  0.79464033,\n",
      "        0.78627181,  0.77767987,  0.76085285,  0.76083125,  0.75861484,\n",
      "        0.74147542,  0.73418933,  0.73225707,  0.72212367,  0.7007974 ,\n",
      "        0.69639438,  0.69414041,  0.69280728,  0.68468867,  0.66725066,\n",
      "        0.65428065,  0.65427501,  0.64658154,  0.64516261,  0.64373387,\n",
      "        0.63188129,  0.61370725,  0.6120851 ,  0.60973144,  0.60346012,\n",
      "        0.5997059 ,  0.5979799 ,  0.58442247,  0.58434367,  0.57506083,\n",
      "        0.56329689,  0.5557658 ,  0.55301456,  0.52590789,  0.5214904 ,\n",
      "        0.51696815,  0.51677558,  0.51676771,  0.50017379,  0.47921478,\n",
      "        0.45945567,  0.45924678,  0.45845861,  0.45264528,  0.44332053,\n",
      "        0.40075511,  0.354162  ,  0.35208192,  0.34328754,  0.3223359 ,\n",
      "        0.30401715,  0.30065404,  0.29547276,  0.28959983,  0.25452835,\n",
      "        0.2356221 ,  0.23298071,  0.22730706,  0.18376352,  0.18051033,\n",
      "        0.17810984,  0.16239269,  0.1618977 ,  0.14871072,  0.13424899,\n",
      "        0.07465486,  0.07247928,  0.043701  ,  0.03074702,  0.00193229,\n",
      "       -0.01863599, -0.02707397, -0.0716902 , -0.07470262, -0.1443277 ,\n",
      "       -0.1631557 , -0.19492757, -0.26847225, -0.3115227 , -0.31498767])\n",
      "ipdb> c2ws_all[pair_idx]\n",
      "array([[[-9.47230041e-01, -2.86601007e-01,  1.43579572e-01,\n",
      "         -5.78787744e-01],\n",
      "        [-3.20554614e-01,  8.46898139e-01, -4.24273670e-01,\n",
      "          1.71030188e+00],\n",
      "        [-7.45058149e-09, -4.47909713e-01, -8.94078672e-01,\n",
      "          3.60414648e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-8.67967725e-01, -4.17226225e-01,  2.69359231e-01,\n",
      "         -1.08582175e+00],\n",
      "        [-4.96620715e-01,  7.29206145e-01, -4.70771968e-01,\n",
      "          1.89774251e+00],\n",
      "        [ 0.00000000e+00, -5.42384148e-01, -8.40130508e-01,\n",
      "          3.38667440e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-4.03749019e-01, -8.36814165e-01,  3.69768649e-01,\n",
      "         -1.49058497e+00],\n",
      "        [-9.14869845e-01,  3.69301587e-01, -1.63185745e-01,\n",
      "          6.57822728e-01],\n",
      "        [ 0.00000000e+00, -4.04176146e-01, -9.14681196e-01,\n",
      "          3.68719745e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]]])\n",
      "ipdb> frames\n",
      "60\n",
      "ipdb> gen_render_path(c2ws_all[pair_idx], N_views=frames)\n",
      "array([[[-9.47230039e-01, -2.86601049e-01,  1.43579567e-01,\n",
      "         -5.78787744e-01],\n",
      "        [-3.20554602e-01,  8.46898249e-01, -4.24273618e-01,\n",
      "          1.71030188e+00],\n",
      "        [-1.99287684e-08, -4.47909807e-01, -8.94078746e-01,\n",
      "          3.60414648e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-9.44086266e-01, -2.93968387e-01,  1.49277293e-01,\n",
      "         -6.04139444e-01],\n",
      "        [-3.29698533e-01,  8.41773612e-01, -4.27452994e-01,\n",
      "          1.71967391e+00],\n",
      "        [-1.92791014e-08, -4.52769006e-01, -8.91627852e-01,\n",
      "          3.59327288e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-9.40854225e-01, -3.01254547e-01,  1.55045234e-01,\n",
      "         -6.29491144e-01],\n",
      "        [-3.38811639e-01,  8.36561054e-01, -4.30548807e-01,\n",
      "          1.72904594e+00],\n",
      "        [-1.86294343e-08, -4.57614794e-01, -8.89150550e-01,\n",
      "          3.58239927e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-9.37534220e-01, -3.08458307e-01,  1.60882131e-01,\n",
      "         -6.54842845e-01],\n",
      "        [-3.47893068e-01,  8.31261823e-01, -4.33559909e-01,\n",
      "          1.73841797e+00],\n",
      "        [-1.79797672e-08, -4.62447029e-01, -8.86646912e-01,\n",
      "          3.57152567e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-9.34126560e-01, -3.15578461e-01,  1.66786706e-01,\n",
      "         -6.80194545e-01],\n",
      "        [-3.56941971e-01,  8.25877186e-01, -4.36485170e-01,\n",
      "          1.74779000e+00],\n",
      "        [-1.73301001e-08, -4.67265566e-01, -8.84117012e-01,\n",
      "          3.56065207e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-9.30631564e-01, -3.22613826e-01,  1.72757668e-01,\n",
      "         -7.05546245e-01],\n",
      "        [-3.65957501e-01,  8.20408426e-01, -4.39323482e-01,\n",
      "          1.75716203e+00],\n",
      "        [-1.66804331e-08, -4.72070263e-01, -8.81560926e-01,\n",
      "          3.54977846e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-9.27049559e-01, -3.29563237e-01,  1.78793704e-01,\n",
      "         -7.30897945e-01],\n",
      "        [-3.74938816e-01,  8.14856846e-01, -4.42073755e-01,\n",
      "          1.76653407e+00],\n",
      "        [-1.60307660e-08, -4.76860979e-01, -8.78978730e-01,\n",
      "          3.53890486e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-9.23380879e-01, -3.36425549e-01,  1.84893488e-01,\n",
      "         -7.56249645e-01],\n",
      "        [-3.83885077e-01,  8.09223764e-01, -4.44734918e-01,\n",
      "          1.77590610e+00],\n",
      "        [-1.53810990e-08, -4.81637570e-01, -8.76370499e-01,\n",
      "          3.52803125e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-9.19625868e-01, -3.43199637e-01,  1.91055676e-01,\n",
      "         -7.81601346e-01],\n",
      "        [-3.92795446e-01,  8.03510517e-01, -4.47305922e-01,\n",
      "          1.78527813e+00],\n",
      "        [-1.47314319e-08, -4.86399896e-01, -8.73736311e-01,\n",
      "          3.51715765e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-9.15784877e-01, -3.49884397e-01,  1.97278908e-01,\n",
      "         -8.06953046e-01],\n",
      "        [-4.01669091e-01,  7.97718454e-01, -4.49785737e-01,\n",
      "          1.79465016e+00],\n",
      "        [-1.40817648e-08, -4.91147815e-01, -8.71076244e-01,\n",
      "          3.50628405e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-9.11858265e-01, -3.56478743e-01,  2.03561808e-01,\n",
      "         -8.32304746e-01],\n",
      "        [-4.10505181e-01,  7.91848946e-01, -4.52173355e-01,\n",
      "          1.80402219e+00],\n",
      "        [-1.34320978e-08, -4.95881188e-01, -8.68390378e-01,\n",
      "          3.49541044e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-9.07846399e-01, -3.62981615e-01,  2.09902984e-01,\n",
      "         -8.57656446e-01],\n",
      "        [-4.19302892e-01,  7.85903375e-01, -4.54467787e-01,\n",
      "          1.81339422e+00],\n",
      "        [-1.27824307e-08, -5.00599872e-01, -8.65678790e-01,\n",
      "          3.48453684e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-9.03749654e-01, -3.69391968e-01,  2.16301032e-01,\n",
      "         -8.83008146e-01],\n",
      "        [-4.28061400e-01,  7.79883141e-01, -4.56668067e-01,\n",
      "          1.82276626e+00],\n",
      "        [-1.21327636e-08, -5.05303730e-01, -8.62941562e-01,\n",
      "          3.47366323e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-8.99568413e-01, -3.75708783e-01,  2.22754528e-01,\n",
      "         -9.08359846e-01],\n",
      "        [-4.36779887e-01,  7.73789659e-01, -4.58773249e-01,\n",
      "          1.83213829e+00],\n",
      "        [-1.14830966e-08, -5.09992622e-01, -8.60178776e-01,\n",
      "          3.46278963e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-8.95303068e-01, -3.81931060e-01,  2.29262039e-01,\n",
      "         -9.33711547e-01],\n",
      "        [-4.45457537e-01,  7.67624357e-01, -4.60782409e-01,\n",
      "          1.84151032e+00],\n",
      "        [-1.08334295e-08, -5.14666408e-01, -8.57390511e-01,\n",
      "          3.45191603e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-8.90954016e-01, -3.88057823e-01,  2.35822112e-01,\n",
      "         -9.59063247e-01],\n",
      "        [-4.54093539e-01,  7.61388681e-01, -4.62694645e-01,\n",
      "          1.85088235e+00],\n",
      "        [-1.01837624e-08, -5.19324950e-01, -8.54576852e-01,\n",
      "          3.44104242e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-8.86521664e-01, -3.94088114e-01,  2.42433285e-01,\n",
      "         -9.84414947e-01],\n",
      "        [-4.62687086e-01,  7.55084087e-01, -4.64509077e-01,\n",
      "          1.86025438e+00],\n",
      "        [-9.53409540e-09, -5.23968110e-01, -8.51737882e-01,\n",
      "          3.43016882e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-8.82006427e-01, -4.00021001e-01,  2.49094080e-01,\n",
      "         -1.00976665e+00],\n",
      "        [-4.71237374e-01,  7.48712047e-01, -4.66224847e-01,\n",
      "          1.86962641e+00],\n",
      "        [-8.88442832e-09, -5.28595752e-01, -8.48873684e-01,\n",
      "          3.41929522e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-8.77408728e-01, -4.05855573e-01,  2.55803007e-01,\n",
      "         -1.03511835e+00],\n",
      "        [-4.79743603e-01,  7.42274049e-01, -4.67841118e-01,\n",
      "          1.87899845e+00],\n",
      "        [-8.23476123e-09, -5.33207737e-01, -8.45984344e-01,\n",
      "          3.40842161e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-8.72728995e-01, -4.11590942e-01,  2.62558561e-01,\n",
      "         -1.06047005e+00],\n",
      "        [-4.88204979e-01,  7.35771589e-01, -4.69357079e-01,\n",
      "          1.88837048e+00],\n",
      "        [-7.58509419e-09, -5.37803929e-01, -8.43069946e-01,\n",
      "          3.39754801e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-8.67967666e-01, -4.17226242e-01,  2.69359228e-01,\n",
      "         -1.08582175e+00],\n",
      "        [-4.96620711e-01,  7.29206179e-01, -4.70771938e-01,\n",
      "          1.89774251e+00],\n",
      "        [-6.93542715e-09, -5.42384192e-01, -8.40130578e-01,\n",
      "          3.38667440e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-8.51752476e-01, -4.42401646e-01,  2.80710711e-01,\n",
      "         -1.10605991e+00],\n",
      "        [-5.23944386e-01,  7.19192173e-01, -4.56338578e-01,\n",
      "          1.83574652e+00],\n",
      "        [-8.50055970e-09, -5.35764315e-01, -8.44367573e-01,\n",
      "          3.40170056e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-8.34677428e-01, -4.67331011e-01,  2.91402328e-01,\n",
      "         -1.12629807e+00],\n",
      "        [-5.50739132e-01,  7.08267544e-01, -4.41637290e-01,\n",
      "          1.77375053e+00],\n",
      "        [-1.00656923e-08, -5.29111342e-01, -8.48552407e-01,\n",
      "          3.41672671e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-8.16759760e-01, -4.91980291e-01,  3.01428080e-01,\n",
      "         -1.14653623e+00],\n",
      "        [-5.76977897e-01,  6.96438654e-01, -4.26696270e-01,\n",
      "          1.71175454e+00],\n",
      "        [-1.16308249e-08, -5.22425683e-01, -8.52684822e-01,\n",
      "          3.43175286e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-7.98017561e-01, -5.16315617e-01,  3.10783135e-01,\n",
      "         -1.16677439e+00],\n",
      "        [-6.02634195e-01,  6.83713170e-01, -4.11543835e-01,\n",
      "          1.64975855e+00],\n",
      "        [-1.31959575e-08, -5.15707752e-01, -8.56764562e-01,\n",
      "          3.44677901e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-7.78469750e-01, -5.40303353e-01,  3.19463825e-01,\n",
      "         -1.18701255e+00],\n",
      "        [-6.27682124e-01,  6.70100052e-01, -3.96208370e-01,\n",
      "          1.58776256e+00],\n",
      "        [-1.47610901e-08, -5.08957962e-01, -8.60791376e-01,\n",
      "          3.46180516e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-7.58136062e-01, -5.63910145e-01,  3.27467649e-01,\n",
      "         -1.20725071e+00],\n",
      "        [-6.52096398e-01,  6.55609547e-01, -3.80718281e-01,\n",
      "          1.52576658e+00],\n",
      "        [-1.63262227e-08, -5.02176732e-01, -8.64765014e-01,\n",
      "          3.47683132e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-7.37037023e-01, -5.87102966e-01,  3.34793269e-01,\n",
      "         -1.22748888e+00],\n",
      "        [-6.75852370e-01,  6.40253183e-01, -3.65101951e-01,\n",
      "          1.46377059e+00],\n",
      "        [-1.78913553e-08, -4.95364480e-01, -8.68685232e-01,\n",
      "          3.49185747e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-7.15193934e-01, -6.09849175e-01,  3.41440508e-01,\n",
      "         -1.24772704e+00],\n",
      "        [-6.98926059e-01,  6.24043752e-01, -3.49387692e-01,\n",
      "          1.40177460e+00],\n",
      "        [-1.94564878e-08, -4.88521627e-01, -8.72551787e-01,\n",
      "          3.50688362e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-6.92628846e-01, -6.32116555e-01,  3.47410337e-01,\n",
      "         -1.26796520e+00],\n",
      "        [-7.21294171e-01,  6.06995298e-01, -3.33603697e-01,\n",
      "          1.33977861e+00],\n",
      "        [-2.10216204e-08, -4.81648595e-01, -8.76364440e-01,\n",
      "          3.52190977e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-6.69364538e-01, -6.53873371e-01,  3.52704876e-01,\n",
      "         -1.28820336e+00],\n",
      "        [-7.42934126e-01,  5.89123103e-01, -3.17777995e-01,\n",
      "          1.27778262e+00],\n",
      "        [-2.25867530e-08, -4.74745810e-01, -8.80122955e-01,\n",
      "          3.53693593e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-6.45424496e-01, -6.75088412e-01,  3.57327379e-01,\n",
      "         -1.30844152e+00],\n",
      "        [-7.63824076e-01,  5.70443670e-01, -3.01938403e-01,\n",
      "          1.21578663e+00],\n",
      "        [-2.41518856e-08, -4.67813697e-01, -8.83827101e-01,\n",
      "          3.55196208e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-6.20832888e-01, -6.95731042e-01,  3.61282221e-01,\n",
      "         -1.32867968e+00],\n",
      "        [-7.83942935e-01,  5.50974701e-01, -2.86112486e-01,\n",
      "          1.15379064e+00],\n",
      "        [-2.57170182e-08, -4.60852685e-01, -8.87476649e-01,\n",
      "          3.56698823e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-5.95614540e-01, -7.15771243e-01,  3.64574888e-01,\n",
      "         -1.34891784e+00],\n",
      "        [-8.03270390e-01,  5.30735076e-01, -2.70327504e-01,\n",
      "          1.09179465e+00],\n",
      "        [-2.72821508e-08, -4.53863204e-01, -8.91071373e-01,\n",
      "          3.58201438e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-5.69794909e-01, -7.35179664e-01,  3.67211960e-01,\n",
      "         -1.36915600e+00],\n",
      "        [-8.21786932e-01,  5.09744833e-01, -2.54610375e-01,\n",
      "          1.02979866e+00],\n",
      "        [-2.88472834e-08, -4.46845686e-01, -8.94611051e-01,\n",
      "          3.59704053e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-5.43400061e-01, -7.53927666e-01,  3.69201095e-01,\n",
      "         -1.38939416e+00],\n",
      "        [-8.39473867e-01,  4.88025142e-01, -2.38987630e-01,\n",
      "          9.67802674e-01],\n",
      "        [-3.04124159e-08, -4.39800563e-01, -8.98095465e-01,\n",
      "          3.61206669e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-5.16456643e-01, -7.71987362e-01,  3.70551007e-01,\n",
      "         -1.40963233e+00],\n",
      "        [-8.56313340e-01,  4.65598276e-01, -2.23485366e-01,\n",
      "          9.05806684e-01],\n",
      "        [-3.19775486e-08, -4.32728273e-01, -9.01524399e-01,\n",
      "          3.62709284e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-4.88991853e-01, -7.89331665e-01,  3.71271452e-01,\n",
      "         -1.42987049e+00],\n",
      "        [-8.72288351e-01,  4.42487587e-01, -2.08129209e-01,\n",
      "          8.43810695e-01],\n",
      "        [-3.35426812e-08, -4.25629250e-01, -9.04897641e-01,\n",
      "          3.64211899e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-4.61033419e-01, -8.05934325e-01,  3.71373197e-01,\n",
      "         -1.45010865e+00],\n",
      "        [-8.87382774e-01,  4.18717472e-01, -1.92944271e-01,\n",
      "          7.81814706e-01],\n",
      "        [-3.51078138e-08, -4.18503934e-01, -9.08214984e-01,\n",
      "          3.65714514e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-4.32609564e-01, -8.21769973e-01,  3.70868004e-01,\n",
      "         -1.47034681e+00],\n",
      "        [-9.01581369e-01,  3.94313344e-01, -1.77955110e-01,\n",
      "          7.19818717e-01],\n",
      "        [-3.66729463e-08, -4.11352765e-01, -9.11476222e-01,\n",
      "          3.67217129e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-4.03748983e-01, -8.36814161e-01,  3.69768602e-01,\n",
      "         -1.49058497e+00],\n",
      "        [-9.14869804e-01,  3.69301599e-01, -1.63185692e-01,\n",
      "          6.57822728e-01],\n",
      "        [-3.82380789e-08, -4.04176185e-01, -9.14681153e-01,\n",
      "          3.68719745e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-4.41306872e-01, -8.19915678e-01,  3.64673176e-01,\n",
      "         -1.44499511e+00],\n",
      "        [-8.97356253e-01,  4.03222730e-01, -1.79340974e-01,\n",
      "          7.10446686e-01],\n",
      "        [-3.73226134e-08, -4.06386159e-01, -9.13701422e-01,\n",
      "          3.68304490e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-4.78106897e-01, -8.01640275e-01,  3.58868590e-01,\n",
      "         -1.39940525e+00],\n",
      "        [-8.78301654e-01,  4.36375996e-01, -1.95351465e-01,\n",
      "          7.63070643e-01],\n",
      "        [-3.64071479e-08, -4.08593759e-01, -9.12716353e-01,\n",
      "          3.67889235e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-5.14085859e-01, -7.82022650e-01,  3.52358204e-01,\n",
      "         -1.35381539e+00],\n",
      "        [-8.57738730e-01,  4.68705430e-01, -2.11185914e-01,\n",
      "          8.15694600e-01],\n",
      "        [-3.54916823e-08, -4.10798970e-01, -9.11725949e-01,\n",
      "          3.67473980e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-5.49181972e-01, -7.61099780e-01,  3.45146760e-01,\n",
      "         -1.30822552e+00],\n",
      "        [-8.35702795e-01,  5.00156628e-01, -2.26813106e-01,\n",
      "          8.68318558e-01],\n",
      "        [-3.45762168e-08, -4.13001781e-01, -9.10730217e-01,\n",
      "          3.67058725e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-5.83334964e-01, -7.38910849e-01,  3.37240386e-01,\n",
      "         -1.26263566e+00],\n",
      "        [-8.12231691e-01,  5.30676840e-01, -2.42201923e-01,\n",
      "          9.20942515e-01],\n",
      "        [-3.36607513e-08, -4.15202179e-01, -9.09729163e-01,\n",
      "          3.66643471e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-6.16486183e-01, -7.15497173e-01,  3.28646590e-01,\n",
      "         -1.21704580e+00],\n",
      "        [-7.87365726e-01,  5.60215057e-01, -2.57321402e-01,\n",
      "          9.73566473e-01],\n",
      "        [-3.27452858e-08, -4.17400150e-01, -9.08722793e-01,\n",
      "          3.66228216e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-6.48578698e-01, -6.90902128e-01,  3.19374266e-01,\n",
      "         -1.17145594e+00],\n",
      "        [-7.61147602e-01,  5.88722102e-01, -2.72140799e-01,\n",
      "          1.02619043e+00],\n",
      "        [-3.18298202e-08, -4.19595682e-01, -9.07711113e-01,\n",
      "          3.65812961e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-6.79557397e-01, -6.65171063e-01,  3.09433679e-01,\n",
      "         -1.12586608e+00],\n",
      "        [-7.33622345e-01,  6.16150710e-01, -2.86629652e-01,\n",
      "          1.07881439e+00],\n",
      "        [-3.09143547e-08, -4.21788761e-01, -9.06694127e-01,\n",
      "          3.65397706e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-7.09369078e-01, -6.38351219e-01,  2.98836465e-01,\n",
      "         -1.08027622e+00],\n",
      "        [-7.04837223e-01,  6.42455610e-01, -3.00757840e-01,\n",
      "          1.13143834e+00],\n",
      "        [-2.99988892e-08, -4.23979376e-01, -9.05671844e-01,\n",
      "          3.64982451e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-7.37962546e-01, -6.10491640e-01,  2.87595616e-01,\n",
      "         -1.03468636e+00],\n",
      "        [-6.74841671e-01,  6.67593595e-01, -3.14495645e-01,\n",
      "          1.18406230e+00],\n",
      "        [-2.90834237e-08, -4.26167513e-01, -9.04644267e-01,\n",
      "          3.64567196e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-7.65288696e-01, -5.81643086e-01,  2.75725466e-01,\n",
      "         -9.89096496e-01],\n",
      "        [-6.43687200e-01,  6.91523601e-01, -3.27813815e-01,\n",
      "          1.23668626e+00],\n",
      "        [-2.81679582e-08, -4.28353160e-01, -9.03611404e-01,\n",
      "          3.64151942e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-7.91300601e-01, -5.51857934e-01,  2.63241674e-01,\n",
      "         -9.43506634e-01],\n",
      "        [-6.11427312e-01,  7.14206771e-01, -3.40683621e-01,\n",
      "          1.28931022e+00],\n",
      "        [-2.72524926e-08, -4.30536304e-01, -9.02573261e-01,\n",
      "          3.63736687e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-8.15953590e-01, -5.21190087e-01,  2.50161210e-01,\n",
      "         -8.97916773e-01],\n",
      "        [-5.78117409e-01,  7.35606519e-01, -3.53076920e-01,\n",
      "          1.34193417e+00],\n",
      "        [-2.63370271e-08, -4.32716931e-01, -9.01529843e-01,\n",
      "          3.63321432e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-8.39205327e-01, -4.89694874e-01,  2.36502326e-01,\n",
      "         -8.52326912e-01],\n",
      "        [-5.43814693e-01,  7.55688589e-01, -3.64966213e-01,\n",
      "          1.39455813e+00],\n",
      "        [-2.54215616e-08, -4.34895030e-01, -9.00481156e-01,\n",
      "          3.62906177e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-8.61015879e-01, -4.57428947e-01,  2.22284536e-01,\n",
      "         -8.06737050e-01],\n",
      "        [-5.08578073e-01,  7.74421113e-01, -3.76324705e-01,\n",
      "          1.44718209e+00],\n",
      "        [-2.45060960e-08, -4.37070587e-01, -8.99427208e-01,\n",
      "          3.62490922e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-8.81347792e-01, -4.24450180e-01,  2.07528587e-01,\n",
      "         -7.61147189e-01],\n",
      "        [-4.72468062e-01,  7.91774660e-01, -3.87126358e-01,\n",
      "          1.49980605e+00],\n",
      "        [-2.35906305e-08, -4.39243590e-01, -8.98368003e-01,\n",
      "          3.62075667e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-9.00166149e-01, -3.90817566e-01,  1.92256429e-01,\n",
      "         -7.15557328e-01],\n",
      "        [-4.35546673e-01,  8.07722284e-01, -3.97345955e-01,\n",
      "          1.55243000e+00],\n",
      "        [-2.26751650e-08, -4.41414027e-01, -8.97303548e-01,\n",
      "          3.61660413e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-9.17438633e-01, -3.56591105e-01,  1.76491185e-01,\n",
      "         -6.69967467e-01],\n",
      "        [-3.97877311e-01,  8.22239562e-01, -4.06959149e-01,\n",
      "          1.60505396e+00],\n",
      "        [-2.17596995e-08, -4.43581883e-01, -8.96233849e-01,\n",
      "          3.61245158e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]],\n",
      "\n",
      "       [[-9.33135582e-01, -3.21831701e-01,  1.60257112e-01,\n",
      "         -6.24377605e-01],\n",
      "        [-3.59524666e-01,  8.35304637e-01, -4.15942517e-01,\n",
      "          1.65767792e+00],\n",
      "        [-2.08442339e-08, -4.45747148e-01, -8.95158913e-01,\n",
      "          3.60829903e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          1.00000000e+00]]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipdb> gen_render_path(c2ws_all[pair_idx], N_views=frames).shape\n",
      "(60, 4, 4)\n",
      "ipdb> exit()\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mall_rgbs))\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mipdb\u001b[39;00m; ipdb\u001b[38;5;241m.\u001b[39mset_trace()\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop here\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m hello\u001b[38;5;241m=\u001b[39mmello\n\u001b[1;32m     63\u001b[0m imgs_source \u001b[38;5;241m=\u001b[39m unpreprocess(imgs_source)\n",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mall_rgbs))\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mipdb\u001b[39;00m; ipdb\u001b[38;5;241m.\u001b[39mset_trace()\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop here\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m hello\u001b[38;5;241m=\u001b[39mmello\n\u001b[1;32m     63\u001b[0m imgs_source \u001b[38;5;241m=\u001b[39m unpreprocess(imgs_source)\n",
      "File \u001b[0;32m/opt/conda/envs/mvsnerf/lib/python3.8/bdb.py:88\u001b[0m, in \u001b[0;36mBdb.trace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;66;03m# None\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mline\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_call(frame, arg)\n",
      "File \u001b[0;32m/opt/conda/envs/mvsnerf/lib/python3.8/bdb.py:113\u001b[0m, in \u001b[0;36mBdb.dispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_here(frame) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbreak_here(frame):\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_line(frame)\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquitting: \u001b[38;5;28;01mraise\u001b[39;00m BdbQuit\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace_dispatch\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i_scene, scene in enumerate(['hotdog','lego','mic','chair']):#'ship','drums','ficus','materials',\n",
    "\n",
    "    cmd = f'--datadir /mnt/hdd/mvsnerf_data/nerf_synthetic/{scene}\\\n",
    "     --dataset_name blender --white_bkgd --imgScale_test {1.0} '\n",
    "\n",
    "    is_finetued = False # set True if rendering with finetuning\n",
    "    if is_finetued:\n",
    "        cmd += f'--ckpt ./runs_fine_tuning/{scene}_1h/ckpts//latest.tar'\n",
    "        pad = 0 #the padding value should be same as your finetuning ckpt\n",
    "    else:\n",
    "        cmd += '--ckpt ./ckpts/base-3src-dense.tar'\n",
    "        pad = 24 #the padding value should be same as your finetuning ckpt\n",
    "        \n",
    "    args = config_parser(cmd.split())\n",
    "    args.use_viewdirs = True\n",
    "\n",
    "    args.N_samples = 128\n",
    "    args.feat_dim = 8+3*4\n",
    "\n",
    "    datadir = args.datadir\n",
    "    datatype = 'val'\n",
    "    args.chunk = 5120\n",
    "    frames = 60\n",
    "\n",
    "    dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "    val_idx = dataset.img_idx\n",
    "    \n",
    "    save_as_image = False\n",
    "    save_dir = f'results/video2'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        if is_finetued:   \n",
    "            # large baselien\n",
    "            imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)\n",
    "            volume_feature = torch.load(args.ckpt)['volume']['feat_volume']\n",
    "            volume_feature = RefVolume(volume_feature.detach()).cuda()\n",
    "            c2ws_render = nerf_video_path(pose_source['c2ws'].cpu(), N_views=frames)\n",
    "        else:            \n",
    "            # neighboring views with angle distance\n",
    "            c2ws_all = dataset.load_poses_all()\n",
    "            random_selete = torch.randint(0,len(c2ws_all),(1,))     #!!!!!!!!!! you may change this line if rendering a specify view \n",
    "            dis = np.sum(c2ws_all[:,:3,2] * c2ws_all[[random_selete],:3,2], axis=-1)\n",
    "            pair_idx = np.argsort(dis)[::-1][torch.randperm(5)[:3]]\n",
    "            imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)#pair_idx=pair_idx, \n",
    "#             volume_feature, _, _ = MVSNet(imgs_source, proj_mats, near_far_source, pad=pad)\n",
    "\n",
    "#####\n",
    "            c2ws_render = gen_render_path(c2ws_all[pair_idx], N_views=frames)\n",
    "            c2ws_render = torch.from_numpy(np.stack(c2ws_render)).float().to(device)\n",
    "        \n",
    "        print(len(dataset.all_rgbs))\n",
    "        \n",
    "        import ipdb; ipdb.set_trace()\n",
    "        \n",
    "        print(\"stop here\")\n",
    "        hello=mello\n",
    "        \n",
    "            \n",
    "\n",
    "            \n",
    "        imgs_source = unpreprocess(imgs_source)\n",
    "        \n",
    "\n",
    "        try:\n",
    "            tqdm._instances.clear() \n",
    "        except Exception:     \n",
    "            pass\n",
    "        \n",
    "        frames = []\n",
    "        img_directions = dataset.directions.to(device)\n",
    "        for i, c2w in enumerate(tqdm(c2ws_render)):\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            rays_o, rays_d = get_rays(img_directions, c2w)  # both (h*w, 3)\n",
    "            rays = torch.cat([rays_o, rays_d,\n",
    "                     near_far_source[0] * torch.ones_like(rays_o[:, :1]),\n",
    "                     near_far_source[1] * torch.ones_like(rays_o[:, :1])],\n",
    "                    1).to(device)  # (H*W, 3)\n",
    "            \n",
    "        \n",
    "            N_rays_all = rays.shape[0]\n",
    "            rgb_rays, depth_rays_preds = [],[]\n",
    "            for chunk_idx in range(N_rays_all//args.chunk + int(N_rays_all%args.chunk>0)):\n",
    "\n",
    "                xyz_coarse_sampled, rays_o, rays_d, z_vals = ray_marcher(rays[chunk_idx*args.chunk:(chunk_idx+1)*args.chunk],\n",
    "                                                    N_samples=args.N_samples)\n",
    "\n",
    "                # Converting world coordinate to ndc coordinate\n",
    "                H, W = imgs_source.shape[-2:]\n",
    "                inv_scale = torch.tensor([W - 1, H - 1]).to(device)\n",
    "                w2c_ref, intrinsic_ref = pose_source['w2cs'][0], pose_source['intrinsics'][0].clone()\n",
    "                xyz_NDC = get_ndc_coordinate(w2c_ref, intrinsic_ref, xyz_coarse_sampled, inv_scale,\n",
    "                                             near=near_far_source[0], far=near_far_source[1], pad=pad*args.imgScale_test)\n",
    "\n",
    "\n",
    "                # rendering\n",
    "                rgb, disp, acc, depth_pred, alpha, extras = rendering(args, pose_source, xyz_coarse_sampled,\n",
    "                                                                       xyz_NDC, z_vals, rays_o, rays_d,\n",
    "                                                                       volume_feature,imgs_source, **render_kwargs_train)\n",
    "    \n",
    "                \n",
    "                rgb, depth_pred = torch.clamp(rgb.cpu(),0,1.0).numpy(), depth_pred.cpu().numpy()\n",
    "                rgb_rays.append(rgb)\n",
    "                depth_rays_preds.append(depth_pred)\n",
    "\n",
    "            \n",
    "            depth_rays_preds = np.concatenate(depth_rays_preds).reshape(H, W)\n",
    "            depth_rays_preds, _ = visualize_depth_numpy(depth_rays_preds, near_far_source)\n",
    "            \n",
    "            rgb_rays = np.concatenate(rgb_rays).reshape(H, W, 3)\n",
    "            H_crop, W_crop = np.array(rgb_rays.shape[:2])//20\n",
    "#             rgb_rays = rgb_rays[H_crop:-H_crop,W_crop:-W_crop]\n",
    "#             depth_rays_preds = depth_rays_preds[H_crop:-H_crop,W_crop:-W_crop]\n",
    "            img_vis = np.concatenate((rgb_rays*255,depth_rays_preds),axis=1)\n",
    "\n",
    "            frames.append(img_vis.astype('uint8'))\n",
    "#             break\n",
    "    imageio.mimwrite(f'{save_dir}/ft_{scene}_spiral.mp4', np.stack(frames), fps=10, quality=10)\n",
    "# plt.imshow(rgb_rays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ckpts ['./ckpts/base-3src-dense.tar']\n",
      "Reloading from ./ckpts/base-3src-dense.tar\n",
      "===> valing index: [26 60 13 47]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5400/2894581296.py:57: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  dis = np.sum(c2ws_all[:,:3,2] * c2ws_all[[random_selete],:3,2], axis=-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> ref idx: [48 61  0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 60/60 [16:28<00:00, 16.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> valing index: [63, 70, 18, 28]\n",
      "====> ref idx: [6, 43, 33]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 10.91 GiB total capacity; 6.42 GiB already allocated; 763.88 MiB free; 6.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m pair_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(dis)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][torch\u001b[38;5;241m.\u001b[39mrandperm(\u001b[38;5;241m5\u001b[39m)[:\u001b[38;5;241m3\u001b[39m]]\n\u001b[1;32m     59\u001b[0m imgs_source, proj_mats, near_far_source, pose_source \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mread_source_views(device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;66;03m#pair_idx=pair_idx, \u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m volume_feature, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mMVSNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproj_mats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnear_far_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m#####\u001b[39;00m\n\u001b[1;32m     63\u001b[0m c2ws_render \u001b[38;5;241m=\u001b[39m gen_render_path(c2ws_all[pair_idx], N_views\u001b[38;5;241m=\u001b[39mframes)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/documents/mvs/mvsnerf_timing/models_src.py:1011\u001b[0m, in \u001b[0;36mMVSNet.forward\u001b[0;34m(self, imgs, proj_mats, near_far, pad, return_color, lindisp)\u001b[0m\n\u001b[1;32m   1009\u001b[0m depth_values \u001b[38;5;241m=\u001b[39m depth_values\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# volume_feat, in_masks = self.build_volume_costvar(feats_l, proj_mats, depth_values, pad=pad)\u001b[39;00m\n\u001b[0;32m-> 1011\u001b[0m volume_feat, in_masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_volume_costvar_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeats_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproj_mats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_color:\n\u001b[1;32m   1014\u001b[0m     feats_l \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((volume_feat[:,:V\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mview(B, V, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m*\u001b[39mvolume_feat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]),in_masks\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)),dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/documents/mvs/mvsnerf_timing/models_src.py:969\u001b[0m, in \u001b[0;36mMVSNet.build_volume_costvar_img\u001b[0;34m(self, imgs, feats, proj_mats, depth_values, pad)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m    968\u001b[0m     volume_sum \u001b[38;5;241m=\u001b[39m volume_sum \u001b[38;5;241m+\u001b[39m warped_volume\n\u001b[0;32m--> 969\u001b[0m     volume_sq_sum \u001b[38;5;241m=\u001b[39m \u001b[43mvolume_sq_sum\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwarped_volume\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    971\u001b[0m     volume_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m warped_volume\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 10.91 GiB total capacity; 6.42 GiB already allocated; 763.88 MiB free; 6.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# for i_scene, scene in enumerate(['hotdog','lego','mic','chair']):#'ship','drums','ficus','materials',\n",
    "\n",
    "for i_scene, scene in enumerate(['chair']):#'ship','drums','ficus','materials',\n",
    "    \n",
    "    cmd = f'--datadir /mnt/hdd/mvsnerf_data/nerf_synthetic/{scene}\\\n",
    "     --dataset_name blender --white_bkgd --imgScale_test {1.0} --num_src_views 3 '\n",
    "\n",
    "    is_finetued = False # set True if rendering with finetuning\n",
    "    if is_finetued:\n",
    "        cmd += f'--ckpt ./runs_fine_tuning/{scene}_1h/ckpts//latest.tar'\n",
    "        pad = 0 #the padding value should be same as your finetuning ckpt\n",
    "    else:\n",
    "        cmd += '--ckpt ./ckpts/base-3src-dense.tar'\n",
    "        pad = 24 #the padding value should be same as your finetuning ckpt\n",
    "        \n",
    "    args = config_parser(cmd.split())\n",
    "    args.use_viewdirs = True\n",
    "\n",
    "    args.N_samples = 128\n",
    "    args.feat_dim = 8+3*4\n",
    "#     args.use_color_volume = False if not is_finetued else args.use_color_volume\n",
    "\n",
    "    # create models\n",
    "    if i_scene==0 or is_finetued:\n",
    "        render_kwargs_train, render_kwargs_test, start, grad_vars = create_nerf_mvs(args, use_mvs=True, dir_embedder=False, pts_embedder=True)\n",
    "        filter_keys(render_kwargs_train)\n",
    "\n",
    "        MVSNet = render_kwargs_train['network_mvs']\n",
    "        render_kwargs_train.pop('network_mvs')\n",
    "\n",
    "\n",
    "    datadir = args.datadir\n",
    "    datatype = 'val'\n",
    "    args.chunk = 5120\n",
    "    frames = 60\n",
    "\n",
    "\n",
    "    dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "    val_idx = dataset.img_idx\n",
    "    \n",
    "    save_as_image = False\n",
    "    save_dir = f'results/video2'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    MVSNet.train()\n",
    "    MVSNet = MVSNet.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        if is_finetued:   \n",
    "            # large baselien\n",
    "            imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)\n",
    "            volume_feature = torch.load(args.ckpt)['volume']['feat_volume']\n",
    "            volume_feature = RefVolume(volume_feature.detach()).cuda()\n",
    "            c2ws_render = nerf_video_path(pose_source['c2ws'].cpu(), N_views=frames)\n",
    "        else:            \n",
    "            # neighboring views with angle distance\n",
    "            c2ws_all = dataset.load_poses_all()\n",
    "            random_selete = torch.randint(0,len(c2ws_all),(1,))     #!!!!!!!!!! you may change this line if rendering a specify view \n",
    "            dis = np.sum(c2ws_all[:,:3,2] * c2ws_all[[random_selete],:3,2], axis=-1)\n",
    "            pair_idx = np.argsort(dis)[::-1][torch.randperm(5)[:3]]  # 아니 이건 abs 가 안 들어가서 가장 가까운 view 를 고르는 것도 아니고 원...\n",
    "            imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)#pair_idx=pair_idx, \n",
    "            volume_feature, _, _ = MVSNet(imgs_source, proj_mats, near_far_source, pad=pad)\n",
    "            \n",
    "            #####\n",
    "            c2ws_render = gen_render_path(c2ws_all[pair_idx], N_views=frames)\n",
    "            c2ws_render = torch.from_numpy(np.stack(c2ws_render)).float().to(device)\n",
    "\n",
    "            \n",
    "        imgs_source = unpreprocess(imgs_source)\n",
    "        \n",
    "\n",
    "        try:\n",
    "            tqdm._instances.clear() \n",
    "        except Exception:     \n",
    "            pass\n",
    "        \n",
    "        frames = []\n",
    "        img_directions = dataset.directions.to(device)\n",
    "        for i, c2w in enumerate(tqdm(c2ws_render)):\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            rays_o, rays_d = get_rays(img_directions, c2w)  # both (h*w, 3)\n",
    "            rays = torch.cat([rays_o, rays_d,\n",
    "                     near_far_source[0] * torch.ones_like(rays_o[:, :1]),\n",
    "                     near_far_source[1] * torch.ones_like(rays_o[:, :1])],\n",
    "                    1).to(device)  # (H*W, 3)\n",
    "            \n",
    "        \n",
    "            N_rays_all = rays.shape[0]\n",
    "            rgb_rays, depth_rays_preds = [],[]\n",
    "            for chunk_idx in range(N_rays_all//args.chunk + int(N_rays_all%args.chunk>0)):\n",
    "\n",
    "                xyz_coarse_sampled, rays_o, rays_d, z_vals = ray_marcher(rays[chunk_idx*args.chunk:(chunk_idx+1)*args.chunk],\n",
    "                                                    N_samples=args.N_samples)\n",
    "\n",
    "                # Converting world coordinate to ndc coordinate\n",
    "                H, W = imgs_source.shape[-2:]\n",
    "                inv_scale = torch.tensor([W - 1, H - 1]).to(device)\n",
    "                w2c_ref, intrinsic_ref = pose_source['w2cs'][0], pose_source['intrinsics'][0].clone()\n",
    "                xyz_NDC = get_ndc_coordinate(w2c_ref, intrinsic_ref, xyz_coarse_sampled, inv_scale,\n",
    "                                             near=near_far_source[0], far=near_far_source[1], pad=pad*args.imgScale_test)\n",
    "\n",
    "\n",
    "                # rendering\n",
    "                rgb, disp, acc, depth_pred, alpha, extras = rendering(args, pose_source, xyz_coarse_sampled,\n",
    "                                                                       xyz_NDC, z_vals, rays_o, rays_d,\n",
    "                                                                       volume_feature,imgs_source, **render_kwargs_train)\n",
    "    \n",
    "                \n",
    "                rgb, depth_pred = torch.clamp(rgb.cpu(),0,1.0).numpy(), depth_pred.cpu().numpy()\n",
    "                rgb_rays.append(rgb)\n",
    "                depth_rays_preds.append(depth_pred)\n",
    "\n",
    "            \n",
    "            depth_rays_preds = np.concatenate(depth_rays_preds).reshape(H, W)\n",
    "            depth_rays_preds, _ = visualize_depth_numpy(depth_rays_preds, near_far_source)\n",
    "            \n",
    "            rgb_rays = np.concatenate(rgb_rays).reshape(H, W, 3)\n",
    "            H_crop, W_crop = np.array(rgb_rays.shape[:2])//20\n",
    "#             rgb_rays = rgb_rays[H_crop:-H_crop,W_crop:-W_crop]\n",
    "#             depth_rays_preds = depth_rays_preds[H_crop:-H_crop,W_crop:-W_crop]\n",
    "            img_vis = np.concatenate((rgb_rays*255,depth_rays_preds),axis=1)\n",
    "\n",
    "            frames.append(img_vis.astype('uint8'))\n",
    "#             break\n",
    "    imageio.mimwrite(f'{save_dir}/ft_{scene}_spiral.mp4', np.stack(frames), fps=10, quality=10)\n",
    "# plt.imshow(rgb_rays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DTU video rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ckpts ['./runs_fine_tuning/dtu_scan1_2h/ckpts//latest.tar']\n",
      "Reloading from ./runs_fine_tuning/dtu_scan1_2h/ckpts//latest.tar\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [04:15<00:00,  4.25s/it]\n"
     ]
    }
   ],
   "source": [
    "for i_scene, scene in enumerate([1]):# any scene index, like 1,2,3...,,8,21,103,114\n",
    "\n",
    "    cmd = f'--datadir /mnt/data/new_disk/sungx/data/mvs_dataset/DTU/mvs_training/dtu/scan{scene} \\\n",
    "     --dataset_name dtu_ft --imgScale_test {1.0} ' #--use_color_volume\n",
    "    \n",
    "    is_finetued = True # set False if rendering without finetuning\n",
    "    if is_finetued:\n",
    "        cmd += f'--ckpt ./runs_fine_tuning/dtu_scan{scene}_2h/ckpts//latest.tar'\n",
    "    else:\n",
    "        cmd += '--ckpt ./ckpts/mvsnerf-v0.tar'\n",
    "\n",
    "    args = config_parser(cmd.split())\n",
    "    args.use_viewdirs = True\n",
    "\n",
    "    args.N_samples = 128\n",
    "    args.feat_dim =  8+3*4\n",
    "    args.use_color_volume = False if not is_finetued else args.use_color_volume\n",
    "\n",
    "\n",
    "    # create models\n",
    "    if i_scene==0 or is_finetued:\n",
    "        render_kwargs_train, render_kwargs_test, start, grad_vars = create_nerf_mvs(args, use_mvs=True, dir_embedder=False, pts_embedder=True)\n",
    "        filter_keys(render_kwargs_train)\n",
    "\n",
    "        MVSNet = render_kwargs_train['network_mvs']\n",
    "        render_kwargs_train.pop('network_mvs')\n",
    "\n",
    "\n",
    "    datadir = args.datadir\n",
    "    datatype = 'val'\n",
    "    pad = 24 #the padding value should be same as your finetuning ckpt\n",
    "    args.chunk = 5120\n",
    "    frames = 60\n",
    "\n",
    "\n",
    "    dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "    val_idx = dataset.img_idx\n",
    "    \n",
    "    save_as_image = False\n",
    "    save_dir = f'results/video2'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    MVSNet.train()\n",
    "    MVSNet = MVSNet.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        if is_finetued:   \n",
    "            # large baselien\n",
    "            imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)\n",
    "            volume_feature = torch.load(args.ckpt)['volume']['feat_volume']\n",
    "            volume_feature = RefVolume(volume_feature.detach()).cuda()\n",
    "        else:            \n",
    "            # neighboring views with angle distance\n",
    "            c2ws_all = dataset.load_poses_all()\n",
    "            random_selete = torch.randint(0,len(c2ws_all),(1,)) #!!!!!!!!!! you may change this line if rendering a specify view \n",
    "            dis = np.sum(c2ws_all[:,:3,2] * c2ws_all[[random_selete],:3,2], axis=-1)\n",
    "            pair_idx = np.argsort(dis)[::-1][:3]#[25, 21, 33]#[14,15,24]#\n",
    "            imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(pair_idx=pair_idx, device=device)\n",
    "            volume_feature, _, _ = MVSNet(imgs_source, proj_mats, near_far_source, pad=pad)\n",
    "            \n",
    "        imgs_source = unpreprocess(imgs_source)\n",
    "\n",
    "        c2ws_render = gen_render_path(pose_source['c2ws'].cpu().numpy(), N_views=frames)\n",
    "        c2ws_render = torch.from_numpy(np.stack(c2ws_render)).float().to(device)\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            tqdm._instances.clear() \n",
    "        except Exception:     \n",
    "            pass\n",
    "        \n",
    "        frames = []\n",
    "        img_directions = dataset.directions.to(device)\n",
    "        for i, c2w in enumerate(tqdm(c2ws_render)):\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            rays_o, rays_d = get_rays(img_directions, c2w)  # both (h*w, 3)\n",
    "            rays = torch.cat([rays_o, rays_d,\n",
    "                     near_far_source[0] * torch.ones_like(rays_o[:, :1]),\n",
    "                     near_far_source[1] * torch.ones_like(rays_o[:, :1])],\n",
    "                    1).to(device)  # (H*W, 3)\n",
    "            \n",
    "        \n",
    "            N_rays_all = rays.shape[0]\n",
    "            rgb_rays, depth_rays_preds = [],[]\n",
    "            for chunk_idx in range(N_rays_all//args.chunk + int(N_rays_all%args.chunk>0)):\n",
    "\n",
    "                xyz_coarse_sampled, rays_o, rays_d, z_vals = ray_marcher(rays[chunk_idx*args.chunk:(chunk_idx+1)*args.chunk],\n",
    "                                                    N_samples=args.N_samples)\n",
    "\n",
    "                # Converting world coordinate to ndc coordinate\n",
    "                H, W = imgs_source.shape[-2:]\n",
    "                inv_scale = torch.tensor([W - 1, H - 1]).to(device)\n",
    "                w2c_ref, intrinsic_ref = pose_source['w2cs'][0], pose_source['intrinsics'][0].clone()\n",
    "                xyz_NDC = get_ndc_coordinate(w2c_ref, intrinsic_ref, xyz_coarse_sampled, inv_scale,\n",
    "                                             near=near_far_source[0], far=near_far_source[1], pad=pad*args.imgScale_test)\n",
    "\n",
    "\n",
    "                # rendering\n",
    "                rgb, disp, acc, depth_pred, alpha, extras = rendering(args, pose_source, xyz_coarse_sampled,\n",
    "                                                                       xyz_NDC, z_vals, rays_o, rays_d,\n",
    "                                                                       volume_feature,imgs_source, **render_kwargs_train)\n",
    "    \n",
    "                \n",
    "                rgb, depth_pred = torch.clamp(rgb.cpu(),0,1.0).numpy(), depth_pred.cpu().numpy()\n",
    "                rgb_rays.append(rgb)\n",
    "                depth_rays_preds.append(depth_pred)\n",
    "\n",
    "            \n",
    "            depth_rays_preds = np.concatenate(depth_rays_preds).reshape(H, W)\n",
    "            depth_rays_preds, _ = visualize_depth_numpy(depth_rays_preds, near_far_source)\n",
    "            \n",
    "            rgb_rays = np.concatenate(rgb_rays).reshape(H, W, 3)\n",
    "#             H_crop, W_crop = np.array(rgb_rays.shape[:2])//20\n",
    "#             rgb_rays = rgb_rays[H_crop:-H_crop,W_crop:-W_crop]\n",
    "#             depth_rays_preds = depth_rays_preds[H_crop:-H_crop,W_crop:-W_crop]\n",
    "            img_vis = np.concatenate((rgb_rays*255,depth_rays_preds),axis=1)\n",
    "            frames.append(img_vis.astype('uint8'))\n",
    "                \n",
    "    imageio.mimwrite(f'{save_dir}/ft_scan{scene}_spiral2.mp4', np.stack(frames), fps=20, quality=10)\n",
    "# plt.imshow(rgb_rays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# render path generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============> rendering dataset <===================\n",
      "34 34 /mnt/new_disk_2/anpei/Dataset/MVSNeRF/nerf_llff_data/flower\n",
      "16.93030140064795\n",
      "===> valing index: [20  6 22  5]\n",
      "====> ref idx: [12 29 11]\n",
      "============> rendering dataset <===================\n",
      "25 25 /mnt/new_disk_2/anpei/Dataset/MVSNeRF/nerf_llff_data/orchids\n",
      "10.866254797560998\n",
      "===> valing index: [12 10 16 19]\n",
      "====> ref idx: [ 8 13 11]\n",
      "============> rendering dataset <===================\n",
      "41 41 /mnt/new_disk_2/anpei/Dataset/MVSNeRF/nerf_llff_data/room\n",
      "8.030018355528686\n",
      "===> valing index: [35 15 38 21]\n",
      "====> ref idx: [14 39 34]\n",
      "============> rendering dataset <===================\n",
      "26 26 /mnt/new_disk_2/anpei/Dataset/MVSNeRF/nerf_llff_data/leaves\n",
      "26.362457265215173\n",
      "===> valing index: [13 11 16  4]\n",
      "====> ref idx: [12 18  8]\n",
      "============> rendering dataset <===================\n",
      "20 20 /mnt/new_disk_2/anpei/Dataset/MVSNeRF/nerf_llff_data/fern\n",
      "12.738972134007064\n",
      "===> valing index: [12 13  5 19]\n",
      "====> ref idx: [17  2  7]\n",
      "============> rendering dataset <===================\n",
      "62 62 /mnt/new_disk_2/anpei/Dataset/MVSNeRF/nerf_llff_data/horns\n",
      "8.315313360692242\n",
      "===> valing index: [33 40 31 59]\n",
      "====> ref idx: [23 32 24]\n",
      "============> rendering dataset <===================\n",
      "55 55 /mnt/new_disk_2/anpei/Dataset/MVSNeRF/nerf_llff_data/trex\n",
      "10.958461140504273\n",
      "===> valing index: [20 21 53 22]\n",
      "====> ref idx: [52 19 47]\n",
      "============> rendering dataset <===================\n",
      "42 42 /mnt/new_disk_2/anpei/Dataset/MVSNeRF/nerf_llff_data/fortress\n",
      "9.564056923447355\n",
      "===> valing index: [21  9 40 25]\n",
      "====> ref idx: [15 20 26]\n",
      "============> rendering dataset <===================\n",
      "34 34 /mnt/new_disk_2/anpei/Dataset/MVSNeRF/nerf_llff_data/flower\n",
      "16.93030140064795\n",
      "===> valing index: [20  6 22  5]\n",
      "====> ref idx: [12 29 11]\n",
      "============> rendering dataset <===================\n",
      "25 25 /mnt/new_disk_2/anpei/Dataset/MVSNeRF/nerf_llff_data/orchids\n",
      "10.866254797560998\n",
      "===> valing index: [12 10 16 19]\n",
      "====> ref idx: [ 8 13 11]\n",
      "============> rendering dataset <===================\n",
      "41 41 /mnt/new_disk_2/anpei/Dataset/MVSNeRF/nerf_llff_data/room\n",
      "8.030018355528686\n",
      "===> valing index: [35 15 38 21]\n",
      "====> ref idx: [14 39 34]\n",
      "============> rendering dataset <===================\n",
      "26 26 /mnt/new_disk_2/anpei/Dataset/MVSNeRF/nerf_llff_data/leaves\n",
      "26.362457265215173\n",
      "===> valing index: [13 11 16  4]\n",
      "====> ref idx: [12 18  8]\n",
      "============> rendering dataset <===================\n",
      "20 20 /mnt/new_disk_2/anpei/Dataset/MVSNeRF/nerf_llff_data/fern\n",
      "12.738972134007064\n",
      "===> valing index: [12 13  5 19]\n",
      "====> ref idx: [17  2  7]\n",
      "============> rendering dataset <===================\n",
      "62 62 /mnt/new_disk_2/anpei/Dataset/MVSNeRF/nerf_llff_data/horns\n",
      "8.315313360692242\n",
      "===> valing index: [33 40 31 59]\n",
      "====> ref idx: [23 32 24]\n",
      "============> rendering dataset <===================\n",
      "55 55 /mnt/new_disk_2/anpei/Dataset/MVSNeRF/nerf_llff_data/trex\n",
      "10.958461140504273\n",
      "===> valing index: [20 21 53 22]\n",
      "====> ref idx: [52 19 47]\n",
      "============> rendering dataset <===================\n",
      "42 42 /mnt/new_disk_2/anpei/Dataset/MVSNeRF/nerf_llff_data/fortress\n",
      "9.564056923447355\n",
      "===> valing index: [21  9 40 25]\n",
      "====> ref idx: [15 20 26]\n",
      "============> rendering dataset <===================\n",
      "===> valing index: [80 86 22 20]\n",
      "====> ref idx: [12 32 44]\n",
      "============> rendering dataset <===================\n",
      "===> valing index: [20 49 55 72]\n",
      "====> ref idx: [61 80 64]\n",
      "============> rendering dataset <===================\n",
      "===> valing index: [ 8 24 32 78]\n",
      "====> ref idx: [62 56 26]\n",
      "============> rendering dataset <===================\n",
      "===> valing index: [63, 70, 18, 28]\n",
      "====> ref idx: [6, 43, 33]\n",
      "============> rendering dataset <===================\n",
      "===> valing index: [79, 74, 91, 68]\n",
      "====> ref idx: [43, 81, 14]\n",
      "============> rendering dataset <===================\n",
      "===> valing index: [38 23  0  5]\n",
      "====> ref idx: [92 69 56]\n",
      "============> rendering dataset <===================\n",
      "===> valing index: [36 63 46 96]\n",
      "====> ref idx: [34 73 94]\n",
      "============> rendering dataset <===================\n",
      "===> valing index: [26 60 13 47]\n",
      "====> ref idx: [48 61  0]\n",
      "============> rendering dataset <===================\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n"
     ]
    }
   ],
   "source": [
    "render_poses = {}\n",
    "datatype = 'val'\n",
    "for i_scene, scene in enumerate(['xgaze_11images_cropped_colmapCODE']):\n",
    "    # add --use_color_volume if the ckpts are fintuned with this flag\n",
    "    cmd = f'--datadir /home/hengfei/Desktop/research/mvsnerf/xgaze/{scene}  \\\n",
    "     --dataset_name llff --imgScale_test {1.0} \\\n",
    "    --ckpt ./runs_fine_tuning/{scene}-ft/ckpts/latest.tar'\n",
    "\n",
    "    args = config_parser(cmd.split())\n",
    "    args.use_viewdirs = True\n",
    "\n",
    "\n",
    "    print('============> rendering dataset <===================')\n",
    "    dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "    val_idx = dataset.img_idx\n",
    "    \n",
    "\n",
    "    imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)\n",
    "\n",
    "    c2ws_all = dataset.poses\n",
    "    w2cs, c2ws = pose_source['w2cs'], pose_source['c2ws']\n",
    "    pair_idx = torch.load('configs/pairs.th')[f'{scene}_train']\n",
    "    c2ws_render = get_spiral(c2ws_all[pair_idx], near_far_source, rads_scale = 0.1, N_views=60) \n",
    "    \n",
    "    render_poses[f'{scene}_c2ws'] = c2ws_render\n",
    "    render_poses[f'{scene}_intrinsic'] = pose_source['intrinsics'][0].cpu().numpy()\n",
    "    \n",
    "#     render_poses[f'{scene}_near_far_source'] = near_far_source\n",
    "#     render_poses[f'{scene}_c2ws_no_ft'] = c2ws_render\n",
    "#     render_poses[f'{scene}_intrinsic_no_ft'] = pose_source['intrinsics'][0].cpu().numpy()\n",
    "\n",
    "# for i_scene, scene in enumerate(['flower','orchids', 'room','leaves','fern','horns','trex','fortress']):#'flower','orchids', 'room','leaves','fern','horns','trex','fortress'\n",
    "#     # add --use_color_volume if the ckpts are fintuned with this flag\n",
    "#     cmd = f'--datadir /mnt/new_disk_2/anpei/Dataset/MVSNeRF/nerf_llff_data/{scene}  \\\n",
    "#      --dataset_name llff --imgScale_test {1.0} \\\n",
    "#     --ckpt ./runs_new/runs_fine_tuning/{scene}/ckpts//latest.tar'\n",
    "\n",
    "#     args = config_parser(cmd.split())\n",
    "#     args.use_viewdirs = True\n",
    "\n",
    "\n",
    "#     print('============> rendering dataset <===================')\n",
    "#     dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "#     val_idx = dataset.img_idx\n",
    "    \n",
    "\n",
    "#     imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)\n",
    "\n",
    "#     c2ws_all = dataset.poses\n",
    "#     w2cs, c2ws = pose_source['w2cs'], pose_source['c2ws']\n",
    "#     pair_idx = torch.load('configs/pairs.th')[f'{scene}_train']\n",
    "#     c2ws_render = get_spiral(c2ws_all[pair_idx], near_far_source, rads_scale = 0.6, N_views=60) \n",
    "    \n",
    "#     render_poses[f'{scene}_c2ws'] = c2ws_render\n",
    "#     render_poses[f'{scene}_intrinsic'] = pose_source['intrinsics'][0].cpu().numpy()\n",
    "# #######################################\n",
    "# for i_scene, scene in enumerate(['ship','mic','chair','lego','drums','ficus','materials','hotdog']):#\n",
    "\n",
    "#     cmd = f'--datadir /mnt/new_disk_2/anpei/Dataset/nerf_synthetic/{scene}  \\\n",
    "#      --dataset_name blender --white_bkgd --imgScale_test {1.0}  \\\n",
    "#     --ckpt /mnt/new_disk_2/anpei/code/MVS-NeRF/runs_fine_tuning/{scene}/ckpts//latest.tar '\n",
    "\n",
    "#     args = config_parser(cmd.split())\n",
    "\n",
    "\n",
    "#     print('============> rendering dataset <===================')\n",
    "#     dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "\n",
    "\n",
    "#     imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)\n",
    "#     c2ws_render = nerf_video_path(pose_source['c2ws'].cpu(), N_views=60)\n",
    "    \n",
    "#     render_poses[f'{scene}_c2ws'] = c2ws_render.cpu().numpy()\n",
    "#     render_poses[f'{scene}_intrinsic'] = pose_source['intrinsics'][0].cpu().numpy()\n",
    "    \n",
    "# ##################################################\n",
    "# for i_scene, scene in enumerate([1]):\n",
    "\n",
    "#     cmd = f'--datadir /mnt/data/new_disk/sungx/data/mvs_dataset/DTU/mvs_training/dtu/scan{scene}  \\\n",
    "#      --dataset_name dtu_ft --imgScale_test {1.0}   \\\n",
    "#     --ckpt /mnt/new_disk_2/anpei/code/MVS-NeRF/runs_fine_tuning/dtu_scan{scene}/ckpts//latest.tar --netwidth 256 --net_type v0 --use_color_volume'\n",
    "\n",
    "#     args = config_parser(cmd.split())\n",
    "#     args.use_viewdirs = True\n",
    "\n",
    "\n",
    "#     print('============> rendering dataset <===================')\n",
    "#     dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "#     val_idx = dataset.img_idx\n",
    "\n",
    "#     imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)\n",
    "#     imgs_source = unpreprocess(imgs_source)\n",
    "\n",
    "#     c2ws_render = gen_render_path(pose_source['c2ws'].cpu().numpy(), N_views=60)\n",
    "#     render_poses[f'dtu_c2ws'] = c2ws_render\n",
    "#     render_poses[f'dtu_intrinsic'] = pose_source['intrinsics'][0].cpu().numpy()\n",
    "\n",
    "    \n",
    "torch.save(render_poses, './configs/video_path.th')\n",
    "np.save('./configs/video_path.npy',render_poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(234)\n",
    "_EPS = np.finfo(float).eps * 4.0\n",
    "TINY_NUMBER = 1e-6      # float32 only has 7 decimal digits precision\n",
    "\n",
    "def angular_dist_between_2_vectors(vec1, vec2):\n",
    "    vec1_unit = vec1 / (np.linalg.norm(vec1, axis=1, keepdims=True) + TINY_NUMBER)\n",
    "    vec2_unit = vec2 / (np.linalg.norm(vec2, axis=1, keepdims=True) + TINY_NUMBER)\n",
    "    angular_dists = np.arccos(np.clip(np.sum(vec1_unit*vec2_unit, axis=-1), -1.0, 1.0))\n",
    "    return angular_dists\n",
    "\n",
    "\n",
    "def batched_angular_dist_rot_matrix(R1, R2):\n",
    "    '''\n",
    "    calculate the angular distance between two rotation matrices (batched)\n",
    "    :param R1: the first rotation matrix [N, 3, 3]\n",
    "    :param R2: the second rotation matrix [N, 3, 3]\n",
    "    :return: angular distance in radiance [N, ]\n",
    "    '''\n",
    "    assert R1.shape[-1] == 3 and R2.shape[-1] == 3 and R1.shape[-2] == 3 and R2.shape[-2] == 3\n",
    "    return np.arccos(np.clip((np.trace(np.matmul(R2.transpose(0, 2, 1), R1), axis1=1, axis2=2) - 1) / 2.,\n",
    "                             a_min=-1 + TINY_NUMBER, a_max=1 - TINY_NUMBER))\n",
    "\n",
    "\n",
    "def get_nearest_pose_ids(tar_pose, ref_poses, num_select, tar_id=-1, angular_dist_method='vector',\n",
    "                         scene_center=(0, 0, 0)):\n",
    "    '''\n",
    "    Args:\n",
    "        tar_pose: target pose [3, 3]\n",
    "        ref_poses: reference poses [N, 3, 3]\n",
    "        num_select: the number of nearest views to select\n",
    "    Returns: the selected indices\n",
    "    '''\n",
    "    num_cams = len(ref_poses)\n",
    "    # num_select = min(num_select, num_cams-1)\n",
    "    batched_tar_pose = tar_pose[None].repeat(num_cams,axis=0)\n",
    "\n",
    "    if angular_dist_method == 'matrix':\n",
    "        dists = batched_angular_dist_rot_matrix(batched_tar_pose[:, :3, :3], ref_poses[:, :3, :3])\n",
    "    elif angular_dist_method == 'vector':\n",
    "        tar_cam_locs = batched_tar_pose[:, :3, 3]\n",
    "        ref_cam_locs = ref_poses[:, :3, 3]\n",
    "        scene_center = np.array(scene_center)[None, ...]\n",
    "        tar_vectors = tar_cam_locs - scene_center\n",
    "        ref_vectors = ref_cam_locs - scene_center\n",
    "        dists = angular_dist_between_2_vectors(tar_vectors, ref_vectors)\n",
    "    elif angular_dist_method == 'dist':\n",
    "        tar_cam_locs = batched_tar_pose[:, :3, 3]\n",
    "        ref_cam_locs = ref_poses[:, :3, 3]\n",
    "        dists = np.linalg.norm(tar_cam_locs - ref_cam_locs, axis=1)\n",
    "    else:\n",
    "        raise Exception('unknown angular distance calculation method!')\n",
    "\n",
    "    if tar_id >= 0:\n",
    "        assert tar_id < num_cams\n",
    "        dists[tar_id] = 1e3  # make sure not to select the target id itself\n",
    "\n",
    "    sorted_ids = np.argsort(dists)\n",
    "    selected_ids = sorted_ids[:num_select]\n",
    "    # print(angular_dists[selected_ids] * 180 / np.pi)\n",
    "    return selected_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============> rendering dataset <===================\n",
      "===> valing index: [20 49 55 72]\n",
      "====> ref idx: [61 80 64]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 200) to (608, 208) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "render_poses = {}\n",
    "datatype = 'val'\n",
    "fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "# for i_scene, scene in enumerate(['flower']):#\n",
    "#     # add --use_color_volume if the ckpts are fintuned with this flag\n",
    "#     cmd = f'--datadir /mnt/new_disk_2/anpei/Dataset/MVSNeRF/nerf_llff_data/{scene}  \\\n",
    "#      --dataset_name llff --imgScale_test {1.0} \\\n",
    "#     --ckpt ./runs_new/runs_fine_tuning/{scene}/ckpts//latest.tar'\n",
    "\n",
    "#     args = config_parser(cmd.split())\n",
    "#     args.use_viewdirs = True\n",
    "\n",
    "\n",
    "#     print('============> rendering dataset <===================')\n",
    "#     dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "#     val_idx = dataset.img_idx\n",
    "    \n",
    "\n",
    "#     imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)\n",
    "\n",
    "#     c2ws_all = dataset.poses\n",
    "#     w2cs, c2ws = pose_source['w2cs'], pose_source['c2ws']\n",
    "#     pair_idx = torch.load('configs/pairs.th')[f'{scene}_train']\n",
    "#     c2ws_render = get_spiral(c2ws_all[pair_idx], near_far_source, rads_scale = 0.6, N_views=60) \n",
    "    \n",
    "#     images = []\n",
    "#     for i, c2w in enumerate(c2ws_render):\n",
    "#         nearest_pose_ids = get_nearest_pose_ids(c2w,\n",
    "#                                                 c2ws_all[pair_idx],\n",
    "#                                                 3,\n",
    "#                                                 angular_dist_method='vector')  \n",
    "#         idxs = pair_idx[nearest_pose_ids]\n",
    "        \n",
    "#         im=[]\n",
    "#         List = sorted(glob.glob(f'/mnt/new_disk_2/anpei/Dataset/MVSNeRF/nerf_llff_data/{scene}/images_4/*'))\n",
    "#         for idx in idxs:\n",
    "#             im.append(cv2.resize(cv2.imread(List[idx]),None,fx=0.25,fy=0.25))\n",
    "#         im = np.concatenate(im,axis=1)\n",
    "#         images.append(im[...,::-1])\n",
    "    \n",
    "#     imageio.mimwrite(f'./results/test4/{scene}.mp4', np.stack(images), fps=20, quality=10)\n",
    "\n",
    "# for i_scene, scene in enumerate(['flower','orchids', 'room','leaves','fern','horns','trex','fortress']):#'flower','orchids', 'room','leaves','fern','horns','trex','fortress'\n",
    "#     # add --use_color_volume if the ckpts are fintuned with this flag\n",
    "#     cmd = f'--datadir /mnt/new_disk_2/anpei/Dataset/MVSNeRF/nerf_llff_data/{scene}  \\\n",
    "#      --dataset_name llff --imgScale_test {1.0} \\\n",
    "#     --ckpt ./runs_new/runs_fine_tuning/{scene}/ckpts//latest.tar'\n",
    "\n",
    "#     args = config_parser(cmd.split())\n",
    "#     args.use_viewdirs = True\n",
    "\n",
    "\n",
    "#     print('============> rendering dataset <===================')\n",
    "#     dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "#     val_idx = dataset.img_idx\n",
    "    \n",
    "\n",
    "#     imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)\n",
    "\n",
    "#     c2ws_all = dataset.poses\n",
    "#     w2cs, c2ws = pose_source['w2cs'], pose_source['c2ws']\n",
    "#     pair_idx = torch.load('configs/pairs.th')[f'{scene}_train']\n",
    "#     c2ws_render = get_spiral(c2ws_all[pair_idx], near_far_source, rads_scale = 0.6, N_views=60) \n",
    "    \n",
    "#     render_poses[f'{scene}_c2ws'] = c2ws_render\n",
    "#     render_poses[f'{scene}_intrinsic'] = pose_source['intrinsics'][0].cpu().numpy()\n",
    "# #######################################\n",
    "for i_scene, scene in enumerate(['mic']):#\n",
    "\n",
    "    cmd = f'--datadir /mnt/new_disk_2/anpei/Dataset/nerf_synthetic/{scene}  \\\n",
    "     --dataset_name blender --white_bkgd --imgScale_test {1.0}  \\\n",
    "    --ckpt /mnt/new_disk_2/anpei/code/MVS-NeRF/runs_fine_tuning/{scene}/ckpts//latest.tar '\n",
    "\n",
    "    args = config_parser(cmd.split())\n",
    "\n",
    "\n",
    "    print('============> rendering dataset <===================')\n",
    "    dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "\n",
    "    c2ws_all = dataset.load_poses_all()\n",
    "    pair_idx = torch.load('configs/pairs.th')[f'{scene}_train']\n",
    "    imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)\n",
    "    c2ws_render = nerf_video_path(pose_source['c2ws'].cpu(), N_views=60).cpu().numpy()\n",
    "    \n",
    "    \n",
    "    images = []\n",
    "    for i, c2w in enumerate(c2ws_render):\n",
    "        nearest_pose_ids = get_nearest_pose_ids(c2w,\n",
    "                                                c2ws_all[pair_idx],\n",
    "                                                3,\n",
    "                                                angular_dist_method='vector')  \n",
    "        idxs = pair_idx[nearest_pose_ids]\n",
    "        im=[]\n",
    "        List = sorted(glob.glob(f'/mnt/new_disk2/anpei/Dataset/nerf_synthetic/{scene}/train/*'))\n",
    "        for idx in idxs:\n",
    "            temp = cv2.imread(f'/mnt/new_disk2/anpei/Dataset/nerf_synthetic/mic/train/r_{idx}.png',-1)\n",
    "            im.append(cv2.resize(temp,None,fx=0.25,fy=0.25))\n",
    "        im = np.concatenate(im,axis=1)\n",
    "        images.append(im[...,[2,1,0,3]])\n",
    "    \n",
    "    imageio.mimwrite(f'./results/test4/{scene}.mp4', np.stack(images), fps=20, quality=10)\n",
    "    \n",
    "# ##################################################\n",
    "# for i_scene, scene in enumerate([1]):\n",
    "\n",
    "#     cmd = f'--datadir /mnt/data/new_disk/sungx/data/mvs_dataset/DTU/mvs_training/dtu/scan{scene}  \\\n",
    "#      --dataset_name dtu_ft --imgScale_test {1.0}   \\\n",
    "#     --ckpt /mnt/new_disk_2/anpei/code/MVS-NeRF/runs_fine_tuning/dtu_scan{scene}/ckpts//latest.tar --netwidth 256 --net_type v0 --use_color_volume'\n",
    "\n",
    "#     args = config_parser(cmd.split())\n",
    "#     args.use_viewdirs = True\n",
    "\n",
    "\n",
    "#     print('============> rendering dataset <===================')\n",
    "#     dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "#     val_idx = dataset.img_idx\n",
    "\n",
    "#     imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)\n",
    "#     imgs_source = unpreprocess(imgs_source)\n",
    "\n",
    "#     c2ws_render = gen_render_path(pose_source['c2ws'].cpu().numpy(), N_views=60)\n",
    "#     render_poses[f'dtu_c2ws'] = c2ws_render\n",
    "#     render_poses[f'dtu_intrinsic'] = pose_source['intrinsics'][0].cpu().numpy()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
